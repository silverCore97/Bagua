{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QSparseLocal_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqZi1G51qcqkjqqgtJYa97",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverCore97/Bagua/blob/main/QSparseLocal_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Check CUDA version**"
      ],
      "metadata": {
        "id": "FidgI73Gc8m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biBKGSXi6Chp",
        "outputId": "2a1f575f-f6e2-4abe-eab1-7c8724cb19af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Install Bagua**"
      ],
      "metadata": {
        "id": "H1UzpVrNc3wO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qCGqkkr5mr4",
        "outputId": "f0c8342a-924d-406b-8636-463db63b5022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bagua-cuda111 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pytest-benchmark>=3.4 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (3.4.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.4.4)\n",
            "Requirement already satisfied: scikit-optimize>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.9.0)\n",
            "Requirement already satisfied: scikit-learn!=1.0,<=1.0.1,>=0.24 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (4.64.0)\n",
            "Requirement already satisfied: parallel-ssh==2.8.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.8.0)\n",
            "Requirement already satisfied: deprecation>=2.1 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.1.0)\n",
            "Requirement already satisfied: pydantic>=1.8 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.9.0)\n",
            "Requirement already satisfied: prometheus-client>=0.11 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.14.1)\n",
            "Requirement already satisfied: gorilla==0.4.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.4.0)\n",
            "Requirement already satisfied: gevent>=21.8 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (21.12.0)\n",
            "Requirement already satisfied: xxhash>=2.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (3.0.0)\n",
            "Requirement already satisfied: setuptools-rust in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.2.0)\n",
            "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.27.1)\n",
            "Requirement already satisfied: flask>=2.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.1.1)\n",
            "Requirement already satisfied: ssh2-python>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from parallel-ssh==2.8.0->bagua-cuda111) (0.27.0)\n",
            "Requirement already satisfied: ssh-python>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from parallel-ssh==2.8.0->bagua-cuda111) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deprecation>=2.1->bagua-cuda111) (21.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (2.1.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (4.11.3)\n",
            "Requirement already satisfied: Werkzeug>=2.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (2.1.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (3.1.1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (8.1.2)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (1.1.2)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (5.4.0)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (4.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6.0->flask>=2.0->bagua-cuda111) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6.0->flask>=2.0->bagua-cuda111) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=3.0->flask>=2.0->bagua-cuda111) (2.0.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.7/dist-packages (from pytest-benchmark>=3.4->bagua-cuda111) (8.0.0)\n",
            "Requirement already satisfied: pytest>=3.8 in /usr/local/lib/python3.7/dist-packages (from pytest-benchmark>=3.4->bagua-cuda111) (7.1.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (1.1.1)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (1.11.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (1.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (21.4.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (3.1.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize>=0.8.1->bagua-cuda111) (21.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize>=0.8.1->bagua-cuda111) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deprecation>=2.1->bagua-cuda111) (3.0.8)\n",
            "Requirement already satisfied: semantic-version<3,>=2.8.2 in /usr/local/lib/python3.7/dist-packages (from setuptools-rust->bagua-cuda111) (2.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install bagua-cuda111"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import environment variables**"
      ],
      "metadata": {
        "id": "IThXKxGEcv4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['RANK'] = '0'\n",
        "os.environ['WORLD_SIZE'] = '1'\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '29500'"
      ],
      "metadata": {
        "id": "ykSOeQsnTLCz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **QSparseLocal Algorithm Implementation**"
      ],
      "metadata": {
        "id": "1M-JsrdWcjzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "from bagua.torch_api.bucket import BaguaBucket\n",
        "from bagua.torch_api.tensor import BaguaTensor\n",
        "from bagua.torch_api.data_parallel.bagua_distributed import BaguaDistributedDataParallel\n",
        "from bagua.torch_api.algorithms import Algorithm, AlgorithmImpl\n",
        "from bagua.torch_api.communication import BaguaProcessGroup\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import torch\n",
        "import math\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "class QSparseLocalOptimizer(Optimizer):\n",
        "    def __init__(\n",
        "            self,\n",
        "            params,\n",
        "            lr: float = 1e-3,  ## Later step dependent learning rate\n",
        "            k: int = 1000,\n",
        "            schedule: List = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create a dedicated optimizer used for\n",
        "        `QSparseLocal <https://tutorials.baguasys.com/algorithms/q-adam>`_ algorithm.\n",
        "\n",
        "        Args:\n",
        "            params (iterable): Iterable of parameters to optimize or dicts defining\n",
        "                parameter groups.\n",
        "            lr: Learning rate.\n",
        "            k: How many tensor components are kept during sparsification\n",
        "            schedule: Description of synchronization schedule\n",
        "        \"\"\"\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "\n",
        "\n",
        "        defaults = dict(lr=lr, k=k, schedule=schedule)\n",
        "        super(QSparseLocalOptimizer, self).__init__(params, defaults)\n",
        "        # TODO: QSparseLocal optimizer maintain `step_id` in its state\n",
        "        self.step_id = 0\n",
        "        self.schedule = schedule\n",
        "        self.k = k\n",
        "\n",
        "        # initialize global and local model, and memory for error compensation\n",
        "        for group_id, group in enumerate(self.param_groups):\n",
        "            params_with_grad = []\n",
        "            for p in group[\"params\"]:\n",
        "                params_with_grad.append(p)\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state[\"local\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    state[\"global\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    state[\"memory\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    state[\"qsl_grad\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    '''\n",
        "                    local: Local parameter vector\n",
        "                    global: Global paramenter vector (same for all workers)\n",
        "                    memory: Term for error compensation for quantization and sparsification of gradient tensor\n",
        "                    qsl_grad: The quantized and sparsified gradient tensor to be sent to master (to allreduce)\n",
        "                    '''\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(QSparseLocalOptimizer, self).__setstate__(state)\n",
        "\n",
        "    ## input: Uncompressed Gradient tensor\n",
        "    ## Output: Quantized and sparsified Gradient tensor\n",
        "    def qsp(self, input):\n",
        "\n",
        "        ###To do: Allow other quantization\n",
        "        def signq(self, var):\n",
        "            return torch.sign(var)  # Returns a new tensor with the signs of the elements of input\n",
        "\n",
        "        ### To do: Allow rand_k sparsification\n",
        "        org_shape = input.size()\n",
        "        numel = torch.numel(input)\n",
        "        K = min(numel, self.k)  # k is the optimizer's k,\n",
        "                                # K is the actual value used for sparsification\n",
        "\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = torch.reshape(input, [-1])\n",
        "\n",
        "        #print(\"Flat input: \",flat_input)\n",
        "        #print(\"Input size\",flat_input.size())\n",
        "\n",
        "        # Get values and index tensor of chosen components (Set dim=-1 for testing)\n",
        "        values, indices = torch.topk(flat_input, K,dim=-1)  #flat_input instead of input\n",
        "\n",
        "        # Added self to signq input\n",
        "\n",
        "        # torch.zeros() on cpu not cuda, use torch.zeros_like() with memory_format=torch.preserve_format as default\n",
        "        # flat_quantized = torch.zeros(flat_input.size()).scatter(0, indices, signq(self,values))\n",
        "\n",
        "\n",
        "        flat_quantized=torch.zeros_like(flat_input).scatter(0, indices, signq(self,values))\n",
        "        quantized = torch.reshape(flat_quantized, shape=org_shape)\n",
        "\n",
        "        return quantized\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        self.step_id += 1\n",
        "        for group_id, group in enumerate(self.param_groups):\n",
        "\n",
        "            lr = group[\"lr\"]\n",
        "\n",
        "            for param_id, param in enumerate(group[\"params\"]):\n",
        "                state = self.state[param]\n",
        "\n",
        "                state[\"local\"].add_(param.grad, alpha=-lr)\n",
        "\n",
        "                #  If synchronization in this round\n",
        "                if self.schedule is None or self.step_id in self.schedule:\n",
        "                    # Calculate quantized gradient\n",
        "                    state[\"qsl_grad\"] = self.qsp(state[\"memory\"] + state[\"global\"] - state[\"local\"])\n",
        "\n",
        "                    # Update memory\n",
        "                    state[\"memory\"].add_(state[\"global\"]).add_(-state[\"local\"]).add_(-state[\"qsl_grad\"])\n",
        "\n",
        "                # Local weights equal to local model\n",
        "                # Step 5\n",
        "                param.data=state[\"local\"]\n",
        "\n",
        "\n",
        "class QSparseLocalAlgorithmImpl(AlgorithmImpl):\n",
        "    def __init__(\n",
        "            self,\n",
        "            process_group: BaguaProcessGroup,\n",
        "            q_sparse_local_optimizer: QSparseLocalOptimizer,\n",
        "            hierarchical: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implementation of the\n",
        "        `QSparseLocal Algorithm <https://tutorials.baguasys.com/algorithms/q-adam>`_\n",
        "        .\n",
        "\n",
        "        Args:\n",
        "            process_group: The process group to work on.\n",
        "            q_sparse_local_optimizer: A QSparseLocalOptimizer initialized with model parameters.\n",
        "            hierarchical: Enable hierarchical communication.\n",
        "        \"\"\"\n",
        "        super(QSparseLocalAlgorithmImpl, self).__init__(process_group)\n",
        "        self.hierarchical = hierarchical\n",
        "        self.optimizer = q_sparse_local_optimizer\n",
        "        self.schedule = self.optimizer.schedule\n",
        "        \n",
        "\n",
        "    #def need_reset(self):\n",
        "    #    return True\n",
        "\n",
        "    def init_tensors(self, bagua_distributed_data_parallel: BaguaDistributedDataParallel):\n",
        "        parameters =  bagua_distributed_data_parallel.bagua_build_params()\n",
        "\n",
        "        for idx, (name, param) in enumerate(parameters.__reversed__()):\n",
        "            param._q_sparse_local_name = name\n",
        "            param._q_sparse_local_idx = idx\n",
        "\n",
        "        tensor_groups = []\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for param in group[\"params\"]:\n",
        "                if self.schedule is None or self.optimizer.step_id in self.schedule:\n",
        "\n",
        "                    # Second half Step 4\n",
        "                    # register local and global parameter vector\n",
        "\n",
        "                    def set_weights(param, t):\n",
        "                        # Gradient is subtracted from global and local model\n",
        "\n",
        "                        self.optimizer.state[param][\"qsl_grad\"] = t\n",
        "\n",
        "                    registered_tensor = param.bagua_ensure_grad().ensure_bagua_tensor(\n",
        "                        param._q_sparse_local_name,\n",
        "                        bagua_distributed_data_parallel.bagua_module_name,\n",
        "                        getter_closure=lambda param: self.optimizer.state[param][\"qsl_grad\"],\n",
        "                        setter_closure=set_weights,\n",
        "                    )\n",
        "                    tensor_groups.append(registered_tensor)\n",
        "                else:\n",
        "                    # Nothing happens\n",
        "                    pass\n",
        "\n",
        "        tensor_groups.sort(key=lambda x: x._q_sparse_local_idx)\n",
        "        return tensor_groups\n",
        "\n",
        "    def tensors_to_buckets(\n",
        "            self, tensors: List[List[BaguaTensor]], do_flatten: bool\n",
        "    ) -> List[BaguaBucket]:\n",
        "        bagua_buckets = []\n",
        "        #print(\"Tensor qsparselocal:\\n\\n\",tensors)\n",
        "        #print(\"\\n\\nTensor shape: \",tensors.shape())\n",
        "        #print(\"Attributes:\\n\\n\", dir(tensors[0][0]))\n",
        "        #print(\"\\n\\nLength\\n\\n\", len(tensors[0][0]))\n",
        "        #print(\"\\n\\nLength\\n\\n\", len(tensors[0][1]))\n",
        "        #print(\"\\n\\nLength\\n\\n\", len(tensors[0][2]))\n",
        "        #print(\"\\n\\nLength\\n\\n\", len(tensors[0][3]))\n",
        "        #print(\"\\n\\nDatapointer: \", tensors[0][0].data_ptr)\n",
        "        #print(\"\\n\\nDatapointer: \", tensors[0][1].data_ptr)\n",
        "        #print(\"\\n\\nDatapointer: \", tensors[0][2].data_ptr)\n",
        "        #print(\"\\n\\nDatapointer: \", tensors[0][3].data_ptr)\n",
        "        for idx, bucket in enumerate(tensors):\n",
        "            bagua_bucket = BaguaBucket(\n",
        "                bucket,\n",
        "                flatten=do_flatten,\n",
        "                #flatten=True,\n",
        "                name=str(idx),\n",
        "                alignment=self.process_group.get_global_communicator().nranks(),\n",
        "            )\n",
        "            bagua_buckets.append(bagua_bucket)\n",
        "        return bagua_buckets\n",
        "\n",
        "    def init_operations(\n",
        "            self,\n",
        "             bagua_distributed_data_parallel: BaguaDistributedDataParallel,\n",
        "            bucket: BaguaBucket,\n",
        "    ):\n",
        "        bucket.clear_ops()\n",
        "        if self.schedule is None or self.optimizer.step_id in self.schedule:\n",
        "            ##### Step 2\n",
        "            def set_weights(*args):\n",
        "                for tensor in bucket.tensors:\n",
        "                    self.optimizer.state[\"global\"].add_(-tensor.bagua_getter_closure())\n",
        "                    self.optimizer.state[\"local\"] = self.optimizer.state[\"global\"]\n",
        "            \n",
        "            bucket.append_python_op(set_weights, group=self.process_group)\n",
        "\n",
        "            ##### Step 3\n",
        "            bucket.append_centralized_synchronous_op(\n",
        "                hierarchical=self.hierarchical,\n",
        "                average=True,\n",
        "                scattergather=True,\n",
        "                compression=\"MinMaxUInt8\",\n",
        "                group=self.process_group,\n",
        "            )\n",
        "        else: # Nothing happens\n",
        "            pass\n",
        "\n",
        "\n",
        "    # Instead of momentum hook, we use a qsl_gradient hook\n",
        "    def init_backward_hook(self,  bagua_distributed_data_parallel: BaguaDistributedDataParallel):\n",
        "\n",
        "        def hook_qsl_grad(parameter_name, parameter):\n",
        "            assert (\n",
        "                    parameter.bagua_backend_tensor().data_ptr()\n",
        "                    == self.optimizer.state[parameter][\"qsl_grad\"].data_ptr()\n",
        "            ), \"bagua backend tensor data_ptr should match _q_sparse_local_grad data_ptr\"\n",
        "            parameter.bagua_mark_communication_ready()\n",
        "\n",
        "        return hook_qsl_grad\n",
        "\n",
        "\n",
        "class QSparseLocalAlgorithm(Algorithm):\n",
        "    def __init__(self, q_sparse_local_optimizer: QSparseLocalOptimizer, hierarchical: bool = True):\n",
        "        \"\"\"\n",
        "        Create an instance of the\n",
        "        `QSparseLocal Algorithm <https://tutorials.baguasys.com/algorithms/q-adam>`_\n",
        "        .\n",
        "\n",
        "        Args:\n",
        "            q_sparse_local_optimizer: A QSparseLocalOptimizer initialized with model parameters.\n",
        "            hierarchical: Enable hierarchical communication.\n",
        "        \"\"\"\n",
        "        self.hierarchical = hierarchical\n",
        "        self.optimizer = q_sparse_local_optimizer\n",
        "\n",
        "    def reify(self, process_group: BaguaProcessGroup) -> QSparseLocalAlgorithmImpl:\n",
        "        return QSparseLocalAlgorithmImpl(\n",
        "            process_group,\n",
        "            q_sparse_local_optimizer=self.optimizer,\n",
        "            hierarchical=self.hierarchical,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "_TpSXZaAd8r8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MNIST example**"
      ],
      "metadata": {
        "id": "mPIWQ8CvcZ0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_list=[]\n",
        "loss_list=[]\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import logging\n",
        "import bagua.torch_api as bagua\n",
        "import bagua_core \n",
        "\n",
        "bagua_core.install_deps()\n",
        "\n",
        "\n",
        "# Model for Neural Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "#------Training Model\n",
        "def train(args, model, train_loader, optimizer, epoch):\n",
        "    #??????? What does model.train() do exactly?\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        #data: features , target: label\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        #??????? Optimizer calculates weights where gradient == 0\n",
        "        optimizer.zero_grad()\n",
        "        # Calculates predicted labels by using the model\n",
        "        output = model(data)\n",
        "        # Loss function using predicted labels and actual labels\n",
        "        loss = F.nll_loss(output, target)\n",
        "        # Backwards propagation    !!!calculates tensor loss gradient \n",
        "        loss.backward()\n",
        "        # Optimizer step selection\n",
        "        if args.fuse_optimizer:\n",
        "            optimizer.fuse_step()\n",
        "        else:\n",
        "            optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            logging.info(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(\n",
        "                output, target, reduction=\"sum\"\n",
        "            ).item()  # sum up batch loss\n",
        "            pred = output.argmax(\n",
        "                dim=1, keepdim=True\n",
        "            )  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    logging.info(\n",
        "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss,\n",
        "            correct,\n",
        "            len(test_loader.dataset),\n",
        "            100.0 * correct / len(test_loader.dataset),\n",
        "        )\n",
        "    )\n",
        "    return test_loss,correct\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description=\"PyTorch MNIST Example\")\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=64,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size for training (default: 64)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--test-batch-size\",\n",
        "        type=int,\n",
        "        default=1000,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size for testing (default: 1000)\",\n",
        "    )\n",
        "\n",
        "# set number of epochs here\n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        type=int,\n",
        "        default=14,\n",
        "        metavar=\"N\",\n",
        "        help=\"number of epochs to train (default: 14)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lr\",\n",
        "        type=float,\n",
        "        default=1.0,\n",
        "        metavar=\"LR\",\n",
        "        help=\"learning rate (default: 1.0)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gamma\",\n",
        "        type=float,\n",
        "        default=0.7,\n",
        "        metavar=\"M\",\n",
        "        help=\"Learning rate step gamma (default: 0.7)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--log-interval\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"N\",\n",
        "        help=\"how many batches to wait before logging training status\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--save-model\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"For Saving the current Model\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--algorithm\",\n",
        "        type=str,\n",
        "        default=\"qsparselocal\",\n",
        "        help=\"gradient_allreduce, bytegrad, decentralized, low_precision_decentralized, qadam, async\",\n",
        "        #Add new algorithm for testing------------------\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--async-sync-interval\",\n",
        "        default=500,\n",
        "        type=int,\n",
        "        help=\"Model synchronization interval(ms) for async algorithm\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--set-deterministic\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"set deterministic or not\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fuse-optimizer\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"fuse optimizer or not\",\n",
        "    )\n",
        "\n",
        "    #args = parser.parse_args() \n",
        "    # New line below solves ipykernel_launcher.py: error: unrecognized arguments\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    \n",
        "    if args.set_deterministic:\n",
        "        print(\"set_deterministic: True\")\n",
        "        np.random.seed(666)\n",
        "        random.seed(666)\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.manual_seed(666)\n",
        "        torch.cuda.manual_seed_all(666 + int(bagua.get_rank()))\n",
        "        torch.set_printoptions(precision=10)\n",
        "\n",
        "    torch.cuda.set_device(bagua.get_local_rank())\n",
        "    bagua.init_process_group()\n",
        "\n",
        "\n",
        "\n",
        "    logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.ERROR)\n",
        "    if bagua.get_rank() == 0:\n",
        "        logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    train_kwargs = {\"batch_size\": args.batch_size}\n",
        "    test_kwargs = {\"batch_size\": args.test_batch_size}\n",
        "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        "    )\n",
        "\n",
        "    if bagua.get_local_rank() == 0:\n",
        "        dataset1 = datasets.MNIST(\n",
        "            \"../data\", train=True, download=True, transform=transform\n",
        "        )\n",
        "        torch.distributed.barrier()\n",
        "    else:\n",
        "        torch.distributed.barrier()\n",
        "        dataset1 = datasets.MNIST(\n",
        "            \"../data\", train=True, download=True, transform=transform\n",
        "        )\n",
        "\n",
        "    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        dataset1, num_replicas=bagua.get_world_size(), rank=bagua.get_rank()\n",
        "    )\n",
        "    train_kwargs.update(\n",
        "        {\n",
        "            \"sampler\": train_sampler,\n",
        "            \"batch_size\": args.batch_size // bagua.get_world_size(),\n",
        "            \"shuffle\": False,\n",
        "        }\n",
        "    )\n",
        "    # Train and Test dataset\n",
        "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "    # ??????????\n",
        "    model = Net().cuda()\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "    if args.algorithm == \"gradient_allreduce\":\n",
        "        from bagua.torch_api.algorithms import gradient_allreduce\n",
        "\n",
        "        algorithm = gradient_allreduce.GradientAllReduceAlgorithm()\n",
        "    elif args.algorithm == \"decentralized\":\n",
        "        from bagua.torch_api.algorithms import decentralized\n",
        "\n",
        "        algorithm = decentralized.DecentralizedAlgorithm()\n",
        "    elif args.algorithm == \"low_precision_decentralized\":\n",
        "        from bagua.torch_api.algorithms import decentralized\n",
        "\n",
        "        algorithm = decentralized.LowPrecisionDecentralizedAlgorithm()\n",
        "    elif args.algorithm == \"bytegrad\":\n",
        "        from bagua.torch_api.algorithms import bytegrad\n",
        "\n",
        "        algorithm = bytegrad.ByteGradAlgorithm()\n",
        "    elif args.algorithm == \"qadam\":\n",
        "        from bagua.torch_api.algorithms import q_adam\n",
        "\n",
        "        optimizer = q_adam.QAdamOptimizer(\n",
        "            model.parameters(), lr=args.lr, warmup_steps=100\n",
        "        )\n",
        "        algorithm = q_adam.QAdamAlgorithm(optimizer)\n",
        "    elif args.algorithm == \"qsparselocal\":\n",
        "      # Set lower learning rate, no convergence for lr = 1\n",
        "        optimizer = QSparseLocalOptimizer(\n",
        "            model.parameters(), lr=0.0001\n",
        "        )\n",
        "        algorithm = QSparseLocalAlgorithm(optimizer)\n",
        "    elif args.algorithm == \"async\":\n",
        "        from bagua.torch_api.algorithms import async_model_average\n",
        "\n",
        "        algorithm = async_model_average.AsyncModelAverageAlgorithm(\n",
        "            sync_interval_ms=args.async_sync_interval,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    #  Model von Bagua\n",
        "    model = model.with_bagua(\n",
        "        [optimizer],\n",
        "        algorithm,\n",
        "        do_flatten=not args.fuse_optimizer,\n",
        "    )\n",
        "\n",
        "    # Optimizer from Bagua if args.fuse_optimizer==True\n",
        "    if args.fuse_optimizer:\n",
        "        optimizer = bagua.contrib.fuse_optimizer(optimizer)\n",
        "\n",
        "    #------------ Loss, accuracy\n",
        "    loss_list =[]\n",
        "    acc_list = []\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        if args.algorithm == \"async\":\n",
        "            model.bagua_algorithm.resume(model)\n",
        "\n",
        "        train(args, model, train_loader, optimizer, epoch)\n",
        "\n",
        "        if args.algorithm == \"async\":\n",
        "            model.bagua_algorithm.abort(model)\n",
        "\n",
        "        new_loss,new_acc =test(model, test_loader)\n",
        "        loss_list.append(new_loss)\n",
        "        acc_list.append(new_acc/100.0)\n",
        "        scheduler.step()\n",
        "\n",
        "    if args.save_model:\n",
        "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    ep =[]\n",
        "\n",
        "\n",
        "    for i in range(1, args.epochs + 1):\n",
        "      ep.append(i)\n",
        "\n",
        "    print(loss_list)\n",
        "    print(acc_list)\n",
        "    \n",
        "     \n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.subplot(211)\n",
        "    plt.plot(ep,loss_list)\n",
        "    plt.subplot(212)\n",
        "    plt.plot(ep,acc_list)\n",
        "\n",
        "    plt.show()  \n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9tjsC05-5qV_",
        "outputId": "fdd79fa1-9974-4144-f50d-dc5915287f88"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc_version:  11.1\n",
            "The destination directory /root/.local/share/bagua/nccl already exists.\n",
            "Installing nccl 2.10.3 for CUDA 11.1 to: /root/.local/share/bagua/nccl\n",
            "Downloading https://developer.download.nvidia.com/compute/redist/nccl/v2.10/nccl_2.10.3-1+cuda11.0_x86_64.txz...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nccl_2.10.3-1+cuda11.0_x86_64.txz: 138MB [00:00, 200MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting...\n",
            "Installing...\n",
            "Cleaning up...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gevent/threadpool.py\", line 157, in _before_run_task\n",
            "    _sys.settrace(_get_thread_trace())\n",
            "\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gevent/threadpool.py\", line 162, in _after_run_task\n",
            "    _sys.settrace(None)\n",
            "\n",
            "INFO:root:Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.314644\n",
            "INFO:root:Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.302578\n",
            "INFO:root:Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.302585\n",
            "INFO:root:Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.302580\n",
            "INFO:root:Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.302581\n",
            "INFO:root:Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.302575\n",
            "INFO:root:Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.302580\n",
            "INFO:root:Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.302559\n",
            "INFO:root:Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.302581\n",
            "INFO:root:Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.302589\n",
            "INFO:root:Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.302566\n",
            "INFO:root:Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.302571\n",
            "INFO:root:Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.302577\n",
            "INFO:root:Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.302580\n",
            "INFO:root:Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.302547\n",
            "INFO:root:Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.302548\n",
            "INFO:root:Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.302607\n",
            "INFO:root:Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.302578\n",
            "INFO:root:Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.302577\n",
            "INFO:root:Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.302597\n",
            "INFO:root:Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.302602\n",
            "INFO:root:Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.302526\n",
            "INFO:root:Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.302608\n",
            "INFO:root:Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.302569\n",
            "INFO:root:Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.302601\n",
            "INFO:root:Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.302609\n",
            "INFO:root:Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.302580\n",
            "INFO:root:Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.302594\n",
            "INFO:root:Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.302552\n",
            "INFO:root:Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.302613\n",
            "INFO:root:Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.302574\n",
            "INFO:root:Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.302537\n",
            "INFO:root:Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.302586\n",
            "INFO:root:Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.302541\n",
            "INFO:root:Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.302555\n",
            "INFO:root:Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.302554\n",
            "INFO:root:Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.302546\n",
            "INFO:root:Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.302552\n",
            "INFO:root:Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.302545\n",
            "INFO:root:Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.302620\n",
            "INFO:root:Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.302612\n",
            "INFO:root:Train Epoch: 1 [29440/60000 (49%)]\tLoss: 2.302556\n",
            "INFO:root:Train Epoch: 1 [30080/60000 (50%)]\tLoss: 2.302554\n",
            "INFO:root:Train Epoch: 1 [30720/60000 (51%)]\tLoss: 2.302576\n",
            "INFO:root:Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.302596\n",
            "INFO:root:Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 1 [32640/60000 (54%)]\tLoss: 2.302546\n",
            "INFO:root:Train Epoch: 1 [33280/60000 (55%)]\tLoss: 2.302620\n",
            "INFO:root:Train Epoch: 1 [33920/60000 (57%)]\tLoss: 2.302530\n",
            "INFO:root:Train Epoch: 1 [34560/60000 (58%)]\tLoss: 2.302618\n",
            "INFO:root:Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.302572\n",
            "INFO:root:Train Epoch: 1 [35840/60000 (60%)]\tLoss: 2.302525\n",
            "INFO:root:Train Epoch: 1 [36480/60000 (61%)]\tLoss: 2.302582\n",
            "INFO:root:Train Epoch: 1 [37120/60000 (62%)]\tLoss: 2.302595\n",
            "INFO:root:Train Epoch: 1 [37760/60000 (63%)]\tLoss: 2.302503\n",
            "INFO:root:Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.302623\n",
            "INFO:root:Train Epoch: 1 [39040/60000 (65%)]\tLoss: 2.302581\n",
            "INFO:root:Train Epoch: 1 [39680/60000 (66%)]\tLoss: 2.302565\n",
            "INFO:root:Train Epoch: 1 [40320/60000 (67%)]\tLoss: 2.302565\n",
            "INFO:root:Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.302579\n",
            "INFO:root:Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.302600\n",
            "INFO:root:Train Epoch: 1 [42240/60000 (70%)]\tLoss: 2.302573\n",
            "INFO:root:Train Epoch: 1 [42880/60000 (71%)]\tLoss: 2.302572\n",
            "INFO:root:Train Epoch: 1 [43520/60000 (72%)]\tLoss: 2.302525\n",
            "INFO:root:Train Epoch: 1 [44160/60000 (74%)]\tLoss: 2.302492\n",
            "INFO:root:Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.302459\n",
            "INFO:root:Train Epoch: 1 [45440/60000 (76%)]\tLoss: 2.302530\n",
            "INFO:root:Train Epoch: 1 [46080/60000 (77%)]\tLoss: 2.302561\n",
            "INFO:root:Train Epoch: 1 [46720/60000 (78%)]\tLoss: 2.302592\n",
            "INFO:root:Train Epoch: 1 [47360/60000 (79%)]\tLoss: 2.302539\n",
            "INFO:root:Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2.302568\n",
            "INFO:root:Train Epoch: 1 [48640/60000 (81%)]\tLoss: 2.302570\n",
            "INFO:root:Train Epoch: 1 [49280/60000 (82%)]\tLoss: 2.302397\n",
            "INFO:root:Train Epoch: 1 [49920/60000 (83%)]\tLoss: 2.302446\n",
            "INFO:root:Train Epoch: 1 [50560/60000 (84%)]\tLoss: 2.302513\n",
            "INFO:root:Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.302569\n",
            "INFO:root:Train Epoch: 1 [51840/60000 (86%)]\tLoss: 2.302553\n",
            "INFO:root:Train Epoch: 1 [52480/60000 (87%)]\tLoss: 2.302519\n",
            "INFO:root:Train Epoch: 1 [53120/60000 (88%)]\tLoss: 2.302538\n",
            "INFO:root:Train Epoch: 1 [53760/60000 (90%)]\tLoss: 2.302531\n",
            "INFO:root:Train Epoch: 1 [54400/60000 (91%)]\tLoss: 2.302600\n",
            "INFO:root:Train Epoch: 1 [55040/60000 (92%)]\tLoss: 2.302631\n",
            "INFO:root:Train Epoch: 1 [55680/60000 (93%)]\tLoss: 2.302632\n",
            "INFO:root:Train Epoch: 1 [56320/60000 (94%)]\tLoss: 2.302634\n",
            "INFO:root:Train Epoch: 1 [56960/60000 (95%)]\tLoss: 2.302598\n",
            "INFO:root:Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.302504\n",
            "INFO:root:Train Epoch: 1 [58240/60000 (97%)]\tLoss: 2.302534\n",
            "INFO:root:Train Epoch: 1 [58880/60000 (98%)]\tLoss: 2.302549\n",
            "INFO:root:Train Epoch: 1 [59520/60000 (99%)]\tLoss: 2.302564\n",
            "INFO:root:\n",
            "Test set: Average loss: 2.3026, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "INFO:root:Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.302567\n",
            "INFO:root:Train Epoch: 2 [640/60000 (1%)]\tLoss: 2.302551\n",
            "INFO:root:Train Epoch: 2 [1280/60000 (2%)]\tLoss: 2.302524\n",
            "INFO:root:Train Epoch: 2 [1920/60000 (3%)]\tLoss: 2.302476\n",
            "INFO:root:Train Epoch: 2 [2560/60000 (4%)]\tLoss: 2.302568\n",
            "INFO:root:Train Epoch: 2 [3200/60000 (5%)]\tLoss: 2.302591\n",
            "INFO:root:Train Epoch: 2 [3840/60000 (6%)]\tLoss: 2.302584\n",
            "INFO:root:Train Epoch: 2 [4480/60000 (7%)]\tLoss: 2.302539\n",
            "INFO:root:Train Epoch: 2 [5120/60000 (9%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 2 [5760/60000 (10%)]\tLoss: 2.302514\n",
            "INFO:root:Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.302548\n",
            "INFO:root:Train Epoch: 2 [7040/60000 (12%)]\tLoss: 2.302526\n",
            "INFO:root:Train Epoch: 2 [7680/60000 (13%)]\tLoss: 2.302421\n",
            "INFO:root:Train Epoch: 2 [8320/60000 (14%)]\tLoss: 2.302579\n",
            "INFO:root:Train Epoch: 2 [8960/60000 (15%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 2 [9600/60000 (16%)]\tLoss: 2.302472\n",
            "INFO:root:Train Epoch: 2 [10240/60000 (17%)]\tLoss: 2.302504\n",
            "INFO:root:Train Epoch: 2 [10880/60000 (18%)]\tLoss: 2.302559\n",
            "INFO:root:Train Epoch: 2 [11520/60000 (19%)]\tLoss: 2.302542\n",
            "INFO:root:Train Epoch: 2 [12160/60000 (20%)]\tLoss: 2.302419\n",
            "INFO:root:Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.302446\n",
            "INFO:root:Train Epoch: 2 [13440/60000 (22%)]\tLoss: 2.302650\n",
            "INFO:root:Train Epoch: 2 [14080/60000 (23%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 2 [14720/60000 (25%)]\tLoss: 2.302554\n",
            "INFO:root:Train Epoch: 2 [15360/60000 (26%)]\tLoss: 2.302619\n",
            "INFO:root:Train Epoch: 2 [16000/60000 (27%)]\tLoss: 2.302660\n",
            "INFO:root:Train Epoch: 2 [16640/60000 (28%)]\tLoss: 2.302389\n",
            "INFO:root:Train Epoch: 2 [17280/60000 (29%)]\tLoss: 2.302675\n",
            "INFO:root:Train Epoch: 2 [17920/60000 (30%)]\tLoss: 2.302532\n",
            "INFO:root:Train Epoch: 2 [18560/60000 (31%)]\tLoss: 2.302605\n",
            "INFO:root:Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.302639\n",
            "INFO:root:Train Epoch: 2 [19840/60000 (33%)]\tLoss: 2.302585\n",
            "INFO:root:Train Epoch: 2 [20480/60000 (34%)]\tLoss: 2.302639\n",
            "INFO:root:Train Epoch: 2 [21120/60000 (35%)]\tLoss: 2.302510\n",
            "INFO:root:Train Epoch: 2 [21760/60000 (36%)]\tLoss: 2.302647\n",
            "INFO:root:Train Epoch: 2 [22400/60000 (37%)]\tLoss: 2.302549\n",
            "INFO:root:Train Epoch: 2 [23040/60000 (38%)]\tLoss: 2.302443\n",
            "INFO:root:Train Epoch: 2 [23680/60000 (39%)]\tLoss: 2.302582\n",
            "INFO:root:Train Epoch: 2 [24320/60000 (41%)]\tLoss: 2.302445\n",
            "INFO:root:Train Epoch: 2 [24960/60000 (42%)]\tLoss: 2.302505\n",
            "INFO:root:Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.302520\n",
            "INFO:root:Train Epoch: 2 [26240/60000 (44%)]\tLoss: 2.302487\n",
            "INFO:root:Train Epoch: 2 [26880/60000 (45%)]\tLoss: 2.302491\n",
            "INFO:root:Train Epoch: 2 [27520/60000 (46%)]\tLoss: 2.302470\n",
            "INFO:root:Train Epoch: 2 [28160/60000 (47%)]\tLoss: 2.302680\n",
            "INFO:root:Train Epoch: 2 [28800/60000 (48%)]\tLoss: 2.302660\n",
            "INFO:root:Train Epoch: 2 [29440/60000 (49%)]\tLoss: 2.302501\n",
            "INFO:root:Train Epoch: 2 [30080/60000 (50%)]\tLoss: 2.302508\n",
            "INFO:root:Train Epoch: 2 [30720/60000 (51%)]\tLoss: 2.302543\n",
            "INFO:root:Train Epoch: 2 [31360/60000 (52%)]\tLoss: 2.302616\n",
            "INFO:root:Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 2 [32640/60000 (54%)]\tLoss: 2.302486\n",
            "INFO:root:Train Epoch: 2 [33280/60000 (55%)]\tLoss: 2.302662\n",
            "INFO:root:Train Epoch: 2 [33920/60000 (57%)]\tLoss: 2.302467\n",
            "INFO:root:Train Epoch: 2 [34560/60000 (58%)]\tLoss: 2.302653\n",
            "INFO:root:Train Epoch: 2 [35200/60000 (59%)]\tLoss: 2.302552\n",
            "INFO:root:Train Epoch: 2 [35840/60000 (60%)]\tLoss: 2.302452\n",
            "INFO:root:Train Epoch: 2 [36480/60000 (61%)]\tLoss: 2.302575\n",
            "INFO:root:Train Epoch: 2 [37120/60000 (62%)]\tLoss: 2.302619\n",
            "INFO:root:Train Epoch: 2 [37760/60000 (63%)]\tLoss: 2.302410\n",
            "INFO:root:Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.302652\n",
            "INFO:root:Train Epoch: 2 [39040/60000 (65%)]\tLoss: 2.302579\n",
            "INFO:root:Train Epoch: 2 [39680/60000 (66%)]\tLoss: 2.302536\n",
            "INFO:root:Train Epoch: 2 [40320/60000 (67%)]\tLoss: 2.302527\n",
            "INFO:root:Train Epoch: 2 [40960/60000 (68%)]\tLoss: 2.302584\n",
            "INFO:root:Train Epoch: 2 [41600/60000 (69%)]\tLoss: 2.302620\n",
            "INFO:root:Train Epoch: 2 [42240/60000 (70%)]\tLoss: 2.302555\n",
            "INFO:root:Train Epoch: 2 [42880/60000 (71%)]\tLoss: 2.302567\n",
            "INFO:root:Train Epoch: 2 [43520/60000 (72%)]\tLoss: 2.302473\n",
            "INFO:root:Train Epoch: 2 [44160/60000 (74%)]\tLoss: 2.302381\n",
            "INFO:root:Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.302344\n",
            "INFO:root:Train Epoch: 2 [45440/60000 (76%)]\tLoss: 2.302481\n",
            "INFO:root:Train Epoch: 2 [46080/60000 (77%)]\tLoss: 2.302549\n",
            "INFO:root:Train Epoch: 2 [46720/60000 (78%)]\tLoss: 2.302608\n",
            "INFO:root:Train Epoch: 2 [47360/60000 (79%)]\tLoss: 2.302483\n",
            "INFO:root:Train Epoch: 2 [48000/60000 (80%)]\tLoss: 2.302536\n",
            "INFO:root:Train Epoch: 2 [48640/60000 (81%)]\tLoss: 2.302546\n",
            "INFO:root:Train Epoch: 2 [49280/60000 (82%)]\tLoss: 2.302231\n",
            "INFO:root:Train Epoch: 2 [49920/60000 (83%)]\tLoss: 2.302323\n",
            "INFO:root:Train Epoch: 2 [50560/60000 (84%)]\tLoss: 2.302445\n",
            "INFO:root:Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.302551\n",
            "INFO:root:Train Epoch: 2 [51840/60000 (86%)]\tLoss: 2.302524\n",
            "INFO:root:Train Epoch: 2 [52480/60000 (87%)]\tLoss: 2.302459\n",
            "INFO:root:Train Epoch: 2 [53120/60000 (88%)]\tLoss: 2.302502\n",
            "INFO:root:Train Epoch: 2 [53760/60000 (90%)]\tLoss: 2.302497\n",
            "INFO:root:Train Epoch: 2 [54400/60000 (91%)]\tLoss: 2.302608\n",
            "INFO:root:Train Epoch: 2 [55040/60000 (92%)]\tLoss: 2.302664\n",
            "INFO:root:Train Epoch: 2 [55680/60000 (93%)]\tLoss: 2.302661\n",
            "INFO:root:Train Epoch: 2 [56320/60000 (94%)]\tLoss: 2.302662\n",
            "INFO:root:Train Epoch: 2 [56960/60000 (95%)]\tLoss: 2.302605\n",
            "INFO:root:Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.302442\n",
            "INFO:root:Train Epoch: 2 [58240/60000 (97%)]\tLoss: 2.302500\n",
            "INFO:root:Train Epoch: 2 [58880/60000 (98%)]\tLoss: 2.302524\n",
            "INFO:root:Train Epoch: 2 [59520/60000 (99%)]\tLoss: 2.302551\n",
            "INFO:root:\n",
            "Test set: Average loss: 2.3025, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "INFO:root:Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.302556\n",
            "INFO:root:Train Epoch: 3 [640/60000 (1%)]\tLoss: 2.302529\n",
            "INFO:root:Train Epoch: 3 [1280/60000 (2%)]\tLoss: 2.302482\n",
            "INFO:root:Train Epoch: 3 [1920/60000 (3%)]\tLoss: 2.302405\n",
            "INFO:root:Train Epoch: 3 [2560/60000 (4%)]\tLoss: 2.302557\n",
            "INFO:root:Train Epoch: 3 [3200/60000 (5%)]\tLoss: 2.302593\n",
            "INFO:root:Train Epoch: 3 [3840/60000 (6%)]\tLoss: 2.302582\n",
            "INFO:root:Train Epoch: 3 [4480/60000 (7%)]\tLoss: 2.302511\n",
            "INFO:root:Train Epoch: 3 [5120/60000 (9%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 3 [5760/60000 (10%)]\tLoss: 2.302472\n",
            "INFO:root:Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.302524\n",
            "INFO:root:Train Epoch: 3 [7040/60000 (12%)]\tLoss: 2.302489\n",
            "INFO:root:Train Epoch: 3 [7680/60000 (13%)]\tLoss: 2.302326\n",
            "INFO:root:Train Epoch: 3 [8320/60000 (14%)]\tLoss: 2.302579\n",
            "INFO:root:Train Epoch: 3 [8960/60000 (15%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 3 [9600/60000 (16%)]\tLoss: 2.302407\n",
            "INFO:root:Train Epoch: 3 [10240/60000 (17%)]\tLoss: 2.302459\n",
            "INFO:root:Train Epoch: 3 [10880/60000 (18%)]\tLoss: 2.302546\n",
            "INFO:root:Train Epoch: 3 [11520/60000 (19%)]\tLoss: 2.302517\n",
            "INFO:root:Train Epoch: 3 [12160/60000 (20%)]\tLoss: 2.302330\n",
            "INFO:root:Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.302375\n",
            "INFO:root:Train Epoch: 3 [13440/60000 (22%)]\tLoss: 2.302681\n",
            "INFO:root:Train Epoch: 3 [14080/60000 (23%)]\tLoss: 2.302587\n",
            "INFO:root:Train Epoch: 3 [14720/60000 (25%)]\tLoss: 2.302537\n",
            "INFO:root:Train Epoch: 3 [15360/60000 (26%)]\tLoss: 2.302634\n",
            "INFO:root:Train Epoch: 3 [16000/60000 (27%)]\tLoss: 2.302701\n",
            "INFO:root:Train Epoch: 3 [16640/60000 (28%)]\tLoss: 2.302295\n",
            "INFO:root:Train Epoch: 3 [17280/60000 (29%)]\tLoss: 2.302722\n",
            "INFO:root:Train Epoch: 3 [17920/60000 (30%)]\tLoss: 2.302505\n",
            "INFO:root:Train Epoch: 3 [18560/60000 (31%)]\tLoss: 2.302607\n",
            "INFO:root:Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.302660\n",
            "INFO:root:Train Epoch: 3 [19840/60000 (33%)]\tLoss: 2.302590\n",
            "INFO:root:Train Epoch: 3 [20480/60000 (34%)]\tLoss: 2.302670\n",
            "INFO:root:Train Epoch: 3 [21120/60000 (35%)]\tLoss: 2.302480\n",
            "INFO:root:Train Epoch: 3 [21760/60000 (36%)]\tLoss: 2.302671\n",
            "INFO:root:Train Epoch: 3 [22400/60000 (37%)]\tLoss: 2.302531\n",
            "INFO:root:Train Epoch: 3 [23040/60000 (38%)]\tLoss: 2.302377\n",
            "INFO:root:Train Epoch: 3 [23680/60000 (39%)]\tLoss: 2.302578\n",
            "INFO:root:Train Epoch: 3 [24320/60000 (41%)]\tLoss: 2.302379\n",
            "INFO:root:Train Epoch: 3 [24960/60000 (42%)]\tLoss: 2.302471\n",
            "INFO:root:Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.302496\n",
            "INFO:root:Train Epoch: 3 [26240/60000 (44%)]\tLoss: 2.302447\n",
            "INFO:root:Train Epoch: 3 [26880/60000 (45%)]\tLoss: 2.302447\n",
            "INFO:root:Train Epoch: 3 [27520/60000 (46%)]\tLoss: 2.302418\n",
            "INFO:root:Train Epoch: 3 [28160/60000 (47%)]\tLoss: 2.302722\n",
            "INFO:root:Train Epoch: 3 [28800/60000 (48%)]\tLoss: 2.302694\n",
            "INFO:root:Train Epoch: 3 [29440/60000 (49%)]\tLoss: 2.302462\n",
            "INFO:root:Train Epoch: 3 [30080/60000 (50%)]\tLoss: 2.302475\n",
            "INFO:root:Train Epoch: 3 [30720/60000 (51%)]\tLoss: 2.302520\n",
            "INFO:root:Train Epoch: 3 [31360/60000 (52%)]\tLoss: 2.302631\n",
            "INFO:root:Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 3 [32640/60000 (54%)]\tLoss: 2.302445\n",
            "INFO:root:Train Epoch: 3 [33280/60000 (55%)]\tLoss: 2.302692\n",
            "INFO:root:Train Epoch: 3 [33920/60000 (57%)]\tLoss: 2.302423\n",
            "INFO:root:Train Epoch: 3 [34560/60000 (58%)]\tLoss: 2.302677\n",
            "INFO:root:Train Epoch: 3 [35200/60000 (59%)]\tLoss: 2.302537\n",
            "INFO:root:Train Epoch: 3 [35840/60000 (60%)]\tLoss: 2.302402\n",
            "INFO:root:Train Epoch: 3 [36480/60000 (61%)]\tLoss: 2.302571\n",
            "INFO:root:Train Epoch: 3 [37120/60000 (62%)]\tLoss: 2.302635\n",
            "INFO:root:Train Epoch: 3 [37760/60000 (63%)]\tLoss: 2.302346\n",
            "INFO:root:Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.302673\n",
            "INFO:root:Train Epoch: 3 [39040/60000 (65%)]\tLoss: 2.302577\n",
            "INFO:root:Train Epoch: 3 [39680/60000 (66%)]\tLoss: 2.302515\n",
            "INFO:root:Train Epoch: 3 [40320/60000 (67%)]\tLoss: 2.302500\n",
            "INFO:root:Train Epoch: 3 [40960/60000 (68%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 3 [41600/60000 (69%)]\tLoss: 2.302635\n",
            "INFO:root:Train Epoch: 3 [42240/60000 (70%)]\tLoss: 2.302543\n",
            "INFO:root:Train Epoch: 3 [42880/60000 (71%)]\tLoss: 2.302563\n",
            "INFO:root:Train Epoch: 3 [43520/60000 (72%)]\tLoss: 2.302437\n",
            "INFO:root:Train Epoch: 3 [44160/60000 (74%)]\tLoss: 2.302303\n",
            "INFO:root:Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.302264\n",
            "INFO:root:Train Epoch: 3 [45440/60000 (76%)]\tLoss: 2.302447\n",
            "INFO:root:Train Epoch: 3 [46080/60000 (77%)]\tLoss: 2.302541\n",
            "INFO:root:Train Epoch: 3 [46720/60000 (78%)]\tLoss: 2.302618\n",
            "INFO:root:Train Epoch: 3 [47360/60000 (79%)]\tLoss: 2.302444\n",
            "INFO:root:Train Epoch: 3 [48000/60000 (80%)]\tLoss: 2.302515\n",
            "INFO:root:Train Epoch: 3 [48640/60000 (81%)]\tLoss: 2.302531\n",
            "INFO:root:Train Epoch: 3 [49280/60000 (82%)]\tLoss: 2.302116\n",
            "INFO:root:Train Epoch: 3 [49920/60000 (83%)]\tLoss: 2.302239\n",
            "INFO:root:Train Epoch: 3 [50560/60000 (84%)]\tLoss: 2.302398\n",
            "INFO:root:Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.302539\n",
            "INFO:root:Train Epoch: 3 [51840/60000 (86%)]\tLoss: 2.302504\n",
            "INFO:root:Train Epoch: 3 [52480/60000 (87%)]\tLoss: 2.302416\n",
            "INFO:root:Train Epoch: 3 [53120/60000 (88%)]\tLoss: 2.302477\n",
            "INFO:root:Train Epoch: 3 [53760/60000 (90%)]\tLoss: 2.302473\n",
            "INFO:root:Train Epoch: 3 [54400/60000 (91%)]\tLoss: 2.302613\n",
            "INFO:root:Train Epoch: 3 [55040/60000 (92%)]\tLoss: 2.302687\n",
            "INFO:root:Train Epoch: 3 [55680/60000 (93%)]\tLoss: 2.302680\n",
            "INFO:root:Train Epoch: 3 [56320/60000 (94%)]\tLoss: 2.302681\n",
            "INFO:root:Train Epoch: 3 [56960/60000 (95%)]\tLoss: 2.302611\n",
            "INFO:root:Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.302399\n",
            "INFO:root:Train Epoch: 3 [58240/60000 (97%)]\tLoss: 2.302476\n",
            "INFO:root:Train Epoch: 3 [58880/60000 (98%)]\tLoss: 2.302506\n",
            "INFO:root:Train Epoch: 3 [59520/60000 (99%)]\tLoss: 2.302541\n",
            "INFO:root:\n",
            "Test set: Average loss: 2.3025, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "INFO:root:Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.302547\n",
            "INFO:root:Train Epoch: 4 [640/60000 (1%)]\tLoss: 2.302513\n",
            "INFO:root:Train Epoch: 4 [1280/60000 (2%)]\tLoss: 2.302454\n",
            "INFO:root:Train Epoch: 4 [1920/60000 (3%)]\tLoss: 2.302356\n",
            "INFO:root:Train Epoch: 4 [2560/60000 (4%)]\tLoss: 2.302549\n",
            "INFO:root:Train Epoch: 4 [3200/60000 (5%)]\tLoss: 2.302595\n",
            "INFO:root:Train Epoch: 4 [3840/60000 (6%)]\tLoss: 2.302580\n",
            "INFO:root:Train Epoch: 4 [4480/60000 (7%)]\tLoss: 2.302491\n",
            "INFO:root:Train Epoch: 4 [5120/60000 (9%)]\tLoss: 2.302585\n",
            "INFO:root:Train Epoch: 4 [5760/60000 (10%)]\tLoss: 2.302442\n",
            "INFO:root:Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.302507\n",
            "INFO:root:Train Epoch: 4 [7040/60000 (12%)]\tLoss: 2.302463\n",
            "INFO:root:Train Epoch: 4 [7680/60000 (13%)]\tLoss: 2.302260\n",
            "INFO:root:Train Epoch: 4 [8320/60000 (14%)]\tLoss: 2.302578\n",
            "INFO:root:Train Epoch: 4 [8960/60000 (15%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 4 [9600/60000 (16%)]\tLoss: 2.302362\n",
            "INFO:root:Train Epoch: 4 [10240/60000 (17%)]\tLoss: 2.302427\n",
            "INFO:root:Train Epoch: 4 [10880/60000 (18%)]\tLoss: 2.302536\n",
            "INFO:root:Train Epoch: 4 [11520/60000 (19%)]\tLoss: 2.302498\n",
            "INFO:root:Train Epoch: 4 [12160/60000 (20%)]\tLoss: 2.302268\n",
            "INFO:root:Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.302325\n",
            "INFO:root:Train Epoch: 4 [13440/60000 (22%)]\tLoss: 2.302702\n",
            "INFO:root:Train Epoch: 4 [14080/60000 (23%)]\tLoss: 2.302589\n",
            "INFO:root:Train Epoch: 4 [14720/60000 (25%)]\tLoss: 2.302527\n",
            "INFO:root:Train Epoch: 4 [15360/60000 (26%)]\tLoss: 2.302644\n",
            "INFO:root:Train Epoch: 4 [16000/60000 (27%)]\tLoss: 2.302729\n",
            "INFO:root:Train Epoch: 4 [16640/60000 (28%)]\tLoss: 2.302229\n",
            "INFO:root:Train Epoch: 4 [17280/60000 (29%)]\tLoss: 2.302755\n",
            "INFO:root:Train Epoch: 4 [17920/60000 (30%)]\tLoss: 2.302487\n",
            "INFO:root:Train Epoch: 4 [18560/60000 (31%)]\tLoss: 2.302609\n",
            "INFO:root:Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.302675\n",
            "INFO:root:Train Epoch: 4 [19840/60000 (33%)]\tLoss: 2.302592\n",
            "INFO:root:Train Epoch: 4 [20480/60000 (34%)]\tLoss: 2.302693\n",
            "INFO:root:Train Epoch: 4 [21120/60000 (35%)]\tLoss: 2.302459\n",
            "INFO:root:Train Epoch: 4 [21760/60000 (36%)]\tLoss: 2.302688\n",
            "INFO:root:Train Epoch: 4 [22400/60000 (37%)]\tLoss: 2.302519\n",
            "INFO:root:Train Epoch: 4 [23040/60000 (38%)]\tLoss: 2.302331\n",
            "INFO:root:Train Epoch: 4 [23680/60000 (39%)]\tLoss: 2.302576\n",
            "INFO:root:Train Epoch: 4 [24320/60000 (41%)]\tLoss: 2.302333\n",
            "INFO:root:Train Epoch: 4 [24960/60000 (42%)]\tLoss: 2.302447\n",
            "INFO:root:Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.302480\n",
            "INFO:root:Train Epoch: 4 [26240/60000 (44%)]\tLoss: 2.302418\n",
            "INFO:root:Train Epoch: 4 [26880/60000 (45%)]\tLoss: 2.302419\n",
            "INFO:root:Train Epoch: 4 [27520/60000 (46%)]\tLoss: 2.302382\n",
            "INFO:root:Train Epoch: 4 [28160/60000 (47%)]\tLoss: 2.302752\n",
            "INFO:root:Train Epoch: 4 [28800/60000 (48%)]\tLoss: 2.302718\n",
            "INFO:root:Train Epoch: 4 [29440/60000 (49%)]\tLoss: 2.302436\n",
            "INFO:root:Train Epoch: 4 [30080/60000 (50%)]\tLoss: 2.302453\n",
            "INFO:root:Train Epoch: 4 [30720/60000 (51%)]\tLoss: 2.302504\n",
            "INFO:root:Train Epoch: 4 [31360/60000 (52%)]\tLoss: 2.302641\n",
            "INFO:root:Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 4 [32640/60000 (54%)]\tLoss: 2.302415\n",
            "INFO:root:Train Epoch: 4 [33280/60000 (55%)]\tLoss: 2.302712\n",
            "INFO:root:Train Epoch: 4 [33920/60000 (57%)]\tLoss: 2.302393\n",
            "INFO:root:Train Epoch: 4 [34560/60000 (58%)]\tLoss: 2.302694\n",
            "INFO:root:Train Epoch: 4 [35200/60000 (59%)]\tLoss: 2.302527\n",
            "INFO:root:Train Epoch: 4 [35840/60000 (60%)]\tLoss: 2.302366\n",
            "INFO:root:Train Epoch: 4 [36480/60000 (61%)]\tLoss: 2.302567\n",
            "INFO:root:Train Epoch: 4 [37120/60000 (62%)]\tLoss: 2.302647\n",
            "INFO:root:Train Epoch: 4 [37760/60000 (63%)]\tLoss: 2.302301\n",
            "INFO:root:Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.302687\n",
            "INFO:root:Train Epoch: 4 [39040/60000 (65%)]\tLoss: 2.302576\n",
            "INFO:root:Train Epoch: 4 [39680/60000 (66%)]\tLoss: 2.302500\n",
            "INFO:root:Train Epoch: 4 [40320/60000 (67%)]\tLoss: 2.302482\n",
            "INFO:root:Train Epoch: 4 [40960/60000 (68%)]\tLoss: 2.302590\n",
            "INFO:root:Train Epoch: 4 [41600/60000 (69%)]\tLoss: 2.302645\n",
            "INFO:root:Train Epoch: 4 [42240/60000 (70%)]\tLoss: 2.302535\n",
            "INFO:root:Train Epoch: 4 [42880/60000 (71%)]\tLoss: 2.302560\n",
            "INFO:root:Train Epoch: 4 [43520/60000 (72%)]\tLoss: 2.302411\n",
            "INFO:root:Train Epoch: 4 [44160/60000 (74%)]\tLoss: 2.302249\n",
            "INFO:root:Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.302207\n",
            "INFO:root:Train Epoch: 4 [45440/60000 (76%)]\tLoss: 2.302423\n",
            "INFO:root:Train Epoch: 4 [46080/60000 (77%)]\tLoss: 2.302534\n",
            "INFO:root:Train Epoch: 4 [46720/60000 (78%)]\tLoss: 2.302626\n",
            "INFO:root:Train Epoch: 4 [47360/60000 (79%)]\tLoss: 2.302416\n",
            "INFO:root:Train Epoch: 4 [48000/60000 (80%)]\tLoss: 2.302499\n",
            "INFO:root:Train Epoch: 4 [48640/60000 (81%)]\tLoss: 2.302520\n",
            "INFO:root:Train Epoch: 4 [49280/60000 (82%)]\tLoss: 2.302036\n",
            "INFO:root:Train Epoch: 4 [49920/60000 (83%)]\tLoss: 2.302179\n",
            "INFO:root:Train Epoch: 4 [50560/60000 (84%)]\tLoss: 2.302365\n",
            "INFO:root:Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.302530\n",
            "INFO:root:Train Epoch: 4 [51840/60000 (86%)]\tLoss: 2.302490\n",
            "INFO:root:Train Epoch: 4 [52480/60000 (87%)]\tLoss: 2.302387\n",
            "INFO:root:Train Epoch: 4 [53120/60000 (88%)]\tLoss: 2.302459\n",
            "INFO:root:Train Epoch: 4 [53760/60000 (90%)]\tLoss: 2.302457\n",
            "INFO:root:Train Epoch: 4 [54400/60000 (91%)]\tLoss: 2.302616\n",
            "INFO:root:Train Epoch: 4 [55040/60000 (92%)]\tLoss: 2.302703\n",
            "INFO:root:Train Epoch: 4 [55680/60000 (93%)]\tLoss: 2.302695\n",
            "INFO:root:Train Epoch: 4 [56320/60000 (94%)]\tLoss: 2.302695\n",
            "INFO:root:Train Epoch: 4 [56960/60000 (95%)]\tLoss: 2.302615\n",
            "INFO:root:Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.302369\n",
            "INFO:root:Train Epoch: 4 [58240/60000 (97%)]\tLoss: 2.302459\n",
            "INFO:root:Train Epoch: 4 [58880/60000 (98%)]\tLoss: 2.302493\n",
            "INFO:root:Train Epoch: 4 [59520/60000 (99%)]\tLoss: 2.302534\n",
            "INFO:root:\n",
            "Test set: Average loss: 2.3025, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "INFO:root:Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.302542\n",
            "INFO:root:Train Epoch: 5 [640/60000 (1%)]\tLoss: 2.302502\n",
            "INFO:root:Train Epoch: 5 [1280/60000 (2%)]\tLoss: 2.302434\n",
            "INFO:root:Train Epoch: 5 [1920/60000 (3%)]\tLoss: 2.302322\n",
            "INFO:root:Train Epoch: 5 [2560/60000 (4%)]\tLoss: 2.302544\n",
            "INFO:root:Train Epoch: 5 [3200/60000 (5%)]\tLoss: 2.302597\n",
            "INFO:root:Train Epoch: 5 [3840/60000 (6%)]\tLoss: 2.302579\n",
            "INFO:root:Train Epoch: 5 [4480/60000 (7%)]\tLoss: 2.302477\n",
            "INFO:root:Train Epoch: 5 [5120/60000 (9%)]\tLoss: 2.302585\n",
            "INFO:root:Train Epoch: 5 [5760/60000 (10%)]\tLoss: 2.302422\n",
            "INFO:root:Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.302495\n",
            "INFO:root:Train Epoch: 5 [7040/60000 (12%)]\tLoss: 2.302445\n",
            "INFO:root:Train Epoch: 5 [7680/60000 (13%)]\tLoss: 2.302214\n",
            "INFO:root:Train Epoch: 5 [8320/60000 (14%)]\tLoss: 2.302577\n",
            "INFO:root:Train Epoch: 5 [8960/60000 (15%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 5 [9600/60000 (16%)]\tLoss: 2.302330\n",
            "INFO:root:Train Epoch: 5 [10240/60000 (17%)]\tLoss: 2.302404\n",
            "INFO:root:Train Epoch: 5 [10880/60000 (18%)]\tLoss: 2.302531\n",
            "INFO:root:Train Epoch: 5 [11520/60000 (19%)]\tLoss: 2.302487\n",
            "INFO:root:Train Epoch: 5 [12160/60000 (20%)]\tLoss: 2.302224\n",
            "INFO:root:Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.302292\n",
            "INFO:root:Train Epoch: 5 [13440/60000 (22%)]\tLoss: 2.302718\n",
            "INFO:root:Train Epoch: 5 [14080/60000 (23%)]\tLoss: 2.302590\n",
            "INFO:root:Train Epoch: 5 [14720/60000 (25%)]\tLoss: 2.302519\n",
            "INFO:root:Train Epoch: 5 [15360/60000 (26%)]\tLoss: 2.302651\n",
            "INFO:root:Train Epoch: 5 [16000/60000 (27%)]\tLoss: 2.302749\n",
            "INFO:root:Train Epoch: 5 [16640/60000 (28%)]\tLoss: 2.302183\n",
            "INFO:root:Train Epoch: 5 [17280/60000 (29%)]\tLoss: 2.302778\n",
            "INFO:root:Train Epoch: 5 [17920/60000 (30%)]\tLoss: 2.302475\n",
            "INFO:root:Train Epoch: 5 [18560/60000 (31%)]\tLoss: 2.302610\n",
            "INFO:root:Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.302685\n",
            "INFO:root:Train Epoch: 5 [19840/60000 (33%)]\tLoss: 2.302594\n",
            "INFO:root:Train Epoch: 5 [20480/60000 (34%)]\tLoss: 2.302708\n",
            "INFO:root:Train Epoch: 5 [21120/60000 (35%)]\tLoss: 2.302445\n",
            "INFO:root:Train Epoch: 5 [21760/60000 (36%)]\tLoss: 2.302700\n",
            "INFO:root:Train Epoch: 5 [22400/60000 (37%)]\tLoss: 2.302510\n",
            "INFO:root:Train Epoch: 5 [23040/60000 (38%)]\tLoss: 2.302299\n",
            "INFO:root:Train Epoch: 5 [23680/60000 (39%)]\tLoss: 2.302574\n",
            "INFO:root:Train Epoch: 5 [24320/60000 (41%)]\tLoss: 2.302300\n",
            "INFO:root:Train Epoch: 5 [24960/60000 (42%)]\tLoss: 2.302430\n",
            "INFO:root:Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.302470\n",
            "INFO:root:Train Epoch: 5 [26240/60000 (44%)]\tLoss: 2.302398\n",
            "INFO:root:Train Epoch: 5 [26880/60000 (45%)]\tLoss: 2.302398\n",
            "INFO:root:Train Epoch: 5 [27520/60000 (46%)]\tLoss: 2.302356\n",
            "INFO:root:Train Epoch: 5 [28160/60000 (47%)]\tLoss: 2.302772\n",
            "INFO:root:Train Epoch: 5 [28800/60000 (48%)]\tLoss: 2.302734\n",
            "INFO:root:Train Epoch: 5 [29440/60000 (49%)]\tLoss: 2.302417\n",
            "INFO:root:Train Epoch: 5 [30080/60000 (50%)]\tLoss: 2.302438\n",
            "INFO:root:Train Epoch: 5 [30720/60000 (51%)]\tLoss: 2.302494\n",
            "INFO:root:Train Epoch: 5 [31360/60000 (52%)]\tLoss: 2.302649\n",
            "INFO:root:Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 5 [32640/60000 (54%)]\tLoss: 2.302396\n",
            "INFO:root:Train Epoch: 5 [33280/60000 (55%)]\tLoss: 2.302726\n",
            "INFO:root:Train Epoch: 5 [33920/60000 (57%)]\tLoss: 2.302372\n",
            "INFO:root:Train Epoch: 5 [34560/60000 (58%)]\tLoss: 2.302706\n",
            "INFO:root:Train Epoch: 5 [35200/60000 (59%)]\tLoss: 2.302520\n",
            "INFO:root:Train Epoch: 5 [35840/60000 (60%)]\tLoss: 2.302342\n",
            "INFO:root:Train Epoch: 5 [36480/60000 (61%)]\tLoss: 2.302565\n",
            "INFO:root:Train Epoch: 5 [37120/60000 (62%)]\tLoss: 2.302655\n",
            "INFO:root:Train Epoch: 5 [37760/60000 (63%)]\tLoss: 2.302270\n",
            "INFO:root:Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.302697\n",
            "INFO:root:Train Epoch: 5 [39040/60000 (65%)]\tLoss: 2.302576\n",
            "INFO:root:Train Epoch: 5 [39680/60000 (66%)]\tLoss: 2.302490\n",
            "INFO:root:Train Epoch: 5 [40320/60000 (67%)]\tLoss: 2.302469\n",
            "INFO:root:Train Epoch: 5 [40960/60000 (68%)]\tLoss: 2.302592\n",
            "INFO:root:Train Epoch: 5 [41600/60000 (69%)]\tLoss: 2.302652\n",
            "INFO:root:Train Epoch: 5 [42240/60000 (70%)]\tLoss: 2.302530\n",
            "INFO:root:Train Epoch: 5 [42880/60000 (71%)]\tLoss: 2.302558\n",
            "INFO:root:Train Epoch: 5 [43520/60000 (72%)]\tLoss: 2.302393\n",
            "INFO:root:Train Epoch: 5 [44160/60000 (74%)]\tLoss: 2.302212\n",
            "INFO:root:Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.302169\n",
            "INFO:root:Train Epoch: 5 [45440/60000 (76%)]\tLoss: 2.302407\n",
            "INFO:root:Train Epoch: 5 [46080/60000 (77%)]\tLoss: 2.302531\n",
            "INFO:root:Train Epoch: 5 [46720/60000 (78%)]\tLoss: 2.302631\n",
            "INFO:root:Train Epoch: 5 [47360/60000 (79%)]\tLoss: 2.302397\n",
            "INFO:root:Train Epoch: 5 [48000/60000 (80%)]\tLoss: 2.302488\n",
            "INFO:root:Train Epoch: 5 [48640/60000 (81%)]\tLoss: 2.302512\n",
            "INFO:root:Train Epoch: 5 [49280/60000 (82%)]\tLoss: 2.301980\n",
            "INFO:root:Train Epoch: 5 [49920/60000 (83%)]\tLoss: 2.302138\n",
            "INFO:root:Train Epoch: 5 [50560/60000 (84%)]\tLoss: 2.302342\n",
            "INFO:root:Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.302524\n",
            "INFO:root:Train Epoch: 5 [51840/60000 (86%)]\tLoss: 2.302480\n",
            "INFO:root:Train Epoch: 5 [52480/60000 (87%)]\tLoss: 2.302366\n",
            "INFO:root:Train Epoch: 5 [53120/60000 (88%)]\tLoss: 2.302448\n",
            "INFO:root:Train Epoch: 5 [53760/60000 (90%)]\tLoss: 2.302445\n",
            "INFO:root:Train Epoch: 5 [54400/60000 (91%)]\tLoss: 2.302619\n",
            "INFO:root:Train Epoch: 5 [55040/60000 (92%)]\tLoss: 2.302715\n",
            "INFO:root:Train Epoch: 5 [55680/60000 (93%)]\tLoss: 2.302705\n",
            "INFO:root:Train Epoch: 5 [56320/60000 (94%)]\tLoss: 2.302704\n",
            "INFO:root:Train Epoch: 5 [56960/60000 (95%)]\tLoss: 2.302618\n",
            "INFO:root:Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.302348\n",
            "INFO:root:Train Epoch: 5 [58240/60000 (97%)]\tLoss: 2.302447\n",
            "INFO:root:Train Epoch: 5 [58880/60000 (98%)]\tLoss: 2.302485\n",
            "INFO:root:Train Epoch: 5 [59520/60000 (99%)]\tLoss: 2.302529\n",
            "INFO:root:\n",
            "Test set: Average loss: 2.3025, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "INFO:root:Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.302538\n",
            "INFO:root:Train Epoch: 6 [640/60000 (1%)]\tLoss: 2.302495\n",
            "INFO:root:Train Epoch: 6 [1280/60000 (2%)]\tLoss: 2.302420\n",
            "INFO:root:Train Epoch: 6 [1920/60000 (3%)]\tLoss: 2.302298\n",
            "INFO:root:Train Epoch: 6 [2560/60000 (4%)]\tLoss: 2.302540\n",
            "INFO:root:Train Epoch: 6 [3200/60000 (5%)]\tLoss: 2.302598\n",
            "INFO:root:Train Epoch: 6 [3840/60000 (6%)]\tLoss: 2.302578\n",
            "INFO:root:Train Epoch: 6 [4480/60000 (7%)]\tLoss: 2.302468\n",
            "INFO:root:Train Epoch: 6 [5120/60000 (9%)]\tLoss: 2.302586\n",
            "INFO:root:Train Epoch: 6 [5760/60000 (10%)]\tLoss: 2.302408\n",
            "INFO:root:Train Epoch: 6 [6400/60000 (11%)]\tLoss: 2.302487\n",
            "INFO:root:Train Epoch: 6 [7040/60000 (12%)]\tLoss: 2.302432\n",
            "INFO:root:Train Epoch: 6 [7680/60000 (13%)]\tLoss: 2.302182\n",
            "INFO:root:Train Epoch: 6 [8320/60000 (14%)]\tLoss: 2.302577\n",
            "INFO:root:Train Epoch: 6 [8960/60000 (15%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 6 [9600/60000 (16%)]\tLoss: 2.302309\n",
            "INFO:root:Train Epoch: 6 [10240/60000 (17%)]\tLoss: 2.302389\n",
            "INFO:root:Train Epoch: 6 [10880/60000 (18%)]\tLoss: 2.302526\n",
            "INFO:root:Train Epoch: 6 [11520/60000 (19%)]\tLoss: 2.302478\n",
            "INFO:root:Train Epoch: 6 [12160/60000 (20%)]\tLoss: 2.302194\n",
            "INFO:root:Train Epoch: 6 [12800/60000 (21%)]\tLoss: 2.302267\n",
            "INFO:root:Train Epoch: 6 [13440/60000 (22%)]\tLoss: 2.302728\n",
            "INFO:root:Train Epoch: 6 [14080/60000 (23%)]\tLoss: 2.302592\n",
            "INFO:root:Train Epoch: 6 [14720/60000 (25%)]\tLoss: 2.302513\n",
            "INFO:root:Train Epoch: 6 [15360/60000 (26%)]\tLoss: 2.302657\n",
            "INFO:root:Train Epoch: 6 [16000/60000 (27%)]\tLoss: 2.302762\n",
            "INFO:root:Train Epoch: 6 [16640/60000 (28%)]\tLoss: 2.302151\n",
            "INFO:root:Train Epoch: 6 [17280/60000 (29%)]\tLoss: 2.302794\n",
            "INFO:root:Train Epoch: 6 [17920/60000 (30%)]\tLoss: 2.302466\n",
            "INFO:root:Train Epoch: 6 [18560/60000 (31%)]\tLoss: 2.302611\n",
            "INFO:root:Train Epoch: 6 [19200/60000 (32%)]\tLoss: 2.302693\n",
            "INFO:root:Train Epoch: 6 [19840/60000 (33%)]\tLoss: 2.302596\n",
            "INFO:root:Train Epoch: 6 [20480/60000 (34%)]\tLoss: 2.302720\n",
            "INFO:root:Train Epoch: 6 [21120/60000 (35%)]\tLoss: 2.302435\n",
            "INFO:root:Train Epoch: 6 [21760/60000 (36%)]\tLoss: 2.302707\n",
            "INFO:root:Train Epoch: 6 [22400/60000 (37%)]\tLoss: 2.302504\n",
            "INFO:root:Train Epoch: 6 [23040/60000 (38%)]\tLoss: 2.302277\n",
            "INFO:root:Train Epoch: 6 [23680/60000 (39%)]\tLoss: 2.302573\n",
            "INFO:root:Train Epoch: 6 [24320/60000 (41%)]\tLoss: 2.302278\n",
            "INFO:root:Train Epoch: 6 [24960/60000 (42%)]\tLoss: 2.302419\n",
            "INFO:root:Train Epoch: 6 [25600/60000 (43%)]\tLoss: 2.302462\n",
            "INFO:root:Train Epoch: 6 [26240/60000 (44%)]\tLoss: 2.302385\n",
            "INFO:root:Train Epoch: 6 [26880/60000 (45%)]\tLoss: 2.302384\n",
            "INFO:root:Train Epoch: 6 [27520/60000 (46%)]\tLoss: 2.302339\n",
            "INFO:root:Train Epoch: 6 [28160/60000 (47%)]\tLoss: 2.302786\n",
            "INFO:root:Train Epoch: 6 [28800/60000 (48%)]\tLoss: 2.302745\n",
            "INFO:root:Train Epoch: 6 [29440/60000 (49%)]\tLoss: 2.302404\n",
            "INFO:root:Train Epoch: 6 [30080/60000 (50%)]\tLoss: 2.302427\n",
            "INFO:root:Train Epoch: 6 [30720/60000 (51%)]\tLoss: 2.302485\n",
            "INFO:root:Train Epoch: 6 [31360/60000 (52%)]\tLoss: 2.302654\n",
            "INFO:root:Train Epoch: 6 [32000/60000 (53%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 6 [32640/60000 (54%)]\tLoss: 2.302382\n",
            "INFO:root:Train Epoch: 6 [33280/60000 (55%)]\tLoss: 2.302736\n",
            "INFO:root:Train Epoch: 6 [33920/60000 (57%)]\tLoss: 2.302357\n",
            "INFO:root:Train Epoch: 6 [34560/60000 (58%)]\tLoss: 2.302714\n",
            "INFO:root:Train Epoch: 6 [35200/60000 (59%)]\tLoss: 2.302515\n",
            "INFO:root:Train Epoch: 6 [35840/60000 (60%)]\tLoss: 2.302325\n",
            "INFO:root:Train Epoch: 6 [36480/60000 (61%)]\tLoss: 2.302564\n",
            "INFO:root:Train Epoch: 6 [37120/60000 (62%)]\tLoss: 2.302661\n",
            "INFO:root:Train Epoch: 6 [37760/60000 (63%)]\tLoss: 2.302248\n",
            "INFO:root:Train Epoch: 6 [38400/60000 (64%)]\tLoss: 2.302705\n",
            "INFO:root:Train Epoch: 6 [39040/60000 (65%)]\tLoss: 2.302575\n",
            "INFO:root:Train Epoch: 6 [39680/60000 (66%)]\tLoss: 2.302483\n",
            "INFO:root:Train Epoch: 6 [40320/60000 (67%)]\tLoss: 2.302460\n",
            "INFO:root:Train Epoch: 6 [40960/60000 (68%)]\tLoss: 2.302593\n",
            "INFO:root:Train Epoch: 6 [41600/60000 (69%)]\tLoss: 2.302656\n",
            "INFO:root:Train Epoch: 6 [42240/60000 (70%)]\tLoss: 2.302525\n",
            "INFO:root:Train Epoch: 6 [42880/60000 (71%)]\tLoss: 2.302557\n",
            "INFO:root:Train Epoch: 6 [43520/60000 (72%)]\tLoss: 2.302381\n",
            "INFO:root:Train Epoch: 6 [44160/60000 (74%)]\tLoss: 2.302186\n",
            "INFO:root:Train Epoch: 6 [44800/60000 (75%)]\tLoss: 2.302141\n",
            "INFO:root:Train Epoch: 6 [45440/60000 (76%)]\tLoss: 2.302395\n",
            "INFO:root:Train Epoch: 6 [46080/60000 (77%)]\tLoss: 2.302527\n",
            "INFO:root:Train Epoch: 6 [46720/60000 (78%)]\tLoss: 2.302635\n",
            "INFO:root:Train Epoch: 6 [47360/60000 (79%)]\tLoss: 2.302384\n",
            "INFO:root:Train Epoch: 6 [48000/60000 (80%)]\tLoss: 2.302481\n",
            "INFO:root:Train Epoch: 6 [48640/60000 (81%)]\tLoss: 2.302507\n",
            "INFO:root:Train Epoch: 6 [49280/60000 (82%)]\tLoss: 2.301941\n",
            "INFO:root:Train Epoch: 6 [49920/60000 (83%)]\tLoss: 2.302110\n",
            "INFO:root:Train Epoch: 6 [50560/60000 (84%)]\tLoss: 2.302326\n",
            "INFO:root:Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.302520\n",
            "INFO:root:Train Epoch: 6 [51840/60000 (86%)]\tLoss: 2.302474\n",
            "INFO:root:Train Epoch: 6 [52480/60000 (87%)]\tLoss: 2.302352\n",
            "INFO:root:Train Epoch: 6 [53120/60000 (88%)]\tLoss: 2.302439\n",
            "INFO:root:Train Epoch: 6 [53760/60000 (90%)]\tLoss: 2.302437\n",
            "INFO:root:Train Epoch: 6 [54400/60000 (91%)]\tLoss: 2.302620\n",
            "INFO:root:Train Epoch: 6 [55040/60000 (92%)]\tLoss: 2.302722\n",
            "INFO:root:Train Epoch: 6 [55680/60000 (93%)]\tLoss: 2.302712\n",
            "INFO:root:Train Epoch: 6 [56320/60000 (94%)]\tLoss: 2.302711\n",
            "INFO:root:Train Epoch: 6 [56960/60000 (95%)]\tLoss: 2.302619\n",
            "INFO:root:Train Epoch: 6 [57600/60000 (96%)]\tLoss: 2.302334\n",
            "INFO:root:Train Epoch: 6 [58240/60000 (97%)]\tLoss: 2.302439\n",
            "INFO:root:Train Epoch: 6 [58880/60000 (98%)]\tLoss: 2.302479\n",
            "INFO:root:Train Epoch: 6 [59520/60000 (99%)]\tLoss: 2.302526\n",
            "INFO:root:\n",
            "Test set: Average loss: 2.3025, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "INFO:root:Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.302535\n",
            "INFO:root:Train Epoch: 7 [640/60000 (1%)]\tLoss: 2.302490\n",
            "INFO:root:Train Epoch: 7 [1280/60000 (2%)]\tLoss: 2.302410\n",
            "INFO:root:Train Epoch: 7 [1920/60000 (3%)]\tLoss: 2.302281\n",
            "INFO:root:Train Epoch: 7 [2560/60000 (4%)]\tLoss: 2.302537\n",
            "INFO:root:Train Epoch: 7 [3200/60000 (5%)]\tLoss: 2.302598\n",
            "INFO:root:Train Epoch: 7 [3840/60000 (6%)]\tLoss: 2.302577\n",
            "INFO:root:Train Epoch: 7 [4480/60000 (7%)]\tLoss: 2.302461\n",
            "INFO:root:Train Epoch: 7 [5120/60000 (9%)]\tLoss: 2.302586\n",
            "INFO:root:Train Epoch: 7 [5760/60000 (10%)]\tLoss: 2.302398\n",
            "INFO:root:Train Epoch: 7 [6400/60000 (11%)]\tLoss: 2.302482\n",
            "INFO:root:Train Epoch: 7 [7040/60000 (12%)]\tLoss: 2.302424\n",
            "INFO:root:Train Epoch: 7 [7680/60000 (13%)]\tLoss: 2.302160\n",
            "INFO:root:Train Epoch: 7 [8320/60000 (14%)]\tLoss: 2.302577\n",
            "INFO:root:Train Epoch: 7 [8960/60000 (15%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 7 [9600/60000 (16%)]\tLoss: 2.302294\n",
            "INFO:root:Train Epoch: 7 [10240/60000 (17%)]\tLoss: 2.302378\n",
            "INFO:root:Train Epoch: 7 [10880/60000 (18%)]\tLoss: 2.302524\n",
            "INFO:root:Train Epoch: 7 [11520/60000 (19%)]\tLoss: 2.302472\n",
            "INFO:root:Train Epoch: 7 [12160/60000 (20%)]\tLoss: 2.302173\n",
            "INFO:root:Train Epoch: 7 [12800/60000 (21%)]\tLoss: 2.302250\n",
            "INFO:root:Train Epoch: 7 [13440/60000 (22%)]\tLoss: 2.302735\n",
            "INFO:root:Train Epoch: 7 [14080/60000 (23%)]\tLoss: 2.302593\n",
            "INFO:root:Train Epoch: 7 [14720/60000 (25%)]\tLoss: 2.302510\n",
            "INFO:root:Train Epoch: 7 [15360/60000 (26%)]\tLoss: 2.302660\n",
            "INFO:root:Train Epoch: 7 [16000/60000 (27%)]\tLoss: 2.302772\n",
            "INFO:root:Train Epoch: 7 [16640/60000 (28%)]\tLoss: 2.302129\n",
            "INFO:root:Train Epoch: 7 [17280/60000 (29%)]\tLoss: 2.302805\n",
            "INFO:root:Train Epoch: 7 [17920/60000 (30%)]\tLoss: 2.302460\n",
            "INFO:root:Train Epoch: 7 [18560/60000 (31%)]\tLoss: 2.302612\n",
            "INFO:root:Train Epoch: 7 [19200/60000 (32%)]\tLoss: 2.302697\n",
            "INFO:root:Train Epoch: 7 [19840/60000 (33%)]\tLoss: 2.302597\n",
            "INFO:root:Train Epoch: 7 [20480/60000 (34%)]\tLoss: 2.302727\n",
            "INFO:root:Train Epoch: 7 [21120/60000 (35%)]\tLoss: 2.302428\n",
            "INFO:root:Train Epoch: 7 [21760/60000 (36%)]\tLoss: 2.302714\n",
            "INFO:root:Train Epoch: 7 [22400/60000 (37%)]\tLoss: 2.302500\n",
            "INFO:root:Train Epoch: 7 [23040/60000 (38%)]\tLoss: 2.302261\n",
            "INFO:root:Train Epoch: 7 [23680/60000 (39%)]\tLoss: 2.302573\n",
            "INFO:root:Train Epoch: 7 [24320/60000 (41%)]\tLoss: 2.302262\n",
            "INFO:root:Train Epoch: 7 [24960/60000 (42%)]\tLoss: 2.302410\n",
            "INFO:root:Train Epoch: 7 [25600/60000 (43%)]\tLoss: 2.302456\n",
            "INFO:root:Train Epoch: 7 [26240/60000 (44%)]\tLoss: 2.302375\n",
            "INFO:root:Train Epoch: 7 [26880/60000 (45%)]\tLoss: 2.302373\n",
            "INFO:root:Train Epoch: 7 [27520/60000 (46%)]\tLoss: 2.302326\n",
            "INFO:root:Train Epoch: 7 [28160/60000 (47%)]\tLoss: 2.302796\n",
            "INFO:root:Train Epoch: 7 [28800/60000 (48%)]\tLoss: 2.302753\n",
            "INFO:root:Train Epoch: 7 [29440/60000 (49%)]\tLoss: 2.302395\n",
            "INFO:root:Train Epoch: 7 [30080/60000 (50%)]\tLoss: 2.302419\n",
            "INFO:root:Train Epoch: 7 [30720/60000 (51%)]\tLoss: 2.302480\n",
            "INFO:root:Train Epoch: 7 [31360/60000 (52%)]\tLoss: 2.302657\n",
            "INFO:root:Train Epoch: 7 [32000/60000 (53%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 7 [32640/60000 (54%)]\tLoss: 2.302372\n",
            "INFO:root:Train Epoch: 7 [33280/60000 (55%)]\tLoss: 2.302743\n",
            "INFO:root:Train Epoch: 7 [33920/60000 (57%)]\tLoss: 2.302347\n",
            "INFO:root:Train Epoch: 7 [34560/60000 (58%)]\tLoss: 2.302720\n",
            "INFO:root:Train Epoch: 7 [35200/60000 (59%)]\tLoss: 2.302512\n",
            "INFO:root:Train Epoch: 7 [35840/60000 (60%)]\tLoss: 2.302313\n",
            "INFO:root:Train Epoch: 7 [36480/60000 (61%)]\tLoss: 2.302563\n",
            "INFO:root:Train Epoch: 7 [37120/60000 (62%)]\tLoss: 2.302665\n",
            "INFO:root:Train Epoch: 7 [37760/60000 (63%)]\tLoss: 2.302233\n",
            "INFO:root:Train Epoch: 7 [38400/60000 (64%)]\tLoss: 2.302709\n",
            "INFO:root:Train Epoch: 7 [39040/60000 (65%)]\tLoss: 2.302575\n",
            "INFO:root:Train Epoch: 7 [39680/60000 (66%)]\tLoss: 2.302478\n",
            "INFO:root:Train Epoch: 7 [40320/60000 (67%)]\tLoss: 2.302454\n",
            "INFO:root:Train Epoch: 7 [40960/60000 (68%)]\tLoss: 2.302594\n",
            "INFO:root:Train Epoch: 7 [41600/60000 (69%)]\tLoss: 2.302660\n",
            "INFO:root:Train Epoch: 7 [42240/60000 (70%)]\tLoss: 2.302523\n",
            "INFO:root:Train Epoch: 7 [42880/60000 (71%)]\tLoss: 2.302556\n",
            "INFO:root:Train Epoch: 7 [43520/60000 (72%)]\tLoss: 2.302372\n",
            "INFO:root:Train Epoch: 7 [44160/60000 (74%)]\tLoss: 2.302167\n",
            "INFO:root:Train Epoch: 7 [44800/60000 (75%)]\tLoss: 2.302122\n",
            "INFO:root:Train Epoch: 7 [45440/60000 (76%)]\tLoss: 2.302387\n",
            "INFO:root:Train Epoch: 7 [46080/60000 (77%)]\tLoss: 2.302525\n",
            "INFO:root:Train Epoch: 7 [46720/60000 (78%)]\tLoss: 2.302638\n",
            "INFO:root:Train Epoch: 7 [47360/60000 (79%)]\tLoss: 2.302375\n",
            "INFO:root:Train Epoch: 7 [48000/60000 (80%)]\tLoss: 2.302476\n",
            "INFO:root:Train Epoch: 7 [48640/60000 (81%)]\tLoss: 2.302504\n",
            "INFO:root:Train Epoch: 7 [49280/60000 (82%)]\tLoss: 2.301913\n",
            "INFO:root:Train Epoch: 7 [49920/60000 (83%)]\tLoss: 2.302089\n",
            "INFO:root:Train Epoch: 7 [50560/60000 (84%)]\tLoss: 2.302315\n",
            "INFO:root:Train Epoch: 7 [51200/60000 (85%)]\tLoss: 2.302517\n",
            "INFO:root:Train Epoch: 7 [51840/60000 (86%)]\tLoss: 2.302469\n",
            "INFO:root:Train Epoch: 7 [52480/60000 (87%)]\tLoss: 2.302342\n",
            "INFO:root:Train Epoch: 7 [53120/60000 (88%)]\tLoss: 2.302433\n",
            "INFO:root:Train Epoch: 7 [53760/60000 (90%)]\tLoss: 2.302432\n",
            "INFO:root:Train Epoch: 7 [54400/60000 (91%)]\tLoss: 2.302621\n",
            "INFO:root:Train Epoch: 7 [55040/60000 (92%)]\tLoss: 2.302728\n",
            "INFO:root:Train Epoch: 7 [55680/60000 (93%)]\tLoss: 2.302716\n",
            "INFO:root:Train Epoch: 7 [56320/60000 (94%)]\tLoss: 2.302716\n",
            "INFO:root:Train Epoch: 7 [56960/60000 (95%)]\tLoss: 2.302620\n",
            "INFO:root:Train Epoch: 7 [57600/60000 (96%)]\tLoss: 2.302324\n",
            "INFO:root:Train Epoch: 7 [58240/60000 (97%)]\tLoss: 2.302433\n",
            "INFO:root:Train Epoch: 7 [58880/60000 (98%)]\tLoss: 2.302475\n",
            "INFO:root:Train Epoch: 7 [59520/60000 (99%)]\tLoss: 2.302524\n",
            "INFO:root:\n",
            "Test set: Average loss: 2.3025, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "INFO:root:Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.302533\n",
            "INFO:root:Train Epoch: 8 [640/60000 (1%)]\tLoss: 2.302486\n",
            "INFO:root:Train Epoch: 8 [1280/60000 (2%)]\tLoss: 2.302403\n",
            "INFO:root:Train Epoch: 8 [1920/60000 (3%)]\tLoss: 2.302270\n",
            "INFO:root:Train Epoch: 8 [2560/60000 (4%)]\tLoss: 2.302536\n",
            "INFO:root:Train Epoch: 8 [3200/60000 (5%)]\tLoss: 2.302598\n",
            "INFO:root:Train Epoch: 8 [3840/60000 (6%)]\tLoss: 2.302578\n",
            "INFO:root:Train Epoch: 8 [4480/60000 (7%)]\tLoss: 2.302456\n",
            "INFO:root:Train Epoch: 8 [5120/60000 (9%)]\tLoss: 2.302586\n",
            "INFO:root:Train Epoch: 8 [5760/60000 (10%)]\tLoss: 2.302391\n",
            "INFO:root:Train Epoch: 8 [6400/60000 (11%)]\tLoss: 2.302477\n",
            "INFO:root:Train Epoch: 8 [7040/60000 (12%)]\tLoss: 2.302418\n",
            "INFO:root:Train Epoch: 8 [7680/60000 (13%)]\tLoss: 2.302144\n",
            "INFO:root:Train Epoch: 8 [8320/60000 (14%)]\tLoss: 2.302576\n",
            "INFO:root:Train Epoch: 8 [8960/60000 (15%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 8 [9600/60000 (16%)]\tLoss: 2.302283\n",
            "INFO:root:Train Epoch: 8 [10240/60000 (17%)]\tLoss: 2.302371\n",
            "INFO:root:Train Epoch: 8 [10880/60000 (18%)]\tLoss: 2.302521\n",
            "INFO:root:Train Epoch: 8 [11520/60000 (19%)]\tLoss: 2.302468\n",
            "INFO:root:Train Epoch: 8 [12160/60000 (20%)]\tLoss: 2.302158\n",
            "INFO:root:Train Epoch: 8 [12800/60000 (21%)]\tLoss: 2.302239\n",
            "INFO:root:Train Epoch: 8 [13440/60000 (22%)]\tLoss: 2.302741\n",
            "INFO:root:Train Epoch: 8 [14080/60000 (23%)]\tLoss: 2.302593\n",
            "INFO:root:Train Epoch: 8 [14720/60000 (25%)]\tLoss: 2.302507\n",
            "INFO:root:Train Epoch: 8 [15360/60000 (26%)]\tLoss: 2.302663\n",
            "INFO:root:Train Epoch: 8 [16000/60000 (27%)]\tLoss: 2.302778\n",
            "INFO:root:Train Epoch: 8 [16640/60000 (28%)]\tLoss: 2.302113\n",
            "INFO:root:Train Epoch: 8 [17280/60000 (29%)]\tLoss: 2.302813\n",
            "INFO:root:Train Epoch: 8 [17920/60000 (30%)]\tLoss: 2.302456\n",
            "INFO:root:Train Epoch: 8 [18560/60000 (31%)]\tLoss: 2.302612\n",
            "INFO:root:Train Epoch: 8 [19200/60000 (32%)]\tLoss: 2.302700\n",
            "INFO:root:Train Epoch: 8 [19840/60000 (33%)]\tLoss: 2.302598\n",
            "INFO:root:Train Epoch: 8 [20480/60000 (34%)]\tLoss: 2.302732\n",
            "INFO:root:Train Epoch: 8 [21120/60000 (35%)]\tLoss: 2.302423\n",
            "INFO:root:Train Epoch: 8 [21760/60000 (36%)]\tLoss: 2.302717\n",
            "INFO:root:Train Epoch: 8 [22400/60000 (37%)]\tLoss: 2.302497\n",
            "INFO:root:Train Epoch: 8 [23040/60000 (38%)]\tLoss: 2.302250\n",
            "INFO:root:Train Epoch: 8 [23680/60000 (39%)]\tLoss: 2.302572\n",
            "INFO:root:Train Epoch: 8 [24320/60000 (41%)]\tLoss: 2.302251\n",
            "INFO:root:Train Epoch: 8 [24960/60000 (42%)]\tLoss: 2.302405\n",
            "INFO:root:Train Epoch: 8 [25600/60000 (43%)]\tLoss: 2.302452\n",
            "INFO:root:Train Epoch: 8 [26240/60000 (44%)]\tLoss: 2.302368\n",
            "INFO:root:Train Epoch: 8 [26880/60000 (45%)]\tLoss: 2.302366\n",
            "INFO:root:Train Epoch: 8 [27520/60000 (46%)]\tLoss: 2.302318\n",
            "INFO:root:Train Epoch: 8 [28160/60000 (47%)]\tLoss: 2.302804\n",
            "INFO:root:Train Epoch: 8 [28800/60000 (48%)]\tLoss: 2.302759\n",
            "INFO:root:Train Epoch: 8 [29440/60000 (49%)]\tLoss: 2.302388\n",
            "INFO:root:Train Epoch: 8 [30080/60000 (50%)]\tLoss: 2.302414\n",
            "INFO:root:Train Epoch: 8 [30720/60000 (51%)]\tLoss: 2.302476\n",
            "INFO:root:Train Epoch: 8 [31360/60000 (52%)]\tLoss: 2.302660\n",
            "INFO:root:Train Epoch: 8 [32000/60000 (53%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 8 [32640/60000 (54%)]\tLoss: 2.302365\n",
            "INFO:root:Train Epoch: 8 [33280/60000 (55%)]\tLoss: 2.302748\n",
            "INFO:root:Train Epoch: 8 [33920/60000 (57%)]\tLoss: 2.302340\n",
            "INFO:root:Train Epoch: 8 [34560/60000 (58%)]\tLoss: 2.302724\n",
            "INFO:root:Train Epoch: 8 [35200/60000 (59%)]\tLoss: 2.302509\n",
            "INFO:root:Train Epoch: 8 [35840/60000 (60%)]\tLoss: 2.302305\n",
            "INFO:root:Train Epoch: 8 [36480/60000 (61%)]\tLoss: 2.302562\n",
            "INFO:root:Train Epoch: 8 [37120/60000 (62%)]\tLoss: 2.302668\n",
            "INFO:root:Train Epoch: 8 [37760/60000 (63%)]\tLoss: 2.302221\n",
            "INFO:root:Train Epoch: 8 [38400/60000 (64%)]\tLoss: 2.302712\n",
            "INFO:root:Train Epoch: 8 [39040/60000 (65%)]\tLoss: 2.302575\n",
            "INFO:root:Train Epoch: 8 [39680/60000 (66%)]\tLoss: 2.302475\n",
            "INFO:root:Train Epoch: 8 [40320/60000 (67%)]\tLoss: 2.302450\n",
            "INFO:root:Train Epoch: 8 [40960/60000 (68%)]\tLoss: 2.302595\n",
            "INFO:root:Train Epoch: 8 [41600/60000 (69%)]\tLoss: 2.302662\n",
            "INFO:root:Train Epoch: 8 [42240/60000 (70%)]\tLoss: 2.302521\n",
            "INFO:root:Train Epoch: 8 [42880/60000 (71%)]\tLoss: 2.302556\n",
            "INFO:root:Train Epoch: 8 [43520/60000 (72%)]\tLoss: 2.302366\n",
            "INFO:root:Train Epoch: 8 [44160/60000 (74%)]\tLoss: 2.302155\n",
            "INFO:root:Train Epoch: 8 [44800/60000 (75%)]\tLoss: 2.302109\n",
            "INFO:root:Train Epoch: 8 [45440/60000 (76%)]\tLoss: 2.302382\n",
            "INFO:root:Train Epoch: 8 [46080/60000 (77%)]\tLoss: 2.302523\n",
            "INFO:root:Train Epoch: 8 [46720/60000 (78%)]\tLoss: 2.302639\n",
            "INFO:root:Train Epoch: 8 [47360/60000 (79%)]\tLoss: 2.302368\n",
            "INFO:root:Train Epoch: 8 [48000/60000 (80%)]\tLoss: 2.302472\n",
            "INFO:root:Train Epoch: 8 [48640/60000 (81%)]\tLoss: 2.302501\n",
            "INFO:root:Train Epoch: 8 [49280/60000 (82%)]\tLoss: 2.301894\n",
            "INFO:root:Train Epoch: 8 [49920/60000 (83%)]\tLoss: 2.302074\n",
            "INFO:root:Train Epoch: 8 [50560/60000 (84%)]\tLoss: 2.302307\n",
            "INFO:root:Train Epoch: 8 [51200/60000 (85%)]\tLoss: 2.302515\n",
            "INFO:root:Train Epoch: 8 [51840/60000 (86%)]\tLoss: 2.302465\n",
            "INFO:root:Train Epoch: 8 [52480/60000 (87%)]\tLoss: 2.302335\n",
            "INFO:root:Train Epoch: 8 [53120/60000 (88%)]\tLoss: 2.302429\n",
            "INFO:root:Train Epoch: 8 [53760/60000 (90%)]\tLoss: 2.302428\n",
            "INFO:root:Train Epoch: 8 [54400/60000 (91%)]\tLoss: 2.302623\n",
            "INFO:root:Train Epoch: 8 [55040/60000 (92%)]\tLoss: 2.302732\n",
            "INFO:root:Train Epoch: 8 [55680/60000 (93%)]\tLoss: 2.302720\n",
            "INFO:root:Train Epoch: 8 [56320/60000 (94%)]\tLoss: 2.302719\n",
            "INFO:root:Train Epoch: 8 [56960/60000 (95%)]\tLoss: 2.302622\n",
            "INFO:root:Train Epoch: 8 [57600/60000 (96%)]\tLoss: 2.302317\n",
            "INFO:root:Train Epoch: 8 [58240/60000 (97%)]\tLoss: 2.302429\n",
            "INFO:root:Train Epoch: 8 [58880/60000 (98%)]\tLoss: 2.302471\n",
            "INFO:root:Train Epoch: 8 [59520/60000 (99%)]\tLoss: 2.302522\n",
            "INFO:root:\n",
            "Test set: Average loss: 2.3025, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "INFO:root:Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.302531\n",
            "INFO:root:Train Epoch: 9 [640/60000 (1%)]\tLoss: 2.302484\n",
            "INFO:root:Train Epoch: 9 [1280/60000 (2%)]\tLoss: 2.302398\n",
            "INFO:root:Train Epoch: 9 [1920/60000 (3%)]\tLoss: 2.302262\n",
            "INFO:root:Train Epoch: 9 [2560/60000 (4%)]\tLoss: 2.302534\n",
            "INFO:root:Train Epoch: 9 [3200/60000 (5%)]\tLoss: 2.302599\n",
            "INFO:root:Train Epoch: 9 [3840/60000 (6%)]\tLoss: 2.302577\n",
            "INFO:root:Train Epoch: 9 [4480/60000 (7%)]\tLoss: 2.302453\n",
            "INFO:root:Train Epoch: 9 [5120/60000 (9%)]\tLoss: 2.302586\n",
            "INFO:root:Train Epoch: 9 [5760/60000 (10%)]\tLoss: 2.302386\n",
            "INFO:root:Train Epoch: 9 [6400/60000 (11%)]\tLoss: 2.302474\n",
            "INFO:root:Train Epoch: 9 [7040/60000 (12%)]\tLoss: 2.302413\n",
            "INFO:root:Train Epoch: 9 [7680/60000 (13%)]\tLoss: 2.302133\n",
            "INFO:root:Train Epoch: 9 [8320/60000 (14%)]\tLoss: 2.302576\n",
            "INFO:root:Train Epoch: 9 [8960/60000 (15%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 9 [9600/60000 (16%)]\tLoss: 2.302276\n",
            "INFO:root:Train Epoch: 9 [10240/60000 (17%)]\tLoss: 2.302365\n",
            "INFO:root:Train Epoch: 9 [10880/60000 (18%)]\tLoss: 2.302520\n",
            "INFO:root:Train Epoch: 9 [11520/60000 (19%)]\tLoss: 2.302464\n",
            "INFO:root:Train Epoch: 9 [12160/60000 (20%)]\tLoss: 2.302148\n",
            "INFO:root:Train Epoch: 9 [12800/60000 (21%)]\tLoss: 2.302231\n",
            "INFO:root:Train Epoch: 9 [13440/60000 (22%)]\tLoss: 2.302744\n",
            "INFO:root:Train Epoch: 9 [14080/60000 (23%)]\tLoss: 2.302594\n",
            "INFO:root:Train Epoch: 9 [14720/60000 (25%)]\tLoss: 2.302505\n",
            "INFO:root:Train Epoch: 9 [15360/60000 (26%)]\tLoss: 2.302665\n",
            "INFO:root:Train Epoch: 9 [16000/60000 (27%)]\tLoss: 2.302783\n",
            "INFO:root:Train Epoch: 9 [16640/60000 (28%)]\tLoss: 2.302103\n",
            "INFO:root:Train Epoch: 9 [17280/60000 (29%)]\tLoss: 2.302819\n",
            "INFO:root:Train Epoch: 9 [17920/60000 (30%)]\tLoss: 2.302453\n",
            "INFO:root:Train Epoch: 9 [18560/60000 (31%)]\tLoss: 2.302613\n",
            "INFO:root:Train Epoch: 9 [19200/60000 (32%)]\tLoss: 2.302703\n",
            "INFO:root:Train Epoch: 9 [19840/60000 (33%)]\tLoss: 2.302598\n",
            "INFO:root:Train Epoch: 9 [20480/60000 (34%)]\tLoss: 2.302736\n",
            "INFO:root:Train Epoch: 9 [21120/60000 (35%)]\tLoss: 2.302419\n",
            "INFO:root:Train Epoch: 9 [21760/60000 (36%)]\tLoss: 2.302720\n",
            "INFO:root:Train Epoch: 9 [22400/60000 (37%)]\tLoss: 2.302495\n",
            "INFO:root:Train Epoch: 9 [23040/60000 (38%)]\tLoss: 2.302243\n",
            "INFO:root:Train Epoch: 9 [23680/60000 (39%)]\tLoss: 2.302572\n",
            "INFO:root:Train Epoch: 9 [24320/60000 (41%)]\tLoss: 2.302243\n",
            "INFO:root:Train Epoch: 9 [24960/60000 (42%)]\tLoss: 2.302401\n",
            "INFO:root:Train Epoch: 9 [25600/60000 (43%)]\tLoss: 2.302449\n",
            "INFO:root:Train Epoch: 9 [26240/60000 (44%)]\tLoss: 2.302364\n",
            "INFO:root:Train Epoch: 9 [26880/60000 (45%)]\tLoss: 2.302362\n",
            "INFO:root:Train Epoch: 9 [27520/60000 (46%)]\tLoss: 2.302312\n",
            "INFO:root:Train Epoch: 9 [28160/60000 (47%)]\tLoss: 2.302808\n",
            "INFO:root:Train Epoch: 9 [28800/60000 (48%)]\tLoss: 2.302763\n",
            "INFO:root:Train Epoch: 9 [29440/60000 (49%)]\tLoss: 2.302384\n",
            "INFO:root:Train Epoch: 9 [30080/60000 (50%)]\tLoss: 2.302410\n",
            "INFO:root:Train Epoch: 9 [30720/60000 (51%)]\tLoss: 2.302474\n",
            "INFO:root:Train Epoch: 9 [31360/60000 (52%)]\tLoss: 2.302661\n",
            "INFO:root:Train Epoch: 9 [32000/60000 (53%)]\tLoss: 2.302584\n",
            "INFO:root:Train Epoch: 9 [32640/60000 (54%)]\tLoss: 2.302360\n",
            "INFO:root:Train Epoch: 9 [33280/60000 (55%)]\tLoss: 2.302752\n",
            "INFO:root:Train Epoch: 9 [33920/60000 (57%)]\tLoss: 2.302335\n",
            "INFO:root:Train Epoch: 9 [34560/60000 (58%)]\tLoss: 2.302727\n",
            "INFO:root:Train Epoch: 9 [35200/60000 (59%)]\tLoss: 2.302508\n",
            "INFO:root:Train Epoch: 9 [35840/60000 (60%)]\tLoss: 2.302299\n",
            "INFO:root:Train Epoch: 9 [36480/60000 (61%)]\tLoss: 2.302562\n",
            "INFO:root:Train Epoch: 9 [37120/60000 (62%)]\tLoss: 2.302670\n",
            "INFO:root:Train Epoch: 9 [37760/60000 (63%)]\tLoss: 2.302214\n",
            "INFO:root:Train Epoch: 9 [38400/60000 (64%)]\tLoss: 2.302715\n",
            "INFO:root:Train Epoch: 9 [39040/60000 (65%)]\tLoss: 2.302575\n",
            "INFO:root:Train Epoch: 9 [39680/60000 (66%)]\tLoss: 2.302472\n",
            "INFO:root:Train Epoch: 9 [40320/60000 (67%)]\tLoss: 2.302447\n",
            "INFO:root:Train Epoch: 9 [40960/60000 (68%)]\tLoss: 2.302595\n",
            "INFO:root:Train Epoch: 9 [41600/60000 (69%)]\tLoss: 2.302664\n",
            "INFO:root:Train Epoch: 9 [42240/60000 (70%)]\tLoss: 2.302520\n",
            "INFO:root:Train Epoch: 9 [42880/60000 (71%)]\tLoss: 2.302556\n",
            "INFO:root:Train Epoch: 9 [43520/60000 (72%)]\tLoss: 2.302362\n",
            "INFO:root:Train Epoch: 9 [44160/60000 (74%)]\tLoss: 2.302146\n",
            "INFO:root:Train Epoch: 9 [44800/60000 (75%)]\tLoss: 2.302099\n",
            "INFO:root:Train Epoch: 9 [45440/60000 (76%)]\tLoss: 2.302378\n",
            "INFO:root:Train Epoch: 9 [46080/60000 (77%)]\tLoss: 2.302523\n",
            "INFO:root:Train Epoch: 9 [46720/60000 (78%)]\tLoss: 2.302640\n",
            "INFO:root:Train Epoch: 9 [47360/60000 (79%)]\tLoss: 2.302364\n",
            "INFO:root:Train Epoch: 9 [48000/60000 (80%)]\tLoss: 2.302469\n",
            "INFO:root:Train Epoch: 9 [48640/60000 (81%)]\tLoss: 2.302498\n",
            "INFO:root:Train Epoch: 9 [49280/60000 (82%)]\tLoss: 2.301881\n",
            "INFO:root:Train Epoch: 9 [49920/60000 (83%)]\tLoss: 2.302065\n",
            "INFO:root:Train Epoch: 9 [50560/60000 (84%)]\tLoss: 2.302302\n",
            "INFO:root:Train Epoch: 9 [51200/60000 (85%)]\tLoss: 2.302513\n",
            "INFO:root:Train Epoch: 9 [51840/60000 (86%)]\tLoss: 2.302464\n",
            "INFO:root:Train Epoch: 9 [52480/60000 (87%)]\tLoss: 2.302330\n",
            "INFO:root:Train Epoch: 9 [53120/60000 (88%)]\tLoss: 2.302427\n",
            "INFO:root:Train Epoch: 9 [53760/60000 (90%)]\tLoss: 2.302425\n",
            "INFO:root:Train Epoch: 9 [54400/60000 (91%)]\tLoss: 2.302623\n",
            "INFO:root:Train Epoch: 9 [55040/60000 (92%)]\tLoss: 2.302734\n",
            "INFO:root:Train Epoch: 9 [55680/60000 (93%)]\tLoss: 2.302722\n",
            "INFO:root:Train Epoch: 9 [56320/60000 (94%)]\tLoss: 2.302721\n",
            "INFO:root:Train Epoch: 9 [56960/60000 (95%)]\tLoss: 2.302622\n",
            "INFO:root:Train Epoch: 9 [57600/60000 (96%)]\tLoss: 2.302312\n",
            "INFO:root:Train Epoch: 9 [58240/60000 (97%)]\tLoss: 2.302427\n",
            "INFO:root:Train Epoch: 9 [58880/60000 (98%)]\tLoss: 2.302470\n",
            "INFO:root:Train Epoch: 9 [59520/60000 (99%)]\tLoss: 2.302521\n",
            "INFO:root:\n",
            "Test set: Average loss: 2.3025, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "INFO:root:Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.302531\n",
            "INFO:root:Train Epoch: 10 [640/60000 (1%)]\tLoss: 2.302482\n",
            "INFO:root:Train Epoch: 10 [1280/60000 (2%)]\tLoss: 2.302395\n",
            "INFO:root:Train Epoch: 10 [1920/60000 (3%)]\tLoss: 2.302256\n",
            "INFO:root:Train Epoch: 10 [2560/60000 (4%)]\tLoss: 2.302534\n",
            "INFO:root:Train Epoch: 10 [3200/60000 (5%)]\tLoss: 2.302599\n",
            "INFO:root:Train Epoch: 10 [3840/60000 (6%)]\tLoss: 2.302577\n",
            "INFO:root:Train Epoch: 10 [4480/60000 (7%)]\tLoss: 2.302451\n",
            "INFO:root:Train Epoch: 10 [5120/60000 (9%)]\tLoss: 2.302587\n",
            "INFO:root:Train Epoch: 10 [5760/60000 (10%)]\tLoss: 2.302382\n",
            "INFO:root:Train Epoch: 10 [6400/60000 (11%)]\tLoss: 2.302472\n",
            "INFO:root:Train Epoch: 10 [7040/60000 (12%)]\tLoss: 2.302410\n",
            "INFO:root:Train Epoch: 10 [7680/60000 (13%)]\tLoss: 2.302125\n",
            "INFO:root:Train Epoch: 10 [8320/60000 (14%)]\tLoss: 2.302576\n",
            "INFO:root:Train Epoch: 10 [8960/60000 (15%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 10 [9600/60000 (16%)]\tLoss: 2.302270\n",
            "INFO:root:Train Epoch: 10 [10240/60000 (17%)]\tLoss: 2.302361\n",
            "INFO:root:Train Epoch: 10 [10880/60000 (18%)]\tLoss: 2.302519\n",
            "INFO:root:Train Epoch: 10 [11520/60000 (19%)]\tLoss: 2.302463\n",
            "INFO:root:Train Epoch: 10 [12160/60000 (20%)]\tLoss: 2.302140\n",
            "INFO:root:Train Epoch: 10 [12800/60000 (21%)]\tLoss: 2.302225\n",
            "INFO:root:Train Epoch: 10 [13440/60000 (22%)]\tLoss: 2.302746\n",
            "INFO:root:Train Epoch: 10 [14080/60000 (23%)]\tLoss: 2.302594\n",
            "INFO:root:Train Epoch: 10 [14720/60000 (25%)]\tLoss: 2.302504\n",
            "INFO:root:Train Epoch: 10 [15360/60000 (26%)]\tLoss: 2.302666\n",
            "INFO:root:Train Epoch: 10 [16000/60000 (27%)]\tLoss: 2.302786\n",
            "INFO:root:Train Epoch: 10 [16640/60000 (28%)]\tLoss: 2.302094\n",
            "INFO:root:Train Epoch: 10 [17280/60000 (29%)]\tLoss: 2.302822\n",
            "INFO:root:Train Epoch: 10 [17920/60000 (30%)]\tLoss: 2.302450\n",
            "INFO:root:Train Epoch: 10 [18560/60000 (31%)]\tLoss: 2.302613\n",
            "INFO:root:Train Epoch: 10 [19200/60000 (32%)]\tLoss: 2.302704\n",
            "INFO:root:Train Epoch: 10 [19840/60000 (33%)]\tLoss: 2.302598\n",
            "INFO:root:Train Epoch: 10 [20480/60000 (34%)]\tLoss: 2.302738\n",
            "INFO:root:Train Epoch: 10 [21120/60000 (35%)]\tLoss: 2.302417\n",
            "INFO:root:Train Epoch: 10 [21760/60000 (36%)]\tLoss: 2.302722\n",
            "INFO:root:Train Epoch: 10 [22400/60000 (37%)]\tLoss: 2.302494\n",
            "INFO:root:Train Epoch: 10 [23040/60000 (38%)]\tLoss: 2.302238\n",
            "INFO:root:Train Epoch: 10 [23680/60000 (39%)]\tLoss: 2.302572\n",
            "INFO:root:Train Epoch: 10 [24320/60000 (41%)]\tLoss: 2.302238\n",
            "INFO:root:Train Epoch: 10 [24960/60000 (42%)]\tLoss: 2.302398\n",
            "INFO:root:Train Epoch: 10 [25600/60000 (43%)]\tLoss: 2.302448\n",
            "INFO:root:Train Epoch: 10 [26240/60000 (44%)]\tLoss: 2.302361\n",
            "INFO:root:Train Epoch: 10 [26880/60000 (45%)]\tLoss: 2.302358\n",
            "INFO:root:Train Epoch: 10 [27520/60000 (46%)]\tLoss: 2.302307\n",
            "INFO:root:Train Epoch: 10 [28160/60000 (47%)]\tLoss: 2.302812\n",
            "INFO:root:Train Epoch: 10 [28800/60000 (48%)]\tLoss: 2.302765\n",
            "INFO:root:Train Epoch: 10 [29440/60000 (49%)]\tLoss: 2.302381\n",
            "INFO:root:Train Epoch: 10 [30080/60000 (50%)]\tLoss: 2.302408\n",
            "INFO:root:Train Epoch: 10 [30720/60000 (51%)]\tLoss: 2.302471\n",
            "INFO:root:Train Epoch: 10 [31360/60000 (52%)]\tLoss: 2.302663\n",
            "INFO:root:Train Epoch: 10 [32000/60000 (53%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 10 [32640/60000 (54%)]\tLoss: 2.302357\n",
            "INFO:root:Train Epoch: 10 [33280/60000 (55%)]\tLoss: 2.302754\n",
            "INFO:root:Train Epoch: 10 [33920/60000 (57%)]\tLoss: 2.302331\n",
            "INFO:root:Train Epoch: 10 [34560/60000 (58%)]\tLoss: 2.302729\n",
            "INFO:root:Train Epoch: 10 [35200/60000 (59%)]\tLoss: 2.302507\n",
            "INFO:root:Train Epoch: 10 [35840/60000 (60%)]\tLoss: 2.302295\n",
            "INFO:root:Train Epoch: 10 [36480/60000 (61%)]\tLoss: 2.302561\n",
            "INFO:root:Train Epoch: 10 [37120/60000 (62%)]\tLoss: 2.302671\n",
            "INFO:root:Train Epoch: 10 [37760/60000 (63%)]\tLoss: 2.302210\n",
            "INFO:root:Train Epoch: 10 [38400/60000 (64%)]\tLoss: 2.302717\n",
            "INFO:root:Train Epoch: 10 [39040/60000 (65%)]\tLoss: 2.302575\n",
            "INFO:root:Train Epoch: 10 [39680/60000 (66%)]\tLoss: 2.302471\n",
            "INFO:root:Train Epoch: 10 [40320/60000 (67%)]\tLoss: 2.302444\n",
            "INFO:root:Train Epoch: 10 [40960/60000 (68%)]\tLoss: 2.302595\n",
            "INFO:root:Train Epoch: 10 [41600/60000 (69%)]\tLoss: 2.302665\n",
            "INFO:root:Train Epoch: 10 [42240/60000 (70%)]\tLoss: 2.302519\n",
            "INFO:root:Train Epoch: 10 [42880/60000 (71%)]\tLoss: 2.302555\n",
            "INFO:root:Train Epoch: 10 [43520/60000 (72%)]\tLoss: 2.302359\n",
            "INFO:root:Train Epoch: 10 [44160/60000 (74%)]\tLoss: 2.302140\n",
            "INFO:root:Train Epoch: 10 [44800/60000 (75%)]\tLoss: 2.302093\n",
            "INFO:root:Train Epoch: 10 [45440/60000 (76%)]\tLoss: 2.302375\n",
            "INFO:root:Train Epoch: 10 [46080/60000 (77%)]\tLoss: 2.302523\n",
            "INFO:root:Train Epoch: 10 [46720/60000 (78%)]\tLoss: 2.302641\n",
            "INFO:root:Train Epoch: 10 [47360/60000 (79%)]\tLoss: 2.302361\n",
            "INFO:root:Train Epoch: 10 [48000/60000 (80%)]\tLoss: 2.302468\n",
            "INFO:root:Train Epoch: 10 [48640/60000 (81%)]\tLoss: 2.302497\n",
            "INFO:root:Train Epoch: 10 [49280/60000 (82%)]\tLoss: 2.301872\n",
            "INFO:root:Train Epoch: 10 [49920/60000 (83%)]\tLoss: 2.302058\n",
            "INFO:root:Train Epoch: 10 [50560/60000 (84%)]\tLoss: 2.302298\n",
            "INFO:root:Train Epoch: 10 [51200/60000 (85%)]\tLoss: 2.302513\n",
            "INFO:root:Train Epoch: 10 [51840/60000 (86%)]\tLoss: 2.302462\n",
            "INFO:root:Train Epoch: 10 [52480/60000 (87%)]\tLoss: 2.302327\n",
            "INFO:root:Train Epoch: 10 [53120/60000 (88%)]\tLoss: 2.302425\n",
            "INFO:root:Train Epoch: 10 [53760/60000 (90%)]\tLoss: 2.302423\n",
            "INFO:root:Train Epoch: 10 [54400/60000 (91%)]\tLoss: 2.302623\n",
            "INFO:root:Train Epoch: 10 [55040/60000 (92%)]\tLoss: 2.302736\n",
            "INFO:root:Train Epoch: 10 [55680/60000 (93%)]\tLoss: 2.302724\n",
            "INFO:root:Train Epoch: 10 [56320/60000 (94%)]\tLoss: 2.302722\n",
            "INFO:root:Train Epoch: 10 [56960/60000 (95%)]\tLoss: 2.302622\n",
            "INFO:root:Train Epoch: 10 [57600/60000 (96%)]\tLoss: 2.302308\n",
            "INFO:root:Train Epoch: 10 [58240/60000 (97%)]\tLoss: 2.302425\n",
            "INFO:root:Train Epoch: 10 [58880/60000 (98%)]\tLoss: 2.302468\n",
            "INFO:root:Train Epoch: 10 [59520/60000 (99%)]\tLoss: 2.302520\n",
            "INFO:root:\n",
            "Test set: Average loss: 2.3025, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "INFO:root:Train Epoch: 11 [0/60000 (0%)]\tLoss: 2.302530\n",
            "INFO:root:Train Epoch: 11 [640/60000 (1%)]\tLoss: 2.302481\n",
            "INFO:root:Train Epoch: 11 [1280/60000 (2%)]\tLoss: 2.302393\n",
            "INFO:root:Train Epoch: 11 [1920/60000 (3%)]\tLoss: 2.302252\n",
            "INFO:root:Train Epoch: 11 [2560/60000 (4%)]\tLoss: 2.302533\n",
            "INFO:root:Train Epoch: 11 [3200/60000 (5%)]\tLoss: 2.302599\n",
            "INFO:root:Train Epoch: 11 [3840/60000 (6%)]\tLoss: 2.302577\n",
            "INFO:root:Train Epoch: 11 [4480/60000 (7%)]\tLoss: 2.302449\n",
            "INFO:root:Train Epoch: 11 [5120/60000 (9%)]\tLoss: 2.302586\n",
            "INFO:root:Train Epoch: 11 [5760/60000 (10%)]\tLoss: 2.302380\n",
            "INFO:root:Train Epoch: 11 [6400/60000 (11%)]\tLoss: 2.302471\n",
            "INFO:root:Train Epoch: 11 [7040/60000 (12%)]\tLoss: 2.302408\n",
            "INFO:root:Train Epoch: 11 [7680/60000 (13%)]\tLoss: 2.302120\n",
            "INFO:root:Train Epoch: 11 [8320/60000 (14%)]\tLoss: 2.302576\n",
            "INFO:root:Train Epoch: 11 [8960/60000 (15%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 11 [9600/60000 (16%)]\tLoss: 2.302266\n",
            "INFO:root:Train Epoch: 11 [10240/60000 (17%)]\tLoss: 2.302359\n",
            "INFO:root:Train Epoch: 11 [10880/60000 (18%)]\tLoss: 2.302518\n",
            "INFO:root:Train Epoch: 11 [11520/60000 (19%)]\tLoss: 2.302461\n",
            "INFO:root:Train Epoch: 11 [12160/60000 (20%)]\tLoss: 2.302135\n",
            "INFO:root:Train Epoch: 11 [12800/60000 (21%)]\tLoss: 2.302221\n",
            "INFO:root:Train Epoch: 11 [13440/60000 (22%)]\tLoss: 2.302748\n",
            "INFO:root:Train Epoch: 11 [14080/60000 (23%)]\tLoss: 2.302594\n",
            "INFO:root:Train Epoch: 11 [14720/60000 (25%)]\tLoss: 2.302503\n",
            "INFO:root:Train Epoch: 11 [15360/60000 (26%)]\tLoss: 2.302667\n",
            "INFO:root:Train Epoch: 11 [16000/60000 (27%)]\tLoss: 2.302789\n",
            "INFO:root:Train Epoch: 11 [16640/60000 (28%)]\tLoss: 2.302089\n",
            "INFO:root:Train Epoch: 11 [17280/60000 (29%)]\tLoss: 2.302825\n",
            "INFO:root:Train Epoch: 11 [17920/60000 (30%)]\tLoss: 2.302449\n",
            "INFO:root:Train Epoch: 11 [18560/60000 (31%)]\tLoss: 2.302613\n",
            "INFO:root:Train Epoch: 11 [19200/60000 (32%)]\tLoss: 2.302706\n",
            "INFO:root:Train Epoch: 11 [19840/60000 (33%)]\tLoss: 2.302598\n",
            "INFO:root:Train Epoch: 11 [20480/60000 (34%)]\tLoss: 2.302740\n",
            "INFO:root:Train Epoch: 11 [21120/60000 (35%)]\tLoss: 2.302415\n",
            "INFO:root:Train Epoch: 11 [21760/60000 (36%)]\tLoss: 2.302724\n",
            "INFO:root:Train Epoch: 11 [22400/60000 (37%)]\tLoss: 2.302493\n",
            "INFO:root:Train Epoch: 11 [23040/60000 (38%)]\tLoss: 2.302234\n",
            "INFO:root:Train Epoch: 11 [23680/60000 (39%)]\tLoss: 2.302571\n",
            "INFO:root:Train Epoch: 11 [24320/60000 (41%)]\tLoss: 2.302234\n",
            "INFO:root:Train Epoch: 11 [24960/60000 (42%)]\tLoss: 2.302397\n",
            "INFO:root:Train Epoch: 11 [25600/60000 (43%)]\tLoss: 2.302446\n",
            "INFO:root:Train Epoch: 11 [26240/60000 (44%)]\tLoss: 2.302358\n",
            "INFO:root:Train Epoch: 11 [26880/60000 (45%)]\tLoss: 2.302356\n",
            "INFO:root:Train Epoch: 11 [27520/60000 (46%)]\tLoss: 2.302305\n",
            "INFO:root:Train Epoch: 11 [28160/60000 (47%)]\tLoss: 2.302814\n",
            "INFO:root:Train Epoch: 11 [28800/60000 (48%)]\tLoss: 2.302767\n",
            "INFO:root:Train Epoch: 11 [29440/60000 (49%)]\tLoss: 2.302378\n",
            "INFO:root:Train Epoch: 11 [30080/60000 (50%)]\tLoss: 2.302406\n",
            "INFO:root:Train Epoch: 11 [30720/60000 (51%)]\tLoss: 2.302470\n",
            "INFO:root:Train Epoch: 11 [31360/60000 (52%)]\tLoss: 2.302663\n",
            "INFO:root:Train Epoch: 11 [32000/60000 (53%)]\tLoss: 2.302584\n",
            "INFO:root:Train Epoch: 11 [32640/60000 (54%)]\tLoss: 2.302355\n",
            "INFO:root:Train Epoch: 11 [33280/60000 (55%)]\tLoss: 2.302756\n",
            "INFO:root:Train Epoch: 11 [33920/60000 (57%)]\tLoss: 2.302329\n",
            "INFO:root:Train Epoch: 11 [34560/60000 (58%)]\tLoss: 2.302731\n",
            "INFO:root:Train Epoch: 11 [35200/60000 (59%)]\tLoss: 2.302506\n",
            "INFO:root:Train Epoch: 11 [35840/60000 (60%)]\tLoss: 2.302292\n",
            "INFO:root:Train Epoch: 11 [36480/60000 (61%)]\tLoss: 2.302561\n",
            "INFO:root:Train Epoch: 11 [37120/60000 (62%)]\tLoss: 2.302672\n",
            "INFO:root:Train Epoch: 11 [37760/60000 (63%)]\tLoss: 2.302206\n",
            "INFO:root:Train Epoch: 11 [38400/60000 (64%)]\tLoss: 2.302717\n",
            "INFO:root:Train Epoch: 11 [39040/60000 (65%)]\tLoss: 2.302575\n",
            "INFO:root:Train Epoch: 11 [39680/60000 (66%)]\tLoss: 2.302470\n",
            "INFO:root:Train Epoch: 11 [40320/60000 (67%)]\tLoss: 2.302443\n",
            "INFO:root:Train Epoch: 11 [40960/60000 (68%)]\tLoss: 2.302595\n",
            "INFO:root:Train Epoch: 11 [41600/60000 (69%)]\tLoss: 2.302666\n",
            "INFO:root:Train Epoch: 11 [42240/60000 (70%)]\tLoss: 2.302518\n",
            "INFO:root:Train Epoch: 11 [42880/60000 (71%)]\tLoss: 2.302555\n",
            "INFO:root:Train Epoch: 11 [43520/60000 (72%)]\tLoss: 2.302357\n",
            "INFO:root:Train Epoch: 11 [44160/60000 (74%)]\tLoss: 2.302135\n",
            "INFO:root:Train Epoch: 11 [44800/60000 (75%)]\tLoss: 2.302088\n",
            "INFO:root:Train Epoch: 11 [45440/60000 (76%)]\tLoss: 2.302373\n",
            "INFO:root:Train Epoch: 11 [46080/60000 (77%)]\tLoss: 2.302522\n",
            "INFO:root:Train Epoch: 11 [46720/60000 (78%)]\tLoss: 2.302642\n",
            "INFO:root:Train Epoch: 11 [47360/60000 (79%)]\tLoss: 2.302358\n",
            "INFO:root:Train Epoch: 11 [48000/60000 (80%)]\tLoss: 2.302466\n",
            "INFO:root:Train Epoch: 11 [48640/60000 (81%)]\tLoss: 2.302496\n",
            "INFO:root:Train Epoch: 11 [49280/60000 (82%)]\tLoss: 2.301865\n",
            "INFO:root:Train Epoch: 11 [49920/60000 (83%)]\tLoss: 2.302053\n",
            "INFO:root:Train Epoch: 11 [50560/60000 (84%)]\tLoss: 2.302295\n",
            "INFO:root:Train Epoch: 11 [51200/60000 (85%)]\tLoss: 2.302512\n",
            "INFO:root:Train Epoch: 11 [51840/60000 (86%)]\tLoss: 2.302460\n",
            "INFO:root:Train Epoch: 11 [52480/60000 (87%)]\tLoss: 2.302324\n",
            "INFO:root:Train Epoch: 11 [53120/60000 (88%)]\tLoss: 2.302423\n",
            "INFO:root:Train Epoch: 11 [53760/60000 (90%)]\tLoss: 2.302422\n",
            "INFO:root:Train Epoch: 11 [54400/60000 (91%)]\tLoss: 2.302624\n",
            "INFO:root:Train Epoch: 11 [55040/60000 (92%)]\tLoss: 2.302737\n",
            "INFO:root:Train Epoch: 11 [55680/60000 (93%)]\tLoss: 2.302724\n",
            "INFO:root:Train Epoch: 11 [56320/60000 (94%)]\tLoss: 2.302724\n",
            "INFO:root:Train Epoch: 11 [56960/60000 (95%)]\tLoss: 2.302623\n",
            "INFO:root:Train Epoch: 11 [57600/60000 (96%)]\tLoss: 2.302306\n",
            "INFO:root:Train Epoch: 11 [58240/60000 (97%)]\tLoss: 2.302423\n",
            "INFO:root:Train Epoch: 11 [58880/60000 (98%)]\tLoss: 2.302467\n",
            "INFO:root:Train Epoch: 11 [59520/60000 (99%)]\tLoss: 2.302520\n",
            "INFO:root:\n",
            "Test set: Average loss: 2.3025, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "INFO:root:Train Epoch: 12 [0/60000 (0%)]\tLoss: 2.302530\n",
            "INFO:root:Train Epoch: 12 [640/60000 (1%)]\tLoss: 2.302480\n",
            "INFO:root:Train Epoch: 12 [1280/60000 (2%)]\tLoss: 2.302391\n",
            "INFO:root:Train Epoch: 12 [1920/60000 (3%)]\tLoss: 2.302249\n",
            "INFO:root:Train Epoch: 12 [2560/60000 (4%)]\tLoss: 2.302533\n",
            "INFO:root:Train Epoch: 12 [3200/60000 (5%)]\tLoss: 2.302599\n",
            "INFO:root:Train Epoch: 12 [3840/60000 (6%)]\tLoss: 2.302577\n",
            "INFO:root:Train Epoch: 12 [4480/60000 (7%)]\tLoss: 2.302448\n",
            "INFO:root:Train Epoch: 12 [5120/60000 (9%)]\tLoss: 2.302587\n",
            "INFO:root:Train Epoch: 12 [5760/60000 (10%)]\tLoss: 2.302378\n",
            "INFO:root:Train Epoch: 12 [6400/60000 (11%)]\tLoss: 2.302470\n",
            "INFO:root:Train Epoch: 12 [7040/60000 (12%)]\tLoss: 2.302407\n",
            "INFO:root:Train Epoch: 12 [7680/60000 (13%)]\tLoss: 2.302116\n",
            "INFO:root:Train Epoch: 12 [8320/60000 (14%)]\tLoss: 2.302576\n",
            "INFO:root:Train Epoch: 12 [8960/60000 (15%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 12 [9600/60000 (16%)]\tLoss: 2.302264\n",
            "INFO:root:Train Epoch: 12 [10240/60000 (17%)]\tLoss: 2.302357\n",
            "INFO:root:Train Epoch: 12 [10880/60000 (18%)]\tLoss: 2.302517\n",
            "INFO:root:Train Epoch: 12 [11520/60000 (19%)]\tLoss: 2.302460\n",
            "INFO:root:Train Epoch: 12 [12160/60000 (20%)]\tLoss: 2.302132\n",
            "INFO:root:Train Epoch: 12 [12800/60000 (21%)]\tLoss: 2.302218\n",
            "INFO:root:Train Epoch: 12 [13440/60000 (22%)]\tLoss: 2.302749\n",
            "INFO:root:Train Epoch: 12 [14080/60000 (23%)]\tLoss: 2.302594\n",
            "INFO:root:Train Epoch: 12 [14720/60000 (25%)]\tLoss: 2.302502\n",
            "INFO:root:Train Epoch: 12 [15360/60000 (26%)]\tLoss: 2.302667\n",
            "INFO:root:Train Epoch: 12 [16000/60000 (27%)]\tLoss: 2.302791\n",
            "INFO:root:Train Epoch: 12 [16640/60000 (28%)]\tLoss: 2.302086\n",
            "INFO:root:Train Epoch: 12 [17280/60000 (29%)]\tLoss: 2.302827\n",
            "INFO:root:Train Epoch: 12 [17920/60000 (30%)]\tLoss: 2.302448\n",
            "INFO:root:Train Epoch: 12 [18560/60000 (31%)]\tLoss: 2.302613\n",
            "INFO:root:Train Epoch: 12 [19200/60000 (32%)]\tLoss: 2.302707\n",
            "INFO:root:Train Epoch: 12 [19840/60000 (33%)]\tLoss: 2.302599\n",
            "INFO:root:Train Epoch: 12 [20480/60000 (34%)]\tLoss: 2.302742\n",
            "INFO:root:Train Epoch: 12 [21120/60000 (35%)]\tLoss: 2.302414\n",
            "INFO:root:Train Epoch: 12 [21760/60000 (36%)]\tLoss: 2.302724\n",
            "INFO:root:Train Epoch: 12 [22400/60000 (37%)]\tLoss: 2.302493\n",
            "INFO:root:Train Epoch: 12 [23040/60000 (38%)]\tLoss: 2.302232\n",
            "INFO:root:Train Epoch: 12 [23680/60000 (39%)]\tLoss: 2.302571\n",
            "INFO:root:Train Epoch: 12 [24320/60000 (41%)]\tLoss: 2.302231\n",
            "INFO:root:Train Epoch: 12 [24960/60000 (42%)]\tLoss: 2.302395\n",
            "INFO:root:Train Epoch: 12 [25600/60000 (43%)]\tLoss: 2.302445\n",
            "INFO:root:Train Epoch: 12 [26240/60000 (44%)]\tLoss: 2.302357\n",
            "INFO:root:Train Epoch: 12 [26880/60000 (45%)]\tLoss: 2.302354\n",
            "INFO:root:Train Epoch: 12 [27520/60000 (46%)]\tLoss: 2.302302\n",
            "INFO:root:Train Epoch: 12 [28160/60000 (47%)]\tLoss: 2.302816\n",
            "INFO:root:Train Epoch: 12 [28800/60000 (48%)]\tLoss: 2.302769\n",
            "INFO:root:Train Epoch: 12 [29440/60000 (49%)]\tLoss: 2.302377\n",
            "INFO:root:Train Epoch: 12 [30080/60000 (50%)]\tLoss: 2.302405\n",
            "INFO:root:Train Epoch: 12 [30720/60000 (51%)]\tLoss: 2.302470\n",
            "INFO:root:Train Epoch: 12 [31360/60000 (52%)]\tLoss: 2.302664\n",
            "INFO:root:Train Epoch: 12 [32000/60000 (53%)]\tLoss: 2.302583\n",
            "INFO:root:Train Epoch: 12 [32640/60000 (54%)]\tLoss: 2.302352\n",
            "INFO:root:Train Epoch: 12 [33280/60000 (55%)]\tLoss: 2.302757\n",
            "INFO:root:Train Epoch: 12 [33920/60000 (57%)]\tLoss: 2.302327\n",
            "INFO:root:Train Epoch: 12 [34560/60000 (58%)]\tLoss: 2.302731\n",
            "INFO:root:Train Epoch: 12 [35200/60000 (59%)]\tLoss: 2.302506\n",
            "INFO:root:Train Epoch: 12 [35840/60000 (60%)]\tLoss: 2.302289\n",
            "INFO:root:Train Epoch: 12 [36480/60000 (61%)]\tLoss: 2.302561\n",
            "INFO:root:Train Epoch: 12 [37120/60000 (62%)]\tLoss: 2.302673\n",
            "INFO:root:Train Epoch: 12 [37760/60000 (63%)]\tLoss: 2.302203\n",
            "INFO:root:Train Epoch: 12 [38400/60000 (64%)]\tLoss: 2.302718\n",
            "INFO:root:Train Epoch: 12 [39040/60000 (65%)]\tLoss: 2.302574\n",
            "INFO:root:Train Epoch: 12 [39680/60000 (66%)]\tLoss: 2.302469\n",
            "INFO:root:Train Epoch: 12 [40320/60000 (67%)]\tLoss: 2.302442\n",
            "INFO:root:Train Epoch: 12 [40960/60000 (68%)]\tLoss: 2.302596\n",
            "INFO:root:Train Epoch: 12 [41600/60000 (69%)]\tLoss: 2.302666\n",
            "INFO:root:Train Epoch: 12 [42240/60000 (70%)]\tLoss: 2.302517\n",
            "INFO:root:Train Epoch: 12 [42880/60000 (71%)]\tLoss: 2.302555\n",
            "INFO:root:Train Epoch: 12 [43520/60000 (72%)]\tLoss: 2.302356\n",
            "INFO:root:Train Epoch: 12 [44160/60000 (74%)]\tLoss: 2.302132\n",
            "INFO:root:Train Epoch: 12 [44800/60000 (75%)]\tLoss: 2.302085\n",
            "INFO:root:Train Epoch: 12 [45440/60000 (76%)]\tLoss: 2.302372\n",
            "INFO:root:Train Epoch: 12 [46080/60000 (77%)]\tLoss: 2.302521\n",
            "INFO:root:Train Epoch: 12 [46720/60000 (78%)]\tLoss: 2.302642\n",
            "INFO:root:Train Epoch: 12 [47360/60000 (79%)]\tLoss: 2.302357\n",
            "INFO:root:Train Epoch: 12 [48000/60000 (80%)]\tLoss: 2.302466\n",
            "INFO:root:Train Epoch: 12 [48640/60000 (81%)]\tLoss: 2.302496\n",
            "INFO:root:Train Epoch: 12 [49280/60000 (82%)]\tLoss: 2.301861\n",
            "INFO:root:Train Epoch: 12 [49920/60000 (83%)]\tLoss: 2.302050\n",
            "INFO:root:Train Epoch: 12 [50560/60000 (84%)]\tLoss: 2.302294\n",
            "INFO:root:Train Epoch: 12 [51200/60000 (85%)]\tLoss: 2.302511\n",
            "INFO:root:Train Epoch: 12 [51840/60000 (86%)]\tLoss: 2.302460\n",
            "INFO:root:Train Epoch: 12 [52480/60000 (87%)]\tLoss: 2.302322\n",
            "INFO:root:Train Epoch: 12 [53120/60000 (88%)]\tLoss: 2.302422\n",
            "INFO:root:Train Epoch: 12 [53760/60000 (90%)]\tLoss: 2.302421\n",
            "INFO:root:Train Epoch: 12 [54400/60000 (91%)]\tLoss: 2.302624\n",
            "INFO:root:Train Epoch: 12 [55040/60000 (92%)]\tLoss: 2.302738\n",
            "INFO:root:Train Epoch: 12 [55680/60000 (93%)]\tLoss: 2.302725\n",
            "INFO:root:Train Epoch: 12 [56320/60000 (94%)]\tLoss: 2.302725\n",
            "INFO:root:Train Epoch: 12 [56960/60000 (95%)]\tLoss: 2.302623\n",
            "INFO:root:Train Epoch: 12 [57600/60000 (96%)]\tLoss: 2.302304\n",
            "INFO:root:Train Epoch: 12 [58240/60000 (97%)]\tLoss: 2.302423\n",
            "INFO:root:Train Epoch: 12 [58880/60000 (98%)]\tLoss: 2.302466\n",
            "INFO:root:Train Epoch: 12 [59520/60000 (99%)]\tLoss: 2.302520\n",
            "INFO:root:\n",
            "Test set: Average loss: 2.3025, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "INFO:root:Train Epoch: 13 [0/60000 (0%)]\tLoss: 2.302529\n",
            "INFO:root:Train Epoch: 13 [640/60000 (1%)]\tLoss: 2.302479\n",
            "INFO:root:Train Epoch: 13 [1280/60000 (2%)]\tLoss: 2.302390\n",
            "INFO:root:Train Epoch: 13 [1920/60000 (3%)]\tLoss: 2.302247\n",
            "INFO:root:Train Epoch: 13 [2560/60000 (4%)]\tLoss: 2.302532\n",
            "INFO:root:Train Epoch: 13 [3200/60000 (5%)]\tLoss: 2.302599\n",
            "INFO:root:Train Epoch: 13 [3840/60000 (6%)]\tLoss: 2.302577\n",
            "INFO:root:Train Epoch: 13 [4480/60000 (7%)]\tLoss: 2.302447\n",
            "INFO:root:Train Epoch: 13 [5120/60000 (9%)]\tLoss: 2.302587\n",
            "INFO:root:Train Epoch: 13 [5760/60000 (10%)]\tLoss: 2.302377\n",
            "INFO:root:Train Epoch: 13 [6400/60000 (11%)]\tLoss: 2.302469\n",
            "INFO:root:Train Epoch: 13 [7040/60000 (12%)]\tLoss: 2.302406\n",
            "INFO:root:Train Epoch: 13 [7680/60000 (13%)]\tLoss: 2.302114\n",
            "INFO:root:Train Epoch: 13 [8320/60000 (14%)]\tLoss: 2.302576\n",
            "INFO:root:Train Epoch: 13 [8960/60000 (15%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 13 [9600/60000 (16%)]\tLoss: 2.302262\n",
            "INFO:root:Train Epoch: 13 [10240/60000 (17%)]\tLoss: 2.302356\n",
            "INFO:root:Train Epoch: 13 [10880/60000 (18%)]\tLoss: 2.302517\n",
            "INFO:root:Train Epoch: 13 [11520/60000 (19%)]\tLoss: 2.302459\n",
            "INFO:root:Train Epoch: 13 [12160/60000 (20%)]\tLoss: 2.302130\n",
            "INFO:root:Train Epoch: 13 [12800/60000 (21%)]\tLoss: 2.302216\n",
            "INFO:root:Train Epoch: 13 [13440/60000 (22%)]\tLoss: 2.302750\n",
            "INFO:root:Train Epoch: 13 [14080/60000 (23%)]\tLoss: 2.302595\n",
            "INFO:root:Train Epoch: 13 [14720/60000 (25%)]\tLoss: 2.302502\n",
            "INFO:root:Train Epoch: 13 [15360/60000 (26%)]\tLoss: 2.302668\n",
            "INFO:root:Train Epoch: 13 [16000/60000 (27%)]\tLoss: 2.302792\n",
            "INFO:root:Train Epoch: 13 [16640/60000 (28%)]\tLoss: 2.302083\n",
            "INFO:root:Train Epoch: 13 [17280/60000 (29%)]\tLoss: 2.302829\n",
            "INFO:root:Train Epoch: 13 [17920/60000 (30%)]\tLoss: 2.302447\n",
            "INFO:root:Train Epoch: 13 [18560/60000 (31%)]\tLoss: 2.302613\n",
            "INFO:root:Train Epoch: 13 [19200/60000 (32%)]\tLoss: 2.302708\n",
            "INFO:root:Train Epoch: 13 [19840/60000 (33%)]\tLoss: 2.302599\n",
            "INFO:root:Train Epoch: 13 [20480/60000 (34%)]\tLoss: 2.302742\n",
            "INFO:root:Train Epoch: 13 [21120/60000 (35%)]\tLoss: 2.302414\n",
            "INFO:root:Train Epoch: 13 [21760/60000 (36%)]\tLoss: 2.302725\n",
            "INFO:root:Train Epoch: 13 [22400/60000 (37%)]\tLoss: 2.302492\n",
            "INFO:root:Train Epoch: 13 [23040/60000 (38%)]\tLoss: 2.302229\n",
            "INFO:root:Train Epoch: 13 [23680/60000 (39%)]\tLoss: 2.302571\n",
            "INFO:root:Train Epoch: 13 [24320/60000 (41%)]\tLoss: 2.302229\n",
            "INFO:root:Train Epoch: 13 [24960/60000 (42%)]\tLoss: 2.302394\n",
            "INFO:root:Train Epoch: 13 [25600/60000 (43%)]\tLoss: 2.302444\n",
            "INFO:root:Train Epoch: 13 [26240/60000 (44%)]\tLoss: 2.302355\n",
            "INFO:root:Train Epoch: 13 [26880/60000 (45%)]\tLoss: 2.302353\n",
            "INFO:root:Train Epoch: 13 [27520/60000 (46%)]\tLoss: 2.302301\n",
            "INFO:root:Train Epoch: 13 [28160/60000 (47%)]\tLoss: 2.302817\n",
            "INFO:root:Train Epoch: 13 [28800/60000 (48%)]\tLoss: 2.302770\n",
            "INFO:root:Train Epoch: 13 [29440/60000 (49%)]\tLoss: 2.302376\n",
            "INFO:root:Train Epoch: 13 [30080/60000 (50%)]\tLoss: 2.302403\n",
            "INFO:root:Train Epoch: 13 [30720/60000 (51%)]\tLoss: 2.302469\n",
            "INFO:root:Train Epoch: 13 [31360/60000 (52%)]\tLoss: 2.302664\n",
            "INFO:root:Train Epoch: 13 [32000/60000 (53%)]\tLoss: 2.302584\n",
            "INFO:root:Train Epoch: 13 [32640/60000 (54%)]\tLoss: 2.302351\n",
            "INFO:root:Train Epoch: 13 [33280/60000 (55%)]\tLoss: 2.302758\n",
            "INFO:root:Train Epoch: 13 [33920/60000 (57%)]\tLoss: 2.302325\n",
            "INFO:root:Train Epoch: 13 [34560/60000 (58%)]\tLoss: 2.302732\n",
            "INFO:root:Train Epoch: 13 [35200/60000 (59%)]\tLoss: 2.302505\n",
            "INFO:root:Train Epoch: 13 [35840/60000 (60%)]\tLoss: 2.302289\n",
            "INFO:root:Train Epoch: 13 [36480/60000 (61%)]\tLoss: 2.302560\n",
            "INFO:root:Train Epoch: 13 [37120/60000 (62%)]\tLoss: 2.302673\n",
            "INFO:root:Train Epoch: 13 [37760/60000 (63%)]\tLoss: 2.302201\n",
            "INFO:root:Train Epoch: 13 [38400/60000 (64%)]\tLoss: 2.302719\n",
            "INFO:root:Train Epoch: 13 [39040/60000 (65%)]\tLoss: 2.302575\n",
            "INFO:root:Train Epoch: 13 [39680/60000 (66%)]\tLoss: 2.302468\n",
            "INFO:root:Train Epoch: 13 [40320/60000 (67%)]\tLoss: 2.302441\n",
            "INFO:root:Train Epoch: 13 [40960/60000 (68%)]\tLoss: 2.302596\n",
            "INFO:root:Train Epoch: 13 [41600/60000 (69%)]\tLoss: 2.302667\n",
            "INFO:root:Train Epoch: 13 [42240/60000 (70%)]\tLoss: 2.302517\n",
            "INFO:root:Train Epoch: 13 [42880/60000 (71%)]\tLoss: 2.302555\n",
            "INFO:root:Train Epoch: 13 [43520/60000 (72%)]\tLoss: 2.302355\n",
            "INFO:root:Train Epoch: 13 [44160/60000 (74%)]\tLoss: 2.302130\n",
            "INFO:root:Train Epoch: 13 [44800/60000 (75%)]\tLoss: 2.302083\n",
            "INFO:root:Train Epoch: 13 [45440/60000 (76%)]\tLoss: 2.302371\n",
            "INFO:root:Train Epoch: 13 [46080/60000 (77%)]\tLoss: 2.302521\n",
            "INFO:root:Train Epoch: 13 [46720/60000 (78%)]\tLoss: 2.302643\n",
            "INFO:root:Train Epoch: 13 [47360/60000 (79%)]\tLoss: 2.302356\n",
            "INFO:root:Train Epoch: 13 [48000/60000 (80%)]\tLoss: 2.302464\n",
            "INFO:root:Train Epoch: 13 [48640/60000 (81%)]\tLoss: 2.302496\n",
            "INFO:root:Train Epoch: 13 [49280/60000 (82%)]\tLoss: 2.301857\n",
            "INFO:root:Train Epoch: 13 [49920/60000 (83%)]\tLoss: 2.302047\n",
            "INFO:root:Train Epoch: 13 [50560/60000 (84%)]\tLoss: 2.302293\n",
            "INFO:root:Train Epoch: 13 [51200/60000 (85%)]\tLoss: 2.302511\n",
            "INFO:root:Train Epoch: 13 [51840/60000 (86%)]\tLoss: 2.302459\n",
            "INFO:root:Train Epoch: 13 [52480/60000 (87%)]\tLoss: 2.302322\n",
            "INFO:root:Train Epoch: 13 [53120/60000 (88%)]\tLoss: 2.302422\n",
            "INFO:root:Train Epoch: 13 [53760/60000 (90%)]\tLoss: 2.302420\n",
            "INFO:root:Train Epoch: 13 [54400/60000 (91%)]\tLoss: 2.302624\n",
            "INFO:root:Train Epoch: 13 [55040/60000 (92%)]\tLoss: 2.302739\n",
            "INFO:root:Train Epoch: 13 [55680/60000 (93%)]\tLoss: 2.302726\n",
            "INFO:root:Train Epoch: 13 [56320/60000 (94%)]\tLoss: 2.302725\n",
            "INFO:root:Train Epoch: 13 [56960/60000 (95%)]\tLoss: 2.302624\n",
            "INFO:root:Train Epoch: 13 [57600/60000 (96%)]\tLoss: 2.302303\n",
            "INFO:root:Train Epoch: 13 [58240/60000 (97%)]\tLoss: 2.302422\n",
            "INFO:root:Train Epoch: 13 [58880/60000 (98%)]\tLoss: 2.302466\n",
            "INFO:root:Train Epoch: 13 [59520/60000 (99%)]\tLoss: 2.302519\n",
            "INFO:root:\n",
            "Test set: Average loss: 2.3025, Accuracy: 1135/10000 (11%)\n",
            "\n",
            "INFO:root:Train Epoch: 14 [0/60000 (0%)]\tLoss: 2.302529\n",
            "INFO:root:Train Epoch: 14 [640/60000 (1%)]\tLoss: 2.302479\n",
            "INFO:root:Train Epoch: 14 [1280/60000 (2%)]\tLoss: 2.302389\n",
            "INFO:root:Train Epoch: 14 [1920/60000 (3%)]\tLoss: 2.302246\n",
            "INFO:root:Train Epoch: 14 [2560/60000 (4%)]\tLoss: 2.302532\n",
            "INFO:root:Train Epoch: 14 [3200/60000 (5%)]\tLoss: 2.302600\n",
            "INFO:root:Train Epoch: 14 [3840/60000 (6%)]\tLoss: 2.302577\n",
            "INFO:root:Train Epoch: 14 [4480/60000 (7%)]\tLoss: 2.302446\n",
            "INFO:root:Train Epoch: 14 [5120/60000 (9%)]\tLoss: 2.302587\n",
            "INFO:root:Train Epoch: 14 [5760/60000 (10%)]\tLoss: 2.302377\n",
            "INFO:root:Train Epoch: 14 [6400/60000 (11%)]\tLoss: 2.302469\n",
            "INFO:root:Train Epoch: 14 [7040/60000 (12%)]\tLoss: 2.302405\n",
            "INFO:root:Train Epoch: 14 [7680/60000 (13%)]\tLoss: 2.302111\n",
            "INFO:root:Train Epoch: 14 [8320/60000 (14%)]\tLoss: 2.302576\n",
            "INFO:root:Train Epoch: 14 [8960/60000 (15%)]\tLoss: 2.302588\n",
            "INFO:root:Train Epoch: 14 [9600/60000 (16%)]\tLoss: 2.302261\n",
            "INFO:root:Train Epoch: 14 [10240/60000 (17%)]\tLoss: 2.302355\n",
            "INFO:root:Train Epoch: 14 [10880/60000 (18%)]\tLoss: 2.302517\n",
            "INFO:root:Train Epoch: 14 [11520/60000 (19%)]\tLoss: 2.302459\n",
            "INFO:root:Train Epoch: 14 [12160/60000 (20%)]\tLoss: 2.302128\n",
            "INFO:root:Train Epoch: 14 [12800/60000 (21%)]\tLoss: 2.302215\n",
            "INFO:root:Train Epoch: 14 [13440/60000 (22%)]\tLoss: 2.302750\n",
            "INFO:root:Train Epoch: 14 [14080/60000 (23%)]\tLoss: 2.302595\n",
            "INFO:root:Train Epoch: 14 [14720/60000 (25%)]\tLoss: 2.302502\n",
            "INFO:root:Train Epoch: 14 [15360/60000 (26%)]\tLoss: 2.302668\n",
            "INFO:root:Train Epoch: 14 [16000/60000 (27%)]\tLoss: 2.302793\n",
            "INFO:root:Train Epoch: 14 [16640/60000 (28%)]\tLoss: 2.302081\n",
            "INFO:root:Train Epoch: 14 [17280/60000 (29%)]\tLoss: 2.302830\n",
            "INFO:root:Train Epoch: 14 [17920/60000 (30%)]\tLoss: 2.302447\n",
            "INFO:root:Train Epoch: 14 [18560/60000 (31%)]\tLoss: 2.302613\n",
            "INFO:root:Train Epoch: 14 [19200/60000 (32%)]\tLoss: 2.302708\n",
            "INFO:root:Train Epoch: 14 [19840/60000 (33%)]\tLoss: 2.302599\n",
            "INFO:root:Train Epoch: 14 [20480/60000 (34%)]\tLoss: 2.302743\n",
            "INFO:root:Train Epoch: 14 [21120/60000 (35%)]\tLoss: 2.302413\n",
            "INFO:root:Train Epoch: 14 [21760/60000 (36%)]\tLoss: 2.302726\n",
            "INFO:root:Train Epoch: 14 [22400/60000 (37%)]\tLoss: 2.302491\n",
            "INFO:root:Train Epoch: 14 [23040/60000 (38%)]\tLoss: 2.302229\n",
            "INFO:root:Train Epoch: 14 [23680/60000 (39%)]\tLoss: 2.302571\n",
            "INFO:root:Train Epoch: 14 [24320/60000 (41%)]\tLoss: 2.302228\n",
            "INFO:root:Train Epoch: 14 [24960/60000 (42%)]\tLoss: 2.302393\n",
            "INFO:root:Train Epoch: 14 [25600/60000 (43%)]\tLoss: 2.302444\n",
            "INFO:root:Train Epoch: 14 [26240/60000 (44%)]\tLoss: 2.302355\n",
            "INFO:root:Train Epoch: 14 [26880/60000 (45%)]\tLoss: 2.302352\n",
            "INFO:root:Train Epoch: 14 [27520/60000 (46%)]\tLoss: 2.302300\n",
            "INFO:root:Train Epoch: 14 [28160/60000 (47%)]\tLoss: 2.302818\n",
            "INFO:root:Train Epoch: 14 [28800/60000 (48%)]\tLoss: 2.302771\n",
            "INFO:root:Train Epoch: 14 [29440/60000 (49%)]\tLoss: 2.302375\n",
            "INFO:root:Train Epoch: 14 [30080/60000 (50%)]\tLoss: 2.302403\n",
            "INFO:root:Train Epoch: 14 [30720/60000 (51%)]\tLoss: 2.302468\n",
            "INFO:root:Train Epoch: 14 [31360/60000 (52%)]\tLoss: 2.302665\n",
            "INFO:root:Train Epoch: 14 [32000/60000 (53%)]\tLoss: 2.302584\n",
            "INFO:root:Train Epoch: 14 [32640/60000 (54%)]\tLoss: 2.302351\n",
            "INFO:root:Train Epoch: 14 [33280/60000 (55%)]\tLoss: 2.302758\n",
            "INFO:root:Train Epoch: 14 [33920/60000 (57%)]\tLoss: 2.302325\n",
            "INFO:root:Train Epoch: 14 [34560/60000 (58%)]\tLoss: 2.302732\n",
            "INFO:root:Train Epoch: 14 [35200/60000 (59%)]\tLoss: 2.302505\n",
            "INFO:root:Train Epoch: 14 [35840/60000 (60%)]\tLoss: 2.302288\n",
            "INFO:root:Train Epoch: 14 [36480/60000 (61%)]\tLoss: 2.302560\n",
            "INFO:root:Train Epoch: 14 [37120/60000 (62%)]\tLoss: 2.302674\n",
            "INFO:root:Train Epoch: 14 [37760/60000 (63%)]\tLoss: 2.302200\n",
            "INFO:root:Train Epoch: 14 [38400/60000 (64%)]\tLoss: 2.302719\n",
            "INFO:root:Train Epoch: 14 [39040/60000 (65%)]\tLoss: 2.302575\n",
            "INFO:root:Train Epoch: 14 [39680/60000 (66%)]\tLoss: 2.302468\n",
            "INFO:root:Train Epoch: 14 [40320/60000 (67%)]\tLoss: 2.302440\n",
            "INFO:root:Train Epoch: 14 [40960/60000 (68%)]\tLoss: 2.302596\n",
            "INFO:root:Train Epoch: 14 [41600/60000 (69%)]\tLoss: 2.302667\n",
            "INFO:root:Train Epoch: 14 [42240/60000 (70%)]\tLoss: 2.302517\n",
            "INFO:root:Train Epoch: 14 [42880/60000 (71%)]\tLoss: 2.302554\n",
            "INFO:root:Train Epoch: 14 [43520/60000 (72%)]\tLoss: 2.302354\n",
            "INFO:root:Train Epoch: 14 [44160/60000 (74%)]\tLoss: 2.302128\n",
            "INFO:root:Train Epoch: 14 [44800/60000 (75%)]\tLoss: 2.302081\n",
            "INFO:root:Train Epoch: 14 [45440/60000 (76%)]\tLoss: 2.302370\n",
            "INFO:root:Train Epoch: 14 [46080/60000 (77%)]\tLoss: 2.302521\n",
            "INFO:root:Train Epoch: 14 [46720/60000 (78%)]\tLoss: 2.302643\n",
            "INFO:root:Train Epoch: 14 [47360/60000 (79%)]\tLoss: 2.302355\n",
            "INFO:root:Train Epoch: 14 [48000/60000 (80%)]\tLoss: 2.302464\n",
            "INFO:root:Train Epoch: 14 [48640/60000 (81%)]\tLoss: 2.302495\n",
            "INFO:root:Train Epoch: 14 [49280/60000 (82%)]\tLoss: 2.301855\n",
            "INFO:root:Train Epoch: 14 [49920/60000 (83%)]\tLoss: 2.302046\n",
            "INFO:root:Train Epoch: 14 [50560/60000 (84%)]\tLoss: 2.302291\n",
            "INFO:root:Train Epoch: 14 [51200/60000 (85%)]\tLoss: 2.302510\n",
            "INFO:root:Train Epoch: 14 [51840/60000 (86%)]\tLoss: 2.302459\n",
            "INFO:root:Train Epoch: 14 [52480/60000 (87%)]\tLoss: 2.302320\n",
            "INFO:root:Train Epoch: 14 [53120/60000 (88%)]\tLoss: 2.302420\n",
            "INFO:root:Train Epoch: 14 [53760/60000 (90%)]\tLoss: 2.302420\n",
            "INFO:root:Train Epoch: 14 [54400/60000 (91%)]\tLoss: 2.302624\n",
            "INFO:root:Train Epoch: 14 [55040/60000 (92%)]\tLoss: 2.302740\n",
            "INFO:root:Train Epoch: 14 [55680/60000 (93%)]\tLoss: 2.302726\n",
            "INFO:root:Train Epoch: 14 [56320/60000 (94%)]\tLoss: 2.302725\n",
            "INFO:root:Train Epoch: 14 [56960/60000 (95%)]\tLoss: 2.302624\n",
            "INFO:root:Train Epoch: 14 [57600/60000 (96%)]\tLoss: 2.302302\n",
            "INFO:root:Train Epoch: 14 [58240/60000 (97%)]\tLoss: 2.302421\n",
            "INFO:root:Train Epoch: 14 [58880/60000 (98%)]\tLoss: 2.302466\n",
            "INFO:root:Train Epoch: 14 [59520/60000 (99%)]\tLoss: 2.302519\n",
            "INFO:root:\n",
            "Test set: Average loss: 2.3025, Accuracy: 1135/10000 (11%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.30255693359375, 2.3025376220703127, 2.302524462890625, 2.302515087890625, 2.3025087158203124, 2.302504052734375, 2.3025008544921874, 2.3024987548828126, 2.3024974853515623, 2.3024958984375, 2.302495458984375, 2.3024951171875, 2.30249482421875, 2.3024942138671873]\n",
            "[11.35, 11.35, 11.35, 11.35, 11.35, 11.35, 11.35, 11.35, 11.35, 11.35, 11.35, 11.35, 11.35, 11.35]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tb"
      ],
      "metadata": {
        "id": "8rSo-g6qDIvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "109069c9-842f-4577-b783-c3363bf6f540"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No traceback available to show.\n"
          ]
        }
      ]
    }
  ]
}