{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QSparseLocal_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqZi1G51qcqkjqqgtJYa97",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverCore97/Bagua/blob/main/QSparseLocal_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Check CUDA version**"
      ],
      "metadata": {
        "id": "FidgI73Gc8m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biBKGSXi6Chp",
        "outputId": "9f763b2c-f631-40b9-d351-53545f890079"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Install Bagua**"
      ],
      "metadata": {
        "id": "H1UzpVrNc3wO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-qCGqkkr5mr4",
        "outputId": "333d8132-245c-41dd-aaeb-e69a4d222998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bagua-cuda111\n",
            "  Downloading bagua_cuda111-0.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.8 MB 750 kB/s \n",
            "\u001b[?25hCollecting gorilla==0.4.0\n",
            "  Downloading gorilla-0.4.0-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting scikit-learn!=1.0,<=1.0.1,>=0.24\n",
            "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 42.6 MB/s \n",
            "\u001b[?25hCollecting deprecation>=2.1\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.21.6)\n",
            "Collecting flask>=2.0\n",
            "  Downloading Flask-2.1.1-py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting setuptools-rust\n",
            "  Downloading setuptools_rust-1.3.0-py3-none-any.whl (21 kB)\n",
            "Collecting pytest-benchmark>=3.4\n",
            "  Downloading pytest_benchmark-3.4.1-py2.py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prometheus-client>=0.11 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (4.64.0)\n",
            "Collecting gevent>=21.8\n",
            "  Downloading gevent-21.12.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 26.6 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting parallel-ssh==2.8.0\n",
            "  Downloading parallel_ssh-2.8.0-py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 613 kB/s \n",
            "\u001b[?25hCollecting xxhash>=2.0\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 38.2 MB/s \n",
            "\u001b[?25hCollecting scikit-optimize>=0.8.1\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting requests>=2.25\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting pydantic>=1.8\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 8.2 MB/s \n",
            "\u001b[?25hCollecting ssh-python>=0.9.0\n",
            "  Downloading ssh_python-0.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 27.8 MB/s \n",
            "\u001b[?25hCollecting ssh2-python>=0.22.0\n",
            "  Downloading ssh2_python-0.27.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 21.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deprecation>=2.1->bagua-cuda111) (21.3)\n",
            "Collecting Werkzeug>=2.0\n",
            "  Downloading Werkzeug-2.1.1-py3-none-any.whl (224 kB)\n",
            "\u001b[K     |████████████████████████████████| 224 kB 21.5 MB/s \n",
            "\u001b[?25hCollecting Jinja2>=3.0\n",
            "  Downloading Jinja2-3.1.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 37.8 MB/s \n",
            "\u001b[?25hCollecting click>=8.0\n",
            "  Downloading click-8.1.2-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (4.11.3)\n",
            "Collecting itsdangerous>=2.0\n",
            "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 14.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (57.4.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (1.1.2)\n",
            "Collecting zope.event\n",
            "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6.0->flask>=2.0->bagua-cuda111) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6.0->flask>=2.0->bagua-cuda111) (4.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=3.0->flask>=2.0->bagua-cuda111) (2.0.1)\n",
            "Collecting pytest>=3.8\n",
            "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 51.8 MB/s \n",
            "\u001b[?25hCollecting py-cpuinfo\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (1.11.0)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (21.4.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (1.1.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (1.1.0)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize>=0.8.1->bagua-cuda111) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deprecation>=2.1->bagua-cuda111) (3.0.8)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-62.1.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 40.7 MB/s \n",
            "\u001b[?25hCollecting semantic-version<3,>=2.8.2\n",
            "  Downloading semantic_version-2.9.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: py-cpuinfo\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=b59f8d4c5444737afd0c4262f2367e6a6eb2a307627c13e8f1bf28a7df1be65b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "Successfully built py-cpuinfo\n",
            "Installing collected packages: setuptools, zope.interface, zope.event, pluggy, Werkzeug, ssh2-python, ssh-python, semantic-version, scikit-learn, pytest, pyaml, py-cpuinfo, Jinja2, itsdangerous, gevent, click, xxhash, setuptools-rust, scikit-optimize, requests, pytest-benchmark, pydantic, parallel-ssh, gorilla, flask, deprecation, colorama, bagua-cuda111\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Attempting uninstall: Jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: itsdangerous\n",
            "    Found existing installation: itsdangerous 1.1.0\n",
            "    Uninstalling itsdangerous-1.1.0:\n",
            "      Successfully uninstalled itsdangerous-1.1.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: flask\n",
            "    Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed Jinja2-3.1.1 Werkzeug-2.1.1 bagua-cuda111-0.9.0 click-8.1.2 colorama-0.4.4 deprecation-2.1.0 flask-2.1.1 gevent-21.12.0 gorilla-0.4.0 itsdangerous-2.1.2 parallel-ssh-2.8.0 pluggy-1.0.0 py-cpuinfo-8.0.0 pyaml-21.10.1 pydantic-1.9.0 pytest-7.1.2 pytest-benchmark-3.4.1 requests-2.27.1 scikit-learn-1.0.1 scikit-optimize-0.9.0 semantic-version-2.9.0 setuptools-62.1.0 setuptools-rust-1.3.0 ssh-python-0.10.0 ssh2-python-0.27.0 xxhash-3.0.0 zope.event-4.5.0 zope.interface-5.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install bagua-cuda111"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import environment variables**"
      ],
      "metadata": {
        "id": "IThXKxGEcv4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['RANK'] = '0'\n",
        "os.environ['WORLD_SIZE'] = '1'\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '29500'"
      ],
      "metadata": {
        "id": "ykSOeQsnTLCz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **QSparseLocal Algorithm Implementation**"
      ],
      "metadata": {
        "id": "1M-JsrdWcjzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "from bagua.torch_api.bucket import BaguaBucket\n",
        "from bagua.torch_api.tensor import BaguaTensor\n",
        "from bagua.torch_api.data_parallel.bagua_distributed import BaguaDistributedDataParallel\n",
        "from bagua.torch_api.algorithms import Algorithm, AlgorithmImpl\n",
        "from bagua.torch_api.communication import BaguaProcessGroup\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import torch\n",
        "import math\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "class QSparseLocalOptimizer(Optimizer):\n",
        "    def __init__(\n",
        "            self,\n",
        "            params,\n",
        "            lr: float = 1e-3,  ## Later step dependent learning rate\n",
        "            k: int = 1000,\n",
        "            schedule: List = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create a dedicated optimizer used for\n",
        "        `QSparseLocal <https://tutorials.baguasys.com/algorithms/q-adam>`_ algorithm.\n",
        "\n",
        "        Args:\n",
        "            params (iterable): Iterable of parameters to optimize or dicts defining\n",
        "                parameter groups.\n",
        "            lr: Learning rate.\n",
        "            k: How many tensor components are kept during sparsification\n",
        "            schedule: Description of synchronization schedule\n",
        "        \"\"\"\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "\n",
        "\n",
        "        defaults = dict(lr=lr, k=k, schedule=schedule)\n",
        "        super(QSparseLocalOptimizer, self).__init__(params, defaults)\n",
        "        # TODO: QSparseLocal optimizer maintain `step_id` in its state\n",
        "        self.step_id = 0\n",
        "        self.schedule = schedule\n",
        "        self.k = k\n",
        "\n",
        "        # initialize global and local model, and memory for error compensation\n",
        "        for group_id, group in enumerate(self.param_groups):\n",
        "            params_with_grad = []\n",
        "            for p in group[\"params\"]:\n",
        "                params_with_grad.append(p)\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state[\"local\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    state[\"global\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    state[\"memory\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    state[\"qsl_grad\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    '''\n",
        "                    local: Local parameter vector\n",
        "                    global: Global paramenter vector (same for all workers)\n",
        "                    memory: Term for error compensation for quantization and sparsification of gradient tensor\n",
        "                    qsl_grad: The quantized and sparsified gradient tensor to be sent to master (to allreduce)\n",
        "                    '''\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(QSparseLocalOptimizer, self).__setstate__(state)\n",
        "\n",
        "    ## input: Uncompressed Gradient tensor\n",
        "    ## Output: Quantized and sparsified Gradient tensor\n",
        "    def qsp(self, input):\n",
        "\n",
        "        ###To do: Allow other quantization\n",
        "        def signq(self, var):\n",
        "            return torch.sign(var)  # Returns a new tensor with the signs of the elements of input\n",
        "\n",
        "        ### To do: Allow rand_k sparsification\n",
        "        org_shape = input.size()\n",
        "        numel = torch.numel(input)\n",
        "        K = min(numel, self.k)  # k is the optimizer's k,\n",
        "                                # K is the actual value used for sparsification\n",
        "\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = torch.reshape(input, [-1])\n",
        "\n",
        "        #print(\"Flat input: \",flat_input)\n",
        "        #print(\"Input size\",flat_input.size())\n",
        "\n",
        "        # Get values and index tensor of chosen components (Set dim=-1 for testing)\n",
        "        values, indices = torch.topk(flat_input, K,dim=-1)  #flat_input instead of input\n",
        "\n",
        "        # Added self to signq input\n",
        "\n",
        "        # torch.zeros() on cpu not cuda, use torch.zeros_like() with memory_format=torch.preserve_format as default\n",
        "        # flat_quantized = torch.zeros(flat_input.size()).scatter(0, indices, signq(self,values))\n",
        "\n",
        "\n",
        "        flat_quantized=torch.zeros_like(flat_input).scatter(0, indices, signq(self,values))\n",
        "        quantized = torch.reshape(flat_quantized, shape=org_shape)\n",
        "\n",
        "        return quantized\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        self.step_id += 1\n",
        "        for group_id, group in enumerate(self.param_groups):\n",
        "\n",
        "            lr = group[\"lr\"]\n",
        "\n",
        "            for param_id, param in enumerate(group[\"params\"]):\n",
        "                state = self.state[param]\n",
        "\n",
        "                state[\"local\"].add_(param.grad, alpha=-lr)\n",
        "\n",
        "                #  If synchronization in this round\n",
        "                if self.schedule is None or self.step_id in self.schedule:\n",
        "                    # Calculate quantized gradient\n",
        "                    state[\"qsl_grad\"] = self.qsp(state[\"memory\"] + state[\"global\"] - state[\"local\"])\n",
        "\n",
        "                    # Update memory\n",
        "                    state[\"memory\"].add_(state[\"global\"]).add_(-state[\"local\"]).add_(-state[\"qsl_grad\"])\n",
        "\n",
        "                # Local weights equal to local model\n",
        "                # Step 5\n",
        "                param.data=state[\"local\"]\n",
        "\n",
        "\n",
        "class QSparseLocalAlgorithmImpl(AlgorithmImpl):\n",
        "    def __init__(\n",
        "            self,\n",
        "            process_group: BaguaProcessGroup,\n",
        "            q_sparse_local_optimizer: QSparseLocalOptimizer,\n",
        "            hierarchical: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implementation of the\n",
        "        `QSparseLocal Algorithm <https://tutorials.baguasys.com/algorithms/q-adam>`_\n",
        "        .\n",
        "\n",
        "        Args:\n",
        "            process_group: The process group to work on.\n",
        "            q_sparse_local_optimizer: A QSparseLocalOptimizer initialized with model parameters.\n",
        "            hierarchical: Enable hierarchical communication.\n",
        "        \"\"\"\n",
        "        super(QSparseLocalAlgorithmImpl, self).__init__(process_group)\n",
        "        self.hierarchical = hierarchical\n",
        "        self.optimizer = q_sparse_local_optimizer\n",
        "        self.schedule = self.optimizer.schedule\n",
        "        \n",
        "\n",
        "    #def need_reset(self):\n",
        "    #    return True\n",
        "\n",
        "    def init_tensors(self, bagua_distributed_data_parallel: BaguaDistributedDataParallel):\n",
        "        parameters =  bagua_distributed_data_parallel.bagua_build_params()\n",
        "\n",
        "        for idx, (name, param) in enumerate(parameters.__reversed__()):\n",
        "            param._q_sparse_local_name = name\n",
        "            param._q_sparse_local_idx = idx\n",
        "\n",
        "        tensor_groups = []\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for param in group[\"params\"]:\n",
        "                if self.schedule is None or self.optimizer.step_id in self.schedule:\n",
        "\n",
        "                    # Second half Step 4\n",
        "                    # register local and global parameter vector\n",
        "\n",
        "                    def set_weights(param, t):\n",
        "                        # Gradient is subtracted from global and local model\n",
        "\n",
        "                        self.optimizer.state[param][\"qsl_grad\"] = t\n",
        "\n",
        "                    registered_tensor = param.bagua_ensure_grad().ensure_bagua_tensor(\n",
        "                        param._q_sparse_local_name,\n",
        "                        bagua_distributed_data_parallel.bagua_module_name,\n",
        "                        getter_closure=lambda param: self.optimizer.state[param][\"qsl_grad\"],\n",
        "                        setter_closure=set_weights,\n",
        "                    )\n",
        "                    tensor_groups.append(registered_tensor)\n",
        "                else:\n",
        "                    # Nothing happens\n",
        "                    pass\n",
        "\n",
        "        tensor_groups.sort(key=lambda x: x._q_sparse_local_idx)\n",
        "        return tensor_groups\n",
        "\n",
        "    def tensors_to_buckets(\n",
        "            self, tensors: List[List[BaguaTensor]], do_flatten: bool\n",
        "    ) -> List[BaguaBucket]:\n",
        "        bagua_buckets = []\n",
        "        #print(\"Tensor qsparselocal:\\n\\n\",tensors)\n",
        "        #print(\"\\n\\nTensor shape: \",tensors.shape())\n",
        "        #print(\"Attributes:\\n\\n\", dir(tensors[0][0]))\n",
        "        #print(\"\\n\\nLength\\n\\n\", len(tensors[0][0]))\n",
        "        #print(\"\\n\\nLength\\n\\n\", len(tensors[0][1]))\n",
        "        #print(\"\\n\\nLength\\n\\n\", len(tensors[0][2]))\n",
        "        #print(\"\\n\\nLength\\n\\n\", len(tensors[0][3]))\n",
        "        #print(\"\\n\\nDatapointer: \", tensors[0][0].data_ptr)\n",
        "        #print(\"\\n\\nDatapointer: \", tensors[0][1].data_ptr)\n",
        "        #print(\"\\n\\nDatapointer: \", tensors[0][2].data_ptr)\n",
        "        #print(\"\\n\\nDatapointer: \", tensors[0][3].data_ptr)\n",
        "        for idx, bucket in enumerate(tensors):\n",
        "            bagua_bucket = BaguaBucket(\n",
        "                bucket,\n",
        "                flatten=do_flatten,\n",
        "                #flatten=True,\n",
        "                name=str(idx),\n",
        "                alignment=self.process_group.get_global_communicator().nranks(),\n",
        "            )\n",
        "            bagua_buckets.append(bagua_bucket)\n",
        "        return bagua_buckets\n",
        "\n",
        "    def init_operations(\n",
        "            self,\n",
        "             bagua_distributed_data_parallel: BaguaDistributedDataParallel,\n",
        "            bucket: BaguaBucket,\n",
        "    ):\n",
        "        bucket.clear_ops()\n",
        "        if self.schedule is None or self.optimizer.step_id in self.schedule:\n",
        "            ##### Step 2\n",
        "            def set_weights(*args):\n",
        "                for tensor in bucket.tensors:\n",
        "                    self.optimizer.state[\"global\"].add_(-tensor.bagua_getter_closure())\n",
        "                    self.optimizer.state[\"local\"] = self.optimizer.state[\"global\"]\n",
        "            \n",
        "            bucket.append_python_op(set_weights, group=self.process_group)\n",
        "\n",
        "            ##### Step 3\n",
        "            bucket.append_centralized_synchronous_op(\n",
        "                hierarchical=self.hierarchical,\n",
        "                average=True,\n",
        "                scattergather=True,\n",
        "                compression=\"MinMaxUInt8\",\n",
        "                group=self.process_group,\n",
        "            )\n",
        "        else: # Nothing happens\n",
        "            pass\n",
        "\n",
        "\n",
        "    # Instead of momentum hook, we use a qsl_gradient hook\n",
        "    def init_backward_hook(self,  bagua_distributed_data_parallel: BaguaDistributedDataParallel):\n",
        "\n",
        "        def hook_qsl_grad(parameter_name, parameter):\n",
        "            assert (\n",
        "                    parameter.bagua_backend_tensor().data_ptr()\n",
        "                    == self.optimizer.state[parameter][\"qsl_grad\"].data_ptr()\n",
        "            ), \"bagua backend tensor data_ptr should match _q_sparse_local_grad data_ptr\"\n",
        "            parameter.bagua_mark_communication_ready()\n",
        "\n",
        "        return hook_qsl_grad\n",
        "\n",
        "\n",
        "class QSparseLocalAlgorithm(Algorithm):\n",
        "    def __init__(self, q_sparse_local_optimizer: QSparseLocalOptimizer, hierarchical: bool = True):\n",
        "        \"\"\"\n",
        "        Create an instance of the\n",
        "        `QSparseLocal Algorithm <https://tutorials.baguasys.com/algorithms/q-adam>`_\n",
        "        .\n",
        "\n",
        "        Args:\n",
        "            q_sparse_local_optimizer: A QSparseLocalOptimizer initialized with model parameters.\n",
        "            hierarchical: Enable hierarchical communication.\n",
        "        \"\"\"\n",
        "        self.hierarchical = hierarchical\n",
        "        self.optimizer = q_sparse_local_optimizer\n",
        "\n",
        "    def reify(self, process_group: BaguaProcessGroup) -> QSparseLocalAlgorithmImpl:\n",
        "        return QSparseLocalAlgorithmImpl(\n",
        "            process_group,\n",
        "            q_sparse_local_optimizer=self.optimizer,\n",
        "            hierarchical=self.hierarchical,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "_TpSXZaAd8r8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "14cca38c-6bf0-4341-c78b-2699d506b983"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Bagua cannot detect bundled NCCL library, Bagua will try to use system NCCL instead. If you encounter any error, please run `import bagua_core; bagua_core.install_deps()` or the `bagua_install_deps.py` script to install bundled libraries.\n",
            "/usr/local/lib/python3.7/dist-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
            "  \"Distutils was imported before Setuptools, but importing Setuptools \"\n",
            "/usr/local/lib/python3.7/dist-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
            "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b4d2ee19b21a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!/usr/bin/env python3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbagua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaguaBucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbagua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaguaTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbagua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbagua_distributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaguaDistributedDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbagua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAlgorithmImpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bagua/torch_api/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mReduceOp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m )\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaguaModule\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaguaTensor\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m from .env import (  # noqa: F401\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bagua/torch_api/distributed.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbagua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbagua_distributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaguaDistributedDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bagua/torch_api/data_parallel/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedDataParallel\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bagua/torch_api/data_parallel/distributed.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbagua\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbagua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_allreduce\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradientAllReduceAlgorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m from bagua.torch_api.communication import (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bagua/torch_api/algorithms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!/usr/bin/env python3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAlgorithmImpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalAlgorithmRegistry\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbytegrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecentralized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_allreduce\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mq_adam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync_model_average\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bagua/torch_api/algorithms/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbagua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbagua_distributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaguaDistributedDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbagua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaguaBucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbagua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaguaTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbagua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunication\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaguaProcessGroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bagua/torch_api/bucket.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbagua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaguaTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbagua\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_contiguous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_flattened_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m from bagua.torch_api.communication import (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bagua/torch_api/tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msetuptools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mLooseVersion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLooseVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'distutils' has no attribute 'version'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MNIST example**"
      ],
      "metadata": {
        "id": "mPIWQ8CvcZ0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_list=[]\n",
        "loss_list=[]\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import logging\n",
        "import bagua.torch_api as bagua\n",
        "import bagua_core \n",
        "\n",
        "bagua_core.install_deps()\n",
        "\n",
        "\n",
        "# Model for Neural Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "#------Training Model\n",
        "def train(args, model, train_loader, optimizer, epoch):\n",
        "    #??????? What does model.train() do exactly?\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        #data: features , target: label\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        #??????? Optimizer calculates weights where gradient == 0\n",
        "        optimizer.zero_grad()\n",
        "        # Calculates predicted labels by using the model\n",
        "        output = model(data)\n",
        "        # Loss function using predicted labels and actual labels\n",
        "        loss = F.nll_loss(output, target)\n",
        "        # Backwards propagation    !!!calculates tensor loss gradient \n",
        "        loss.backward()\n",
        "        # Optimizer step selection\n",
        "        if args.fuse_optimizer:\n",
        "            optimizer.fuse_step()\n",
        "        else:\n",
        "            optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            logging.info(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(\n",
        "                output, target, reduction=\"sum\"\n",
        "            ).item()  # sum up batch loss\n",
        "            pred = output.argmax(\n",
        "                dim=1, keepdim=True\n",
        "            )  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    logging.info(\n",
        "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss,\n",
        "            correct,\n",
        "            len(test_loader.dataset),\n",
        "            100.0 * correct / len(test_loader.dataset),\n",
        "        )\n",
        "    )\n",
        "    return test_loss,correct\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description=\"PyTorch MNIST Example\")\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=64,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size for training (default: 64)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--test-batch-size\",\n",
        "        type=int,\n",
        "        default=1000,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size for testing (default: 1000)\",\n",
        "    )\n",
        "\n",
        "# set number of epochs here\n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        type=int,\n",
        "        default=14,\n",
        "        metavar=\"N\",\n",
        "        help=\"number of epochs to train (default: 14)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lr\",\n",
        "        type=float,\n",
        "        default=1.0,\n",
        "        metavar=\"LR\",\n",
        "        help=\"learning rate (default: 1.0)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gamma\",\n",
        "        type=float,\n",
        "        default=0.7,\n",
        "        metavar=\"M\",\n",
        "        help=\"Learning rate step gamma (default: 0.7)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--log-interval\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"N\",\n",
        "        help=\"how many batches to wait before logging training status\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--save-model\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"For Saving the current Model\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--algorithm\",\n",
        "        type=str,\n",
        "        default=\"qsparselocal\",\n",
        "        help=\"gradient_allreduce, bytegrad, decentralized, low_precision_decentralized, qadam, async\",\n",
        "        #Add new algorithm for testing------------------\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--async-sync-interval\",\n",
        "        default=500,\n",
        "        type=int,\n",
        "        help=\"Model synchronization interval(ms) for async algorithm\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--set-deterministic\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"set deterministic or not\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fuse-optimizer\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"fuse optimizer or not\",\n",
        "    )\n",
        "\n",
        "    #args = parser.parse_args() \n",
        "    # New line below solves ipykernel_launcher.py: error: unrecognized arguments\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    \n",
        "    if args.set_deterministic:\n",
        "        print(\"set_deterministic: True\")\n",
        "        np.random.seed(666)\n",
        "        random.seed(666)\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.manual_seed(666)\n",
        "        torch.cuda.manual_seed_all(666 + int(bagua.get_rank()))\n",
        "        torch.set_printoptions(precision=10)\n",
        "\n",
        "    torch.cuda.set_device(bagua.get_local_rank())\n",
        "    bagua.init_process_group()\n",
        "\n",
        "\n",
        "\n",
        "    logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.ERROR)\n",
        "    if bagua.get_rank() == 0:\n",
        "        logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    train_kwargs = {\"batch_size\": args.batch_size}\n",
        "    test_kwargs = {\"batch_size\": args.test_batch_size}\n",
        "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        "    )\n",
        "\n",
        "    if bagua.get_local_rank() == 0:\n",
        "        dataset1 = datasets.MNIST(\n",
        "            \"../data\", train=True, download=True, transform=transform\n",
        "        )\n",
        "        torch.distributed.barrier()\n",
        "    else:\n",
        "        torch.distributed.barrier()\n",
        "        dataset1 = datasets.MNIST(\n",
        "            \"../data\", train=True, download=True, transform=transform\n",
        "        )\n",
        "\n",
        "    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        dataset1, num_replicas=bagua.get_world_size(), rank=bagua.get_rank()\n",
        "    )\n",
        "    train_kwargs.update(\n",
        "        {\n",
        "            \"sampler\": train_sampler,\n",
        "            \"batch_size\": args.batch_size // bagua.get_world_size(),\n",
        "            \"shuffle\": False,\n",
        "        }\n",
        "    )\n",
        "    # Train and Test dataset\n",
        "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "    # ??????????\n",
        "    model = Net().cuda()\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "    if args.algorithm == \"gradient_allreduce\":\n",
        "        from bagua.torch_api.algorithms import gradient_allreduce\n",
        "\n",
        "        algorithm = gradient_allreduce.GradientAllReduceAlgorithm()\n",
        "    elif args.algorithm == \"decentralized\":\n",
        "        from bagua.torch_api.algorithms import decentralized\n",
        "\n",
        "        algorithm = decentralized.DecentralizedAlgorithm()\n",
        "    elif args.algorithm == \"low_precision_decentralized\":\n",
        "        from bagua.torch_api.algorithms import decentralized\n",
        "\n",
        "        algorithm = decentralized.LowPrecisionDecentralizedAlgorithm()\n",
        "    elif args.algorithm == \"bytegrad\":\n",
        "        from bagua.torch_api.algorithms import bytegrad\n",
        "\n",
        "        algorithm = bytegrad.ByteGradAlgorithm()\n",
        "    elif args.algorithm == \"qadam\":\n",
        "        from bagua.torch_api.algorithms import q_adam\n",
        "\n",
        "        optimizer = q_adam.QAdamOptimizer(\n",
        "            model.parameters(), lr=args.lr, warmup_steps=100\n",
        "        )\n",
        "        algorithm = q_adam.QAdamAlgorithm(optimizer)\n",
        "    elif args.algorithm == \"qsparselocal\":\n",
        "      # Set lower learning rate, no convergence for lr = 1\n",
        "        optimizer = QSparseLocalOptimizer(\n",
        "            model.parameters(), lr=0.0001\n",
        "        )\n",
        "        algorithm = QSparseLocalAlgorithm(optimizer)\n",
        "    elif args.algorithm == \"async\":\n",
        "        from bagua.torch_api.algorithms import async_model_average\n",
        "\n",
        "        algorithm = async_model_average.AsyncModelAverageAlgorithm(\n",
        "            sync_interval_ms=args.async_sync_interval,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    #  Model von Bagua\n",
        "    model = model.with_bagua(\n",
        "        [optimizer],\n",
        "        algorithm,\n",
        "        do_flatten=not args.fuse_optimizer,\n",
        "    )\n",
        "\n",
        "    # Optimizer from Bagua if args.fuse_optimizer==True\n",
        "    if args.fuse_optimizer:\n",
        "        optimizer = bagua.contrib.fuse_optimizer(optimizer)\n",
        "\n",
        "    #------------ Loss, accuracy\n",
        "    loss_list =[]\n",
        "    acc_list = []\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        if args.algorithm == \"async\":\n",
        "            model.bagua_algorithm.resume(model)\n",
        "\n",
        "        train(args, model, train_loader, optimizer, epoch)\n",
        "\n",
        "        if args.algorithm == \"async\":\n",
        "            model.bagua_algorithm.abort(model)\n",
        "\n",
        "        new_loss,new_acc =test(model, test_loader)\n",
        "        loss_list.append(new_loss)\n",
        "        acc_list.append(new_acc/100.0)\n",
        "        scheduler.step()\n",
        "\n",
        "    if args.save_model:\n",
        "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    ep =[]\n",
        "\n",
        "\n",
        "    for i in range(1, args.epochs + 1):\n",
        "      ep.append(i)\n",
        "\n",
        "    print(loss_list)\n",
        "    print(acc_list)\n",
        "    \n",
        "     \n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.subplot(211)\n",
        "    plt.plot(ep,loss_list)\n",
        "    plt.subplot(212)\n",
        "    plt.plot(ep,acc_list)\n",
        "\n",
        "    plt.show()  \n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "9tjsC05-5qV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tb"
      ],
      "metadata": {
        "id": "8rSo-g6qDIvE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}