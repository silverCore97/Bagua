{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverCore97/Bagua/blob/main/QSparseLocal_MNIST_4_5_slow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FidgI73Gc8m7"
      },
      "source": [
        "##**Check CUDA version**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biBKGSXi6Chp",
        "outputId": "249d90a7-066d-4af7-a3bf-30d32a52babb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1UzpVrNc3wO"
      },
      "source": [
        "##**Install Bagua**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qCGqkkr5mr4",
        "outputId": "7c47243f-fa89-4199-8076-458580d24b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bagua-cuda111 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: gorilla==0.4.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.4.0)\n",
            "Requirement already satisfied: gevent>=21.8 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (21.12.0)\n",
            "Requirement already satisfied: setuptools-rust in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.3.0)\n",
            "Requirement already satisfied: parallel-ssh==2.8.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.8.0)\n",
            "Requirement already satisfied: scikit-optimize>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.9.0)\n",
            "Requirement already satisfied: pydantic>=1.8 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.9.0)\n",
            "Requirement already satisfied: pytest-benchmark>=3.4 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (3.4.1)\n",
            "Requirement already satisfied: deprecation>=2.1 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.1.0)\n",
            "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.27.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.4.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (4.64.0)\n",
            "Requirement already satisfied: xxhash>=2.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn!=1.0,<=1.0.1,>=0.24 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.0.1)\n",
            "Requirement already satisfied: prometheus-client>=0.11 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.14.1)\n",
            "Requirement already satisfied: flask>=2.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.1.2)\n",
            "Requirement already satisfied: ssh2-python>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from parallel-ssh==2.8.0->bagua-cuda111) (0.27.0)\n",
            "Requirement already satisfied: ssh-python>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from parallel-ssh==2.8.0->bagua-cuda111) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deprecation>=2.1->bagua-cuda111) (21.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (3.1.2)\n",
            "Requirement already satisfied: Werkzeug>=2.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (2.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (8.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (4.11.3)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (4.5.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (1.1.2)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (5.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (62.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6.0->flask>=2.0->bagua-cuda111) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6.0->flask>=2.0->bagua-cuda111) (4.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=3.0->flask>=2.0->bagua-cuda111) (2.0.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.7/dist-packages (from pytest-benchmark>=3.4->bagua-cuda111) (8.0.0)\n",
            "Requirement already satisfied: pytest>=3.8 in /usr/local/lib/python3.7/dist-packages (from pytest-benchmark>=3.4->bagua-cuda111) (7.1.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (2.0.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (21.4.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (1.1.1)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (1.11.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (1.1.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize>=0.8.1->bagua-cuda111) (21.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize>=0.8.1->bagua-cuda111) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deprecation>=2.1->bagua-cuda111) (3.0.8)\n",
            "Requirement already satisfied: semantic-version<3,>=2.8.2 in /usr/local/lib/python3.7/dist-packages (from setuptools-rust->bagua-cuda111) (2.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install bagua-cuda111"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IThXKxGEcv4z"
      },
      "source": [
        "## **Import environment variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ykSOeQsnTLCz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['RANK'] = '0'\n",
        "os.environ['WORLD_SIZE'] = '1'\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '29500'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M-JsrdWcjzj"
      },
      "source": [
        "## **QSparseLocal Algorithm Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_TpSXZaAd8r8"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from bagua.torch_api.bucket import BaguaBucket\n",
        "from bagua.torch_api.tensor import BaguaTensor\n",
        "from bagua.torch_api.data_parallel.bagua_distributed import BaguaDistributedDataParallel\n",
        "from bagua.torch_api.algorithms import Algorithm, AlgorithmImpl\n",
        "from bagua.torch_api.communication import BaguaProcessGroup\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "sparsify = True\n",
        "use_memory = True\n",
        "quantization_scheme = 'sign'\n",
        "quantization_levels = 256\n",
        "top_k_sparsification = True\n",
        "k = 1000\n",
        "use_normalization = True\n",
        "\n",
        "\n",
        "## input: Uncompressed Gradient tensor\n",
        "## Output: Quantized and sparsified Gradient tensor\n",
        "def qsl(eta_grad,\n",
        "        memory,\n",
        "        topK_flag,\n",
        "        s,\n",
        "        # sparsify,\n",
        "        # use_memory,\n",
        "        # quantization_scheme,\n",
        "        # use_normalization\n",
        "        ):\n",
        "    ###To do: Allow other quantization\n",
        "    def signq(var):\n",
        "        # Normalization according to input\n",
        "        # ||var||_1 * sign(var)/\n",
        "        one_norm = torch.norm(var, p=1)\n",
        "        return one_norm * torch.sign(var + 1e-13) / float(torch.numel(var))\n",
        "        # return torch.sign(var)  # Returns a new tensor with the signs of the elements of input\n",
        "\n",
        "    def qsgd(var):\n",
        "        level_float = s * torch.abs(var) / norm1\n",
        "        previous_level = torch.floor(level_float)\n",
        "        is_next_level = (torch.rand(var.size(), dtype=torch.float32, device = 'cuda') < (level_float - previous_level))\n",
        "        is_next_level = is_next_level.float()\n",
        "        new_level = previous_level + is_next_level\n",
        "        unnormalized = torch.sign(var) * new_level * norm1 / s\n",
        "        beta = float(torch.numel(var)) / float(s * s)\n",
        "        return unnormalized / (1.0 + beta) if use_normalization else unnormalized\n",
        "\n",
        "    def get_quantization(q):\n",
        "        if q == 'qsgd':\n",
        "            return qsgd\n",
        "        elif q == 'sign':\n",
        "            return signq\n",
        "        else:\n",
        "            return lambda x: x\n",
        "\n",
        "    if not sparsify:\n",
        "        norm1 = torch.norm(eta_grad) + torch.constant(1e-5, dtype=torch.float32)\n",
        "        if use_memory:\n",
        "            input = memory + eta_grad\n",
        "        else:\n",
        "            input = eta_grad\n",
        "\n",
        "        func = get_quantization(quantization_scheme)\n",
        "        q = func(input)\n",
        "\n",
        "        return q, input - q\n",
        "\n",
        "    input = memory + eta_grad\n",
        "\n",
        "    org_shape = input.size()\n",
        "    numel = torch.numel(input)\n",
        "    K = min(numel, k)  # k is the optimizer's k,\n",
        "    # K is the actual value used for sparsification\n",
        "\n",
        "    if topK_flag:\n",
        "        # Get values and index tensor of chosen components\n",
        "        # flat shape with absolute values\n",
        "        _, indices = torch.topk(torch.reshape(torch.abs(input), [-1]), K)\n",
        "    else:\n",
        "        indices = torch.from_numpy(np.random.choice(torch.range(numel), K, False))\n",
        "\n",
        "    # Flatten input\n",
        "    flat_input = torch.reshape(input, [-1])\n",
        "    values = torch.gather(flat_input, 0, indices)  # dim=0\n",
        "    norm1 = torch.norm(values)\n",
        "    quantization_func = get_quantization(quantization_scheme)\n",
        "    flattened_quantized = torch.zeros_like(flat_input).scatter(0, indices,\n",
        "                                                               quantization_func(values))\n",
        "    quantization = torch.reshape(flattened_quantized, shape=org_shape)\n",
        "\n",
        "    q_func = lambda: quantization\n",
        "    zero_tensor = lambda: torch.zeros_like(input, dtype=torch.float32)\n",
        "\n",
        "    # q = torch.where( float(0)<norm1, q_func, zero_tensor)    # Where not applicable for choosing functions\n",
        "    if float(0) < norm1:\n",
        "        q = q_func()\n",
        "    else:\n",
        "        q = zero_tensor()\n",
        "\n",
        "    err = input - q\n",
        "\n",
        "    #print(\"\\n\\n input in qsl\",\"of shape\",input.size(),\"\\n\", torch.reshape(input, [-1])[:3])\n",
        "    #print(\"q:\",torch.reshape(q, [-1])[:3])\n",
        "    #print(\"err:\", torch.reshape(err, [-1])[:3])\n",
        "\n",
        "    return q, err\n",
        "\n",
        "\n",
        "class QSparseLocalOptimizer(Optimizer):\n",
        "    def __init__(\n",
        "            self,\n",
        "            params,\n",
        "            lr: float = 1e-3,  ## Later step dependent learning rate\n",
        "            k: int = 1000,\n",
        "            schedule: int = 1\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create a dedicated optimizer used for\n",
        "        `QSparseLocal <https://tutorials.baguasys.com/algorithms/q-adam>`_ algorithm.\n",
        "\n",
        "        Args:\n",
        "            params (iterable): Iterable of parameters to optimize or dicts defining\n",
        "                parameter groups.\n",
        "            lr: Learning rate.\n",
        "            k: How many tensor components are kept during sparsification\n",
        "            schedule: Description of synchronization schedule\n",
        "        \"\"\"\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0 < schedule:\n",
        "            raise ValueError(\"Invalid schedule: {}\".format(lr))\n",
        "\n",
        "        defaults = dict(lr=lr, k=k, schedule=schedule)\n",
        "        super(QSparseLocalOptimizer, self).__init__(params, defaults)\n",
        "        # TODO: QSparseLocal optimizer maintain `step_id` in its state\n",
        "        self.step_id = 0\n",
        "        self.schedule = schedule\n",
        "        self.k = k\n",
        "        self.lr = lr\n",
        "        self.sync = False\n",
        "\n",
        "        # initialize global and local model, and memory for error compensation\n",
        "        for group_id, group in enumerate(self.param_groups):\n",
        "            params_with_grad = []\n",
        "            for p in group[\"params\"]:\n",
        "                params_with_grad.append(p)\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    ##### Instead of state[\"local\"] use param.data as local model\n",
        "                    # state[\"local\"] = torch.zeros_like(\n",
        "                    #     p, memory_format=torch.preserve_format\n",
        "                    # )\n",
        "\n",
        "                    state[\"global\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    # Set global to initialized value of local weights\n",
        "                    state[\"global\"].add_(p.data)\n",
        "\n",
        "                    # print(\"global init\",state[\"global\"].size())\n",
        "                    state[\"memory\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    # print(\"comp init\", state[\"error_comp\"].size())\n",
        "                    state[\"qsl_grad\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    #print(\"Hello\")\n",
        "                    \"\"\"\n",
        "                    ###local: Local parameter vector\n",
        "                    global: Global paramenter vector (same for all workers)\n",
        "                    error_comp: Term for error compensation for quantization and sparsification of gradient tensor\n",
        "                    qsl_grad: The quantized and sparsified gradient tensor to be sent to master (to allreduce)\n",
        "                    \"\"\"\n",
        "                    \"\"\"  ###Leads to problem with gradient\n",
        "                    # Set local weights to 0 in the beginning, format: torch.Size([32, 1, 3, 3])\n",
        "                    \n",
        "                    # p.data uses normal pytorch initializer by default\n",
        "                    p.data = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    \"\"\"\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(QSparseLocalOptimizer, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        self.step_id += 1\n",
        "\n",
        "        # Now schedule defines the number of round per synchronization round\n",
        "        self.sync = self.step_id % self.schedule == 0\n",
        "\n",
        "        ############################## LR adjustment\n",
        "        #Quarter learning every 100 steps\n",
        "        #if self.step_id%300 == 0:\n",
        "        #  self.lr /= 2\n",
        "        ###############################\n",
        "\n",
        "        for group_id, group in enumerate(self.param_groups):\n",
        "\n",
        "            for param_id, param in enumerate(group[\"params\"]):\n",
        "                state = self.state[param]\n",
        "\n",
        "                #print(\"Data: \",torch.reshape(param.data,[-1])[:10])\n",
        "\n",
        "                #Calculate new global and local weights\n",
        "                #if self.sync:\n",
        "                    #new_var = last_var + hvd.allreduce(var-last_var, var, opt, wipe_memory)\n",
        "                    # Nothing happens for the first step\n",
        "\n",
        "                state[\"global\"].add_(state[\"qsl_grad\"])\n",
        "                param.data.add_(-param.data).add_(state[\"global\"])\n",
        "\n",
        "\n",
        "                #LARC\n",
        "                #eta=1/math.sqrt(torch.numel(param.grad))        \n",
        "                #param.data.add_(param.grad, alpha=-min(self.lr,eta*torch.norm(param.data, p=1)/torch.norm(param.grad, p=1)))\n",
        "                \n",
        "                param.data.add_(param.grad,alpha=-self.lr) \n",
        "\n",
        "\n",
        "\n",
        "                ##### In allreduce before allreduce operation\n",
        "                # Compute compressed gradient and new error compensation term\n",
        "                # Horovod: eta local-global\n",
        "                #if self.sync:\n",
        "                #state[\"qsl_grad\"], state[\"memory\"] = qsl(param.data - state[\"global\"], state[\"memory\"],\n",
        "                #                                         topK_flag=top_k_sparsification, s=quantization_levels)\n",
        "\n",
        "                qsl_grad, memory = qsl(param.data - state[\"global\"], state[\"memory\"],\n",
        "                                             topK_flag=top_k_sparsification, s=quantization_levels)\n",
        "                state[\"qsl_grad\"].copy_(qsl_grad)\n",
        "                state[\"memory\"].copy_(memory)\n",
        "\n",
        "\n",
        "\n",
        "class QSparseLocalAlgorithmImpl(AlgorithmImpl):\n",
        "    def __init__(\n",
        "            self,\n",
        "            process_group: BaguaProcessGroup,\n",
        "            q_sparse_local_optimizer: QSparseLocalOptimizer,\n",
        "            hierarchical: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implementation of the\n",
        "        `QSparseLocal Algorithm <https://tutorials.baguasys.com/algorithms/q-adam>`_\n",
        "        .\n",
        "        Args:\n",
        "            process_group: The process group to work on.\n",
        "            q_sparse_local_optimizer: A QSparseLocalOptimizer initialized with model parameters.\n",
        "            hierarchical: Enable hierarchical communication.\n",
        "        \"\"\"\n",
        "        super(QSparseLocalAlgorithmImpl, self).__init__(process_group)\n",
        "        self.hierarchical = hierarchical\n",
        "        self.optimizer = q_sparse_local_optimizer\n",
        "        self.sync = self.optimizer.sync\n",
        "\n",
        "    # Needed to switch between synchronization aand local rounds\n",
        "    def need_reset(self):\n",
        "        return True\n",
        "\n",
        "    def init_tensors(self, bagua_distributed_data_parallel: BaguaDistributedDataParallel):\n",
        "        parameters = bagua_distributed_data_parallel.bagua_build_params()\n",
        "\n",
        "        for idx, (name, param) in enumerate(parameters.__reversed__()):\n",
        "            param._q_sparse_local_name = name\n",
        "            param._q_sparse_local_idx = idx\n",
        "\n",
        "        tensor_groups = []\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for param in group[\"params\"]:\n",
        "                #if self.sync:\n",
        "\n",
        "                # Second half Step 4\n",
        "                def set_weights(param, t):\n",
        "                    # Set compressed gradient to mean of all workers compressed gradients\n",
        "                    self.optimizer.state[param][\"qsl_grad\"] = t\n",
        "\n",
        "                registered_tensor = param.bagua_ensure_grad().ensure_bagua_tensor(\n",
        "                    param._q_sparse_local_name,\n",
        "                    bagua_distributed_data_parallel.bagua_module_name,\n",
        "                    getter_closure=lambda param: self.optimizer.state[param][\"qsl_grad\"],\n",
        "                    setter_closure=set_weights,\n",
        "                )\n",
        "                tensor_groups.append(registered_tensor)\n",
        "                #else:\n",
        "                    # Nothing happens\n",
        "                    #pass\n",
        "\n",
        "        tensor_groups.sort(key=lambda x: x._q_sparse_local_idx)\n",
        "        return tensor_groups\n",
        "\n",
        "    def tensors_to_buckets(\n",
        "            self, tensors: List[List[BaguaTensor]], do_flatten: bool\n",
        "    ) -> List[BaguaBucket]:\n",
        "        bagua_buckets = []\n",
        "\n",
        "        for idx, bucket in enumerate(tensors):\n",
        "            bagua_bucket = BaguaBucket(\n",
        "                bucket,\n",
        "                flatten=do_flatten,\n",
        "                # flatten=True,\n",
        "                name=str(idx),\n",
        "                alignment=self.process_group.get_global_communicator().nranks(),\n",
        "            )\n",
        "            bagua_buckets.append(bagua_bucket)\n",
        "        return bagua_buckets\n",
        "\n",
        "    def init_operations(\n",
        "            self,\n",
        "            bagua_distributed_data_parallel: BaguaDistributedDataParallel,\n",
        "            bucket: BaguaBucket,\n",
        "    ):\n",
        "        bucket.clear_ops()\n",
        "        \n",
        "        # For synchronization round we utilize allreduce\n",
        "        if True:  #self.sync:\n",
        "            # Compression is done by Bagua\n",
        "            bucket.append_centralized_synchronous_op(\n",
        "                hierarchical=self.hierarchical,\n",
        "                average=True,  # Maybe try false and then average it manually to preserve ints\n",
        "                scattergather=True,\n",
        "                compression=\"MinMaxUInt8\",      #TO DO:  Make qsl_grad suitable for compression\n",
        "                group=self.process_group,\n",
        "            )\n",
        "        else:  # Nothing happens\n",
        "            pass\n",
        "\n",
        "    # Instead of momentum hook, we use a qsl_gradient hook\n",
        "    def init_backward_hook(self, bagua_distributed_data_parallel: BaguaDistributedDataParallel):\n",
        "\n",
        "        def hook_qsl_grad(parameter_name, parameter):\n",
        "            assert (\n",
        "                    parameter.bagua_backend_tensor().data_ptr()\n",
        "                    == self.optimizer.state[parameter][\"qsl_grad\"].data_ptr()\n",
        "            ), \"bagua backend tensor data_ptr should match _q_sparse_local_grad data_ptr\"\n",
        "            parameter.bagua_mark_communication_ready()\n",
        "\n",
        "        return hook_qsl_grad\n",
        "\n",
        "\n",
        "class QSparseLocalAlgorithm(Algorithm):\n",
        "    def __init__(self, q_sparse_local_optimizer: QSparseLocalOptimizer, hierarchical: bool = True):\n",
        "        \"\"\"\n",
        "        Create an instance of the\n",
        "        `QSparseLocal Algorithm <https://tutorials.baguasys.com/algorithms/q-adam>`_\n",
        "        .\n",
        "\n",
        "        Args:\n",
        "            q_sparse_local_optimizer: A QSparseLocalOptimizer initialized with model parameters.\n",
        "            hierarchical: Enable hierarchical communication.\n",
        "        \"\"\"\n",
        "        self.hierarchical = hierarchical\n",
        "        self.optimizer = q_sparse_local_optimizer\n",
        "\n",
        "    def reify(self, process_group: BaguaProcessGroup) -> QSparseLocalAlgorithmImpl:\n",
        "        return QSparseLocalAlgorithmImpl(\n",
        "            process_group,\n",
        "            q_sparse_local_optimizer=self.optimizer,\n",
        "            hierarchical=self.hierarchical,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPIWQ8CvcZ0G"
      },
      "source": [
        "## **MNIST example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9tjsC05-5qV_",
        "outputId": "6bcfa747-b123-4584-ca39-467d3eeb7f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc_version:  11.1\n",
            "The destination directory /root/.local/share/bagua/nccl already exists.\n",
            "Installing nccl 2.10.3 for CUDA 11.1 to: /root/.local/share/bagua/nccl\n",
            "Downloading https://developer.download.nvidia.com/compute/redist/nccl/v2.10/nccl_2.10.3-1+cuda11.0_x86_64.txz...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nccl_2.10.3-1+cuda11.0_x86_64.txz: 138MB [00:01, 112MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting...\n",
            "Installing...\n",
            "Cleaning up...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gevent/threadpool.py\", line 157, in _before_run_task\n",
            "    _sys.settrace(_get_thread_trace())\n",
            "\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gevent/threadpool.py\", line 162, in _after_run_task\n",
            "    _sys.settrace(None)\n",
            "\n",
            "INFO:root:Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.308714\n",
            "INFO:root:Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.080063\n",
            "INFO:root:Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.913053\n",
            "INFO:root:Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.398406\n",
            "INFO:root:Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.035040\n",
            "INFO:root:Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.925152\n",
            "INFO:root:Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.739438\n",
            "INFO:root:Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.619415\n",
            "INFO:root:Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.784902\n",
            "INFO:root:Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.650173\n",
            "INFO:root:Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.847470\n",
            "INFO:root:Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.459143\n",
            "INFO:root:Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.461394\n",
            "INFO:root:Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.402816\n",
            "INFO:root:Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.436759\n",
            "INFO:root:Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.660793\n",
            "INFO:root:Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.367578\n",
            "INFO:root:Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.354273\n",
            "INFO:root:Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.242250\n",
            "INFO:root:Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.252935\n",
            "INFO:root:Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.459057\n",
            "INFO:root:Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.303887\n",
            "INFO:root:Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.352842\n",
            "INFO:root:Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.228299\n",
            "INFO:root:Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.321731\n",
            "INFO:root:Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.278280\n",
            "INFO:root:Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.314219\n",
            "INFO:root:Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.251091\n",
            "INFO:root:Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.159112\n",
            "INFO:root:Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.211640\n",
            "INFO:root:Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.284836\n",
            "INFO:root:Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.370881\n",
            "INFO:root:Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.482804\n",
            "INFO:root:Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.224845\n",
            "INFO:root:Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.231396\n",
            "INFO:root:Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.396621\n",
            "INFO:root:Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.347561\n",
            "INFO:root:Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.254348\n",
            "INFO:root:Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.177096\n",
            "INFO:root:Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.174130\n",
            "INFO:root:Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.175767\n",
            "INFO:root:Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.281113\n",
            "INFO:root:Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.124672\n",
            "INFO:root:Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.262106\n",
            "INFO:root:Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.327326\n",
            "INFO:root:Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.299115\n",
            "INFO:root:Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.312879\n",
            "INFO:root:Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.109231\n",
            "INFO:root:Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.113513\n",
            "INFO:root:Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.195833\n",
            "INFO:root:Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.144875\n",
            "INFO:root:Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.168457\n",
            "INFO:root:Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.275023\n",
            "INFO:root:Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.087458\n",
            "INFO:root:Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.230621\n",
            "INFO:root:Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.151010\n",
            "INFO:root:Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.150693\n",
            "INFO:root:Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.217982\n",
            "INFO:root:Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.301951\n",
            "INFO:root:Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.208357\n",
            "INFO:root:Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.057203\n",
            "INFO:root:Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.064152\n",
            "INFO:root:Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.087965\n",
            "INFO:root:Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.270280\n",
            "INFO:root:Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.045196\n",
            "INFO:root:Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.196271\n",
            "INFO:root:Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.112768\n",
            "INFO:root:Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.193273\n",
            "INFO:root:Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.119980\n",
            "INFO:root:Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.087747\n",
            "INFO:root:Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.079636\n",
            "INFO:root:Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.066613\n",
            "INFO:root:Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.096480\n",
            "INFO:root:Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.070077\n",
            "INFO:root:Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.095628\n",
            "INFO:root:Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.067757\n",
            "INFO:root:Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.114094\n",
            "INFO:root:Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.245214\n",
            "INFO:root:Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.177003\n",
            "INFO:root:Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.179684\n",
            "INFO:root:Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.187582\n",
            "INFO:root:Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.054121\n",
            "INFO:root:Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.181669\n",
            "INFO:root:Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.293396\n",
            "INFO:root:Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.160893\n",
            "INFO:root:Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.313757\n",
            "INFO:root:Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.114710\n",
            "INFO:root:Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.290011\n",
            "INFO:root:Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.176087\n",
            "INFO:root:Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.054086\n",
            "INFO:root:Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.202240\n",
            "INFO:root:Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.243065\n",
            "INFO:root:Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.095463\n",
            "INFO:root:Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.169342\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0564, Accuracy: 9816/10000 (98%)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "INFO:root:Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.095359\n",
            "INFO:root:Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.133706\n",
            "INFO:root:Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.080375\n",
            "INFO:root:Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.156037\n",
            "INFO:root:Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.040738\n",
            "INFO:root:Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.209646\n",
            "INFO:root:Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.085732\n",
            "INFO:root:Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.114707\n",
            "INFO:root:Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.087043\n",
            "INFO:root:Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.080139\n",
            "INFO:root:Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.168865\n",
            "INFO:root:Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.056595\n",
            "INFO:root:Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.112294\n",
            "INFO:root:Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.105127\n",
            "INFO:root:Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.127190\n",
            "INFO:root:Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.214525\n",
            "INFO:root:Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.038153\n",
            "INFO:root:Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.102876\n",
            "INFO:root:Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.097636\n",
            "INFO:root:Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.065439\n",
            "INFO:root:Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.282009\n",
            "INFO:root:Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.057713\n",
            "INFO:root:Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.046299\n",
            "INFO:root:Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.038724\n",
            "INFO:root:Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.031033\n",
            "INFO:root:Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.048431\n",
            "INFO:root:Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.047184\n",
            "INFO:root:Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.037068\n",
            "INFO:root:Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.102646\n",
            "INFO:root:Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.193673\n",
            "INFO:root:Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.084924\n",
            "INFO:root:Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.197760\n",
            "INFO:root:Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.291751\n",
            "INFO:root:Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.023054\n",
            "INFO:root:Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.160296\n",
            "INFO:root:Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.235958\n",
            "INFO:root:Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.116093\n",
            "INFO:root:Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.131432\n",
            "INFO:root:Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.058259\n",
            "INFO:root:Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.050287\n",
            "INFO:root:Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.114697\n",
            "INFO:root:Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.180564\n",
            "INFO:root:Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.047420\n",
            "INFO:root:Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.104003\n",
            "INFO:root:Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.241445\n",
            "INFO:root:Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.099508\n",
            "INFO:root:Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.114729\n",
            "INFO:root:Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.109734\n",
            "INFO:root:Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.019552\n",
            "INFO:root:Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.066231\n",
            "INFO:root:Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.044126\n",
            "INFO:root:Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.066376\n",
            "INFO:root:Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.136243\n",
            "INFO:root:Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.051275\n",
            "INFO:root:Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.234112\n",
            "INFO:root:Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.116255\n",
            "INFO:root:Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.154641\n",
            "INFO:root:Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.052171\n",
            "INFO:root:Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.182958\n",
            "INFO:root:Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.036544\n",
            "INFO:root:Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.055889\n",
            "INFO:root:Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.040517\n",
            "INFO:root:Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.155332\n",
            "INFO:root:Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.255101\n",
            "INFO:root:Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.064913\n",
            "INFO:root:Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.067080\n",
            "INFO:root:Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.082714\n",
            "INFO:root:Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.099836\n",
            "INFO:root:Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.132317\n",
            "INFO:root:Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.162077\n",
            "INFO:root:Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.062815\n",
            "INFO:root:Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.031341\n",
            "INFO:root:Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.063909\n",
            "INFO:root:Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.099739\n",
            "INFO:root:Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.047024\n",
            "INFO:root:Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.040980\n",
            "INFO:root:Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.096309\n",
            "INFO:root:Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.118450\n",
            "INFO:root:Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.096988\n",
            "INFO:root:Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.172290\n",
            "INFO:root:Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.126473\n",
            "INFO:root:Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.042870\n",
            "INFO:root:Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.038092\n",
            "INFO:root:Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.146866\n",
            "INFO:root:Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.034379\n",
            "INFO:root:Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.169298\n",
            "INFO:root:Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.052050\n",
            "INFO:root:Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.108526\n",
            "INFO:root:Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.067250\n",
            "INFO:root:Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.093482\n",
            "INFO:root:Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.017738\n",
            "INFO:root:Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.147539\n",
            "INFO:root:Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.132946\n",
            "INFO:root:Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.075505\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0431, Accuracy: 9848/10000 (98%)\n",
            "\n",
            "INFO:root:Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.135694\n",
            "INFO:root:Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.019439\n",
            "INFO:root:Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.063820\n",
            "INFO:root:Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.101850\n",
            "INFO:root:Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.061664\n",
            "INFO:root:Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.216730\n",
            "INFO:root:Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.066357\n",
            "INFO:root:Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.100250\n",
            "INFO:root:Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.086568\n",
            "INFO:root:Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.046671\n",
            "INFO:root:Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.118088\n",
            "INFO:root:Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.042111\n",
            "INFO:root:Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.065334\n",
            "INFO:root:Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.025881\n",
            "INFO:root:Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.162707\n",
            "INFO:root:Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.040765\n",
            "INFO:root:Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.028867\n",
            "INFO:root:Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.013603\n",
            "INFO:root:Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.130582\n",
            "INFO:root:Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.020193\n",
            "INFO:root:Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.045678\n",
            "INFO:root:Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.031878\n",
            "INFO:root:Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.013305\n",
            "INFO:root:Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.091566\n",
            "INFO:root:Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.021592\n",
            "INFO:root:Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.020035\n",
            "INFO:root:Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.119250\n",
            "INFO:root:Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.050324\n",
            "INFO:root:Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.059467\n",
            "INFO:root:Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.040986\n",
            "INFO:root:Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.062557\n",
            "INFO:root:Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.080710\n",
            "INFO:root:Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.391637\n",
            "INFO:root:Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.013256\n",
            "INFO:root:Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.148296\n",
            "INFO:root:Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.105499\n",
            "INFO:root:Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.075439\n",
            "INFO:root:Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.082339\n",
            "INFO:root:Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.055271\n",
            "INFO:root:Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.043058\n",
            "INFO:root:Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.073156\n",
            "INFO:root:Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.170908\n",
            "INFO:root:Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.026096\n",
            "INFO:root:Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.067696\n",
            "INFO:root:Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.123348\n",
            "INFO:root:Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.109355\n",
            "INFO:root:Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.093941\n",
            "INFO:root:Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.046221\n",
            "INFO:root:Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.018021\n",
            "INFO:root:Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.116397\n",
            "INFO:root:Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.052062\n",
            "INFO:root:Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.047892\n",
            "INFO:root:Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.104663\n",
            "INFO:root:Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.023539\n",
            "INFO:root:Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.190282\n",
            "INFO:root:Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.063197\n",
            "INFO:root:Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.019518\n",
            "INFO:root:Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.118879\n",
            "INFO:root:Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.055606\n",
            "INFO:root:Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.040701\n",
            "INFO:root:Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.033552\n",
            "INFO:root:Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.118930\n",
            "INFO:root:Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.012596\n",
            "INFO:root:Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.071562\n",
            "INFO:root:Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.068351\n",
            "INFO:root:Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.057687\n",
            "INFO:root:Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.089884\n",
            "INFO:root:Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.022350\n",
            "INFO:root:Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.047548\n",
            "INFO:root:Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.015149\n",
            "INFO:root:Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.089513\n",
            "INFO:root:Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.018628\n",
            "INFO:root:Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.127260\n",
            "INFO:root:Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.066662\n",
            "INFO:root:Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.049227\n",
            "INFO:root:Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.050535\n",
            "INFO:root:Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.173563\n",
            "INFO:root:Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.072583\n",
            "INFO:root:Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.104854\n",
            "INFO:root:Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.091369\n",
            "INFO:root:Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.076958\n",
            "INFO:root:Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.011100\n",
            "INFO:root:Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.015676\n",
            "INFO:root:Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.066761\n",
            "INFO:root:Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.026646\n",
            "INFO:root:Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.226365\n",
            "INFO:root:Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.039577\n",
            "INFO:root:Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.103396\n",
            "INFO:root:Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.054155\n",
            "INFO:root:Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.104682\n",
            "INFO:root:Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.112437\n",
            "INFO:root:Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.114166\n",
            "INFO:root:Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.035066\n",
            "INFO:root:Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.030373\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0561, Accuracy: 9805/10000 (98%)\n",
            "\n",
            "INFO:root:Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.151361\n",
            "INFO:root:Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.062352\n",
            "INFO:root:Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.052692\n",
            "INFO:root:Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.063516\n",
            "INFO:root:Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.028214\n",
            "INFO:root:Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.013674\n",
            "INFO:root:Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.035946\n",
            "INFO:root:Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.074328\n",
            "INFO:root:Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.112638\n",
            "INFO:root:Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.024873\n",
            "INFO:root:Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.103706\n",
            "INFO:root:Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.013664\n",
            "INFO:root:Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.074393\n",
            "INFO:root:Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.057588\n",
            "INFO:root:Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.031887\n",
            "INFO:root:Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.046312\n",
            "INFO:root:Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.061475\n",
            "INFO:root:Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.036158\n",
            "INFO:root:Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.050148\n",
            "INFO:root:Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.019691\n",
            "INFO:root:Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.059250\n",
            "INFO:root:Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.031556\n",
            "INFO:root:Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.023024\n",
            "INFO:root:Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.089117\n",
            "INFO:root:Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.015785\n",
            "INFO:root:Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.021387\n",
            "INFO:root:Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.082864\n",
            "INFO:root:Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.018425\n",
            "INFO:root:Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.012433\n",
            "INFO:root:Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.015529\n",
            "INFO:root:Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.074698\n",
            "INFO:root:Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.141576\n",
            "INFO:root:Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.186126\n",
            "INFO:root:Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.050251\n",
            "INFO:root:Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.022690\n",
            "INFO:root:Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.146966\n",
            "INFO:root:Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.138345\n",
            "INFO:root:Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.031904\n",
            "INFO:root:Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.017136\n",
            "INFO:root:Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.052843\n",
            "INFO:root:Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.189472\n",
            "INFO:root:Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.056882\n",
            "INFO:root:Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.013978\n",
            "INFO:root:Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.032681\n",
            "INFO:root:Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.093479\n",
            "INFO:root:Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.048928\n",
            "INFO:root:Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.095848\n",
            "INFO:root:Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.034289\n",
            "INFO:root:Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.013644\n",
            "INFO:root:Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.049908\n",
            "INFO:root:Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.013166\n",
            "INFO:root:Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.044234\n",
            "INFO:root:Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.039275\n",
            "INFO:root:Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.009532\n",
            "INFO:root:Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.168221\n",
            "INFO:root:Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.094678\n",
            "INFO:root:Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.062170\n",
            "INFO:root:Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.058445\n",
            "INFO:root:Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.255175\n",
            "INFO:root:Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.049646\n",
            "INFO:root:Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.012612\n",
            "INFO:root:Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.039181\n",
            "INFO:root:Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.029168\n",
            "INFO:root:Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.151066\n",
            "INFO:root:Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.061661\n",
            "INFO:root:Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.038776\n",
            "INFO:root:Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.054534\n",
            "INFO:root:Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.046005\n",
            "INFO:root:Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.048604\n",
            "INFO:root:Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.038525\n",
            "INFO:root:Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.035092\n",
            "INFO:root:Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.008405\n",
            "INFO:root:Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.019768\n",
            "INFO:root:Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.059841\n",
            "INFO:root:Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.020803\n",
            "INFO:root:Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.039003\n",
            "INFO:root:Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.052650\n",
            "INFO:root:Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.039200\n",
            "INFO:root:Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.067916\n",
            "INFO:root:Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.040070\n",
            "INFO:root:Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.050400\n",
            "INFO:root:Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.013302\n",
            "INFO:root:Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.017524\n",
            "INFO:root:Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.068811\n",
            "INFO:root:Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.060219\n",
            "INFO:root:Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.112661\n",
            "INFO:root:Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.039955\n",
            "INFO:root:Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.126107\n",
            "INFO:root:Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.067838\n",
            "INFO:root:Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.028129\n",
            "INFO:root:Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.018135\n",
            "INFO:root:Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.227575\n",
            "INFO:root:Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.027799\n",
            "INFO:root:Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.006752\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0310, Accuracy: 9898/10000 (99%)\n",
            "\n",
            "INFO:root:Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.095290\n",
            "INFO:root:Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.004134\n",
            "INFO:root:Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.019030\n",
            "INFO:root:Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.077224\n",
            "INFO:root:Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.017734\n",
            "INFO:root:Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.089078\n",
            "INFO:root:Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.081059\n",
            "INFO:root:Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.090097\n",
            "INFO:root:Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.015293\n",
            "INFO:root:Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.052132\n",
            "INFO:root:Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.045032\n",
            "INFO:root:Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.011203\n",
            "INFO:root:Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.065595\n",
            "INFO:root:Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.010096\n",
            "INFO:root:Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.032842\n",
            "INFO:root:Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.041012\n",
            "INFO:root:Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.022520\n",
            "INFO:root:Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.018324\n",
            "INFO:root:Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.036492\n",
            "INFO:root:Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.019627\n",
            "INFO:root:Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.029112\n",
            "INFO:root:Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.033116\n",
            "INFO:root:Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.003493\n",
            "INFO:root:Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.035996\n",
            "INFO:root:Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.013823\n",
            "INFO:root:Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.057610\n",
            "INFO:root:Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.014812\n",
            "INFO:root:Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.023568\n",
            "INFO:root:Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.024209\n",
            "INFO:root:Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.040114\n",
            "INFO:root:Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.013538\n",
            "INFO:root:Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.072852\n",
            "INFO:root:Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.323015\n",
            "INFO:root:Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.023899\n",
            "INFO:root:Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.087515\n",
            "INFO:root:Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.077004\n",
            "INFO:root:Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.034239\n",
            "INFO:root:Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.061412\n",
            "INFO:root:Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.027253\n",
            "INFO:root:Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.022818\n",
            "INFO:root:Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.123479\n",
            "INFO:root:Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.173267\n",
            "INFO:root:Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.021085\n",
            "INFO:root:Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.095994\n",
            "INFO:root:Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.091063\n",
            "INFO:root:Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.068647\n",
            "INFO:root:Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.102919\n",
            "INFO:root:Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.014694\n",
            "INFO:root:Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.024886\n",
            "INFO:root:Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.098694\n",
            "INFO:root:Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.012343\n",
            "INFO:root:Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.042582\n",
            "INFO:root:Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.025581\n",
            "INFO:root:Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.017292\n",
            "INFO:root:Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.127690\n",
            "INFO:root:Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.010446\n",
            "INFO:root:Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.038085\n",
            "INFO:root:Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.134942\n",
            "INFO:root:Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.094566\n",
            "INFO:root:Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.012247\n",
            "INFO:root:Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.014751\n",
            "INFO:root:Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.043749\n",
            "INFO:root:Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.087778\n",
            "INFO:root:Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.054838\n",
            "INFO:root:Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.051064\n",
            "INFO:root:Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.004678\n",
            "INFO:root:Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.033320\n",
            "INFO:root:Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.040373\n",
            "INFO:root:Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.106517\n",
            "INFO:root:Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.012577\n",
            "INFO:root:Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.023953\n",
            "INFO:root:Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.013790\n",
            "INFO:root:Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.041116\n",
            "INFO:root:Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.003132\n",
            "INFO:root:Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.048881\n",
            "INFO:root:Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.022644\n",
            "INFO:root:Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.142217\n",
            "INFO:root:Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.055345\n",
            "INFO:root:Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.058647\n",
            "INFO:root:Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.148100\n",
            "INFO:root:Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.067510\n",
            "INFO:root:Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.019283\n",
            "INFO:root:Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.044518\n",
            "INFO:root:Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.058368\n",
            "INFO:root:Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.029257\n",
            "INFO:root:Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.048482\n",
            "INFO:root:Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.004942\n",
            "INFO:root:Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.084041\n",
            "INFO:root:Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.034094\n",
            "INFO:root:Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.049483\n",
            "INFO:root:Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.089239\n",
            "INFO:root:Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.138133\n",
            "INFO:root:Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.068483\n",
            "INFO:root:Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.004907\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0318, Accuracy: 9894/10000 (99%)\n",
            "\n",
            "INFO:root:Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.089436\n",
            "INFO:root:Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.040831\n",
            "INFO:root:Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.054571\n",
            "INFO:root:Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.037329\n",
            "INFO:root:Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.071081\n",
            "INFO:root:Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.031342\n",
            "INFO:root:Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.007418\n",
            "INFO:root:Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.021133\n",
            "INFO:root:Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.093429\n",
            "INFO:root:Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.018278\n",
            "INFO:root:Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.093776\n",
            "INFO:root:Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.009826\n",
            "INFO:root:Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.070084\n",
            "INFO:root:Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.015413\n",
            "INFO:root:Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.096045\n",
            "INFO:root:Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.074170\n",
            "INFO:root:Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.028664\n",
            "INFO:root:Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.028057\n",
            "INFO:root:Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.014868\n",
            "INFO:root:Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.055060\n",
            "INFO:root:Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.045392\n",
            "INFO:root:Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.028865\n",
            "INFO:root:Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.005322\n",
            "INFO:root:Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.039640\n",
            "INFO:root:Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.012205\n",
            "INFO:root:Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.055639\n",
            "INFO:root:Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.026899\n",
            "INFO:root:Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.045930\n",
            "INFO:root:Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.080437\n",
            "INFO:root:Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.055384\n",
            "INFO:root:Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.027562\n",
            "INFO:root:Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.075752\n",
            "INFO:root:Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.093365\n",
            "INFO:root:Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.015531\n",
            "INFO:root:Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.036970\n",
            "INFO:root:Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.126043\n",
            "INFO:root:Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.049815\n",
            "INFO:root:Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.031620\n",
            "INFO:root:Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.020936\n",
            "INFO:root:Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.101657\n",
            "INFO:root:Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.037260\n",
            "INFO:root:Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.060919\n",
            "INFO:root:Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.004784\n",
            "INFO:root:Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.031342\n",
            "INFO:root:Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.019158\n",
            "INFO:root:Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.018729\n",
            "INFO:root:Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.087434\n",
            "INFO:root:Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.004610\n",
            "INFO:root:Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.056651\n",
            "INFO:root:Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.027344\n",
            "INFO:root:Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.008217\n",
            "INFO:root:Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.095963\n",
            "INFO:root:Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.018098\n",
            "INFO:root:Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.072061\n",
            "INFO:root:Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.099983\n",
            "INFO:root:Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.060289\n",
            "INFO:root:Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.010054\n",
            "INFO:root:Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.092076\n",
            "INFO:root:Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.091026\n",
            "INFO:root:Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.009969\n",
            "INFO:root:Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.018232\n",
            "INFO:root:Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.008896\n",
            "INFO:root:Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.051249\n",
            "INFO:root:Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.164052\n",
            "INFO:root:Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.148752\n",
            "INFO:root:Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.012125\n",
            "INFO:root:Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.038793\n",
            "INFO:root:Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.058477\n",
            "INFO:root:Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.060919\n",
            "INFO:root:Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.061295\n",
            "INFO:root:Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.060970\n",
            "INFO:root:Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.074046\n",
            "INFO:root:Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.081938\n",
            "INFO:root:Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.006213\n",
            "INFO:root:Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.069105\n",
            "INFO:root:Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.026127\n",
            "INFO:root:Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.093196\n",
            "INFO:root:Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.124181\n",
            "INFO:root:Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.010185\n",
            "INFO:root:Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.111558\n",
            "INFO:root:Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.090327\n",
            "INFO:root:Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.020191\n",
            "INFO:root:Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.029147\n",
            "INFO:root:Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.060995\n",
            "INFO:root:Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.036597\n",
            "INFO:root:Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.040015\n",
            "INFO:root:Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.024185\n",
            "INFO:root:Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.021898\n",
            "INFO:root:Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.013750\n",
            "INFO:root:Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.084281\n",
            "INFO:root:Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.024007\n",
            "INFO:root:Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.195390\n",
            "INFO:root:Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.054600\n",
            "INFO:root:Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.024892\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0281, Accuracy: 9906/10000 (99%)\n",
            "\n",
            "INFO:root:Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.052765\n",
            "INFO:root:Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.049874\n",
            "INFO:root:Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.057966\n",
            "INFO:root:Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.026032\n",
            "INFO:root:Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.062977\n",
            "INFO:root:Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.040061\n",
            "INFO:root:Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.028883\n",
            "INFO:root:Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.023097\n",
            "INFO:root:Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.029059\n",
            "INFO:root:Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.014259\n",
            "INFO:root:Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.114509\n",
            "INFO:root:Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.028443\n",
            "INFO:root:Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.019406\n",
            "INFO:root:Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.010408\n",
            "INFO:root:Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.047280\n",
            "INFO:root:Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.084344\n",
            "INFO:root:Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.038960\n",
            "INFO:root:Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.009809\n",
            "INFO:root:Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.039848\n",
            "INFO:root:Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.021236\n",
            "INFO:root:Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.040293\n",
            "INFO:root:Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.022727\n",
            "INFO:root:Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.001533\n",
            "INFO:root:Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.031535\n",
            "INFO:root:Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.003312\n",
            "INFO:root:Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.023385\n",
            "INFO:root:Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.007789\n",
            "INFO:root:Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.004064\n",
            "INFO:root:Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.002412\n",
            "INFO:root:Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.082053\n",
            "INFO:root:Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.052661\n",
            "INFO:root:Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.140970\n",
            "INFO:root:Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.279220\n",
            "INFO:root:Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.006605\n",
            "INFO:root:Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.027494\n",
            "INFO:root:Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.075333\n",
            "INFO:root:Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.131702\n",
            "INFO:root:Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.069622\n",
            "INFO:root:Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.014839\n",
            "INFO:root:Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.011260\n",
            "INFO:root:Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.211685\n",
            "INFO:root:Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.141481\n",
            "INFO:root:Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.006705\n",
            "INFO:root:Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.003430\n",
            "INFO:root:Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.062110\n",
            "INFO:root:Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.014609\n",
            "INFO:root:Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.043974\n",
            "INFO:root:Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.011062\n",
            "INFO:root:Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.013571\n",
            "INFO:root:Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.025195\n",
            "INFO:root:Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.002445\n",
            "INFO:root:Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.037415\n",
            "INFO:root:Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.004130\n",
            "INFO:root:Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.002925\n",
            "INFO:root:Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.077301\n",
            "INFO:root:Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.039293\n",
            "INFO:root:Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.002985\n",
            "INFO:root:Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.124328\n",
            "INFO:root:Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.081745\n",
            "INFO:root:Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.053065\n",
            "INFO:root:Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.029718\n",
            "INFO:root:Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.011061\n",
            "INFO:root:Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.003440\n",
            "INFO:root:Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.063979\n",
            "INFO:root:Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.010675\n",
            "INFO:root:Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.012649\n",
            "INFO:root:Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.074750\n",
            "INFO:root:Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.009900\n",
            "INFO:root:Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.101014\n",
            "INFO:root:Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.012059\n",
            "INFO:root:Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.025428\n",
            "INFO:root:Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.014833\n",
            "INFO:root:Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.026255\n",
            "INFO:root:Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.010240\n",
            "INFO:root:Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.017456\n",
            "INFO:root:Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.011444\n",
            "INFO:root:Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.024237\n",
            "INFO:root:Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.068379\n",
            "INFO:root:Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.051597\n",
            "INFO:root:Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.018135\n",
            "INFO:root:Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.066732\n",
            "INFO:root:Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.004334\n",
            "INFO:root:Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.011708\n",
            "INFO:root:Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.080266\n",
            "INFO:root:Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.023162\n",
            "INFO:root:Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.107957\n",
            "INFO:root:Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.015929\n",
            "INFO:root:Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.018323\n",
            "INFO:root:Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.032103\n",
            "INFO:root:Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.090012\n",
            "INFO:root:Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.022502\n",
            "INFO:root:Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.295678\n",
            "INFO:root:Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.021728\n",
            "INFO:root:Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.030590\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0313, Accuracy: 9903/10000 (99%)\n",
            "\n",
            "INFO:root:Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.050588\n",
            "INFO:root:Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.027253\n",
            "INFO:root:Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.049522\n",
            "INFO:root:Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.051430\n",
            "INFO:root:Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.007728\n",
            "INFO:root:Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.009406\n",
            "INFO:root:Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.028594\n",
            "INFO:root:Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.118704\n",
            "INFO:root:Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.041475\n",
            "INFO:root:Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.013695\n",
            "INFO:root:Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.072415\n",
            "INFO:root:Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.009538\n",
            "INFO:root:Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.004389\n",
            "INFO:root:Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.020417\n",
            "INFO:root:Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.142704\n",
            "INFO:root:Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.105302\n",
            "INFO:root:Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.067482\n",
            "INFO:root:Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.000647\n",
            "INFO:root:Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.009685\n",
            "INFO:root:Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.001051\n",
            "INFO:root:Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.020907\n",
            "INFO:root:Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.037223\n",
            "INFO:root:Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.001772\n",
            "INFO:root:Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.014259\n",
            "INFO:root:Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.028934\n",
            "INFO:root:Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.078536\n",
            "INFO:root:Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.006462\n",
            "INFO:root:Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.005747\n",
            "INFO:root:Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.026846\n",
            "INFO:root:Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.047674\n",
            "INFO:root:Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.021534\n",
            "INFO:root:Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.053781\n",
            "INFO:root:Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.215648\n",
            "INFO:root:Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.047961\n",
            "INFO:root:Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.026097\n",
            "INFO:root:Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.026335\n",
            "INFO:root:Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.064571\n",
            "INFO:root:Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.066808\n",
            "INFO:root:Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.005507\n",
            "INFO:root:Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.025788\n",
            "INFO:root:Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.060366\n",
            "INFO:root:Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.037255\n",
            "INFO:root:Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.019228\n",
            "INFO:root:Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.035543\n",
            "INFO:root:Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.027918\n",
            "INFO:root:Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.027266\n",
            "INFO:root:Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.114857\n",
            "INFO:root:Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.004312\n",
            "INFO:root:Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.034440\n",
            "INFO:root:Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.034840\n",
            "INFO:root:Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.002718\n",
            "INFO:root:Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.019720\n",
            "INFO:root:Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.081670\n",
            "INFO:root:Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.004288\n",
            "INFO:root:Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.058052\n",
            "INFO:root:Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.024582\n",
            "INFO:root:Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.014077\n",
            "INFO:root:Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.051260\n",
            "INFO:root:Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.082379\n",
            "INFO:root:Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.017232\n",
            "INFO:root:Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.023956\n",
            "INFO:root:Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.013127\n",
            "INFO:root:Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.017453\n",
            "INFO:root:Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.043751\n",
            "INFO:root:Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.045938\n",
            "INFO:root:Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.007996\n",
            "INFO:root:Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.016609\n",
            "INFO:root:Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.029861\n",
            "INFO:root:Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.021728\n",
            "INFO:root:Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.023506\n",
            "INFO:root:Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.035044\n",
            "INFO:root:Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.026953\n",
            "INFO:root:Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.103035\n",
            "INFO:root:Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.009245\n",
            "INFO:root:Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.048724\n",
            "INFO:root:Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.045408\n",
            "INFO:root:Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.099416\n",
            "INFO:root:Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.150825\n",
            "INFO:root:Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.016646\n",
            "INFO:root:Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.026267\n",
            "INFO:root:Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.045070\n",
            "INFO:root:Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.024404\n",
            "INFO:root:Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.031643\n",
            "INFO:root:Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.043776\n",
            "INFO:root:Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.078094\n",
            "INFO:root:Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.034331\n",
            "INFO:root:Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.027249\n",
            "INFO:root:Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.031897\n",
            "INFO:root:Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.105013\n",
            "INFO:root:Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.037472\n",
            "INFO:root:Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.043450\n",
            "INFO:root:Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.149228\n",
            "INFO:root:Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.017650\n",
            "INFO:root:Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.028830\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0285, Accuracy: 9902/10000 (99%)\n",
            "\n",
            "INFO:root:Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.018366\n",
            "INFO:root:Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.004197\n",
            "INFO:root:Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.012149\n",
            "INFO:root:Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.057126\n",
            "INFO:root:Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.011280\n",
            "INFO:root:Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.071588\n",
            "INFO:root:Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.005059\n",
            "INFO:root:Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.080888\n",
            "INFO:root:Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.023629\n",
            "INFO:root:Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.007350\n",
            "INFO:root:Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.019738\n",
            "INFO:root:Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.005810\n",
            "INFO:root:Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.008622\n",
            "INFO:root:Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.047378\n",
            "INFO:root:Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.005638\n",
            "INFO:root:Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.024249\n",
            "INFO:root:Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.039768\n",
            "INFO:root:Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.039051\n",
            "INFO:root:Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.070996\n",
            "INFO:root:Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.004880\n",
            "INFO:root:Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.009859\n",
            "INFO:root:Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.011141\n",
            "INFO:root:Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.003784\n",
            "INFO:root:Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.012854\n",
            "INFO:root:Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.005892\n",
            "INFO:root:Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.014038\n",
            "INFO:root:Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.049544\n",
            "INFO:root:Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.032223\n",
            "INFO:root:Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.007782\n",
            "INFO:root:Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.026742\n",
            "INFO:root:Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.076750\n",
            "INFO:root:Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.160025\n",
            "INFO:root:Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.082071\n",
            "INFO:root:Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.006355\n",
            "INFO:root:Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.126317\n",
            "INFO:root:Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.015139\n",
            "INFO:root:Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.059955\n",
            "INFO:root:Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.034001\n",
            "INFO:root:Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.044552\n",
            "INFO:root:Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.011492\n",
            "INFO:root:Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.041531\n",
            "INFO:root:Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.032762\n",
            "INFO:root:Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.002133\n",
            "INFO:root:Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.013618\n",
            "INFO:root:Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.042804\n",
            "INFO:root:Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.005544\n",
            "INFO:root:Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.017254\n",
            "INFO:root:Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.018043\n",
            "INFO:root:Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.013303\n",
            "INFO:root:Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.025998\n",
            "INFO:root:Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.076396\n",
            "INFO:root:Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.039283\n",
            "INFO:root:Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.027501\n",
            "INFO:root:Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.005751\n",
            "INFO:root:Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.025883\n",
            "INFO:root:Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.038723\n",
            "INFO:root:Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.040589\n",
            "INFO:root:Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.018748\n",
            "INFO:root:Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.046676\n",
            "INFO:root:Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.001725\n",
            "INFO:root:Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.007701\n",
            "INFO:root:Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.008338\n",
            "INFO:root:Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.032527\n",
            "INFO:root:Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.029322\n",
            "INFO:root:Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.011270\n",
            "INFO:root:Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.002757\n",
            "INFO:root:Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.024956\n",
            "INFO:root:Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.005242\n",
            "INFO:root:Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.008663\n",
            "INFO:root:Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.003679\n",
            "INFO:root:Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.010948\n",
            "INFO:root:Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.001248\n",
            "INFO:root:Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.006207\n",
            "INFO:root:Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.004807\n",
            "INFO:root:Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.021392\n",
            "INFO:root:Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.007694\n",
            "INFO:root:Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.055014\n",
            "INFO:root:Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.028284\n",
            "INFO:root:Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.033084\n",
            "INFO:root:Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.005919\n",
            "INFO:root:Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.023133\n",
            "INFO:root:Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.002417\n",
            "INFO:root:Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.005928\n",
            "INFO:root:Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.031869\n",
            "INFO:root:Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.025546\n",
            "INFO:root:Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.022988\n",
            "INFO:root:Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.058790\n",
            "INFO:root:Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.121634\n",
            "INFO:root:Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.025774\n",
            "INFO:root:Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.047082\n",
            "INFO:root:Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.010163\n",
            "INFO:root:Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.131469\n",
            "INFO:root:Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.016925\n",
            "INFO:root:Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.002291\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0264, Accuracy: 9911/10000 (99%)\n",
            "\n",
            "INFO:root:Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.040882\n",
            "INFO:root:Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.008643\n",
            "INFO:root:Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.067102\n",
            "INFO:root:Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.010083\n",
            "INFO:root:Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.012295\n",
            "INFO:root:Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.004848\n",
            "INFO:root:Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.025384\n",
            "INFO:root:Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.019933\n",
            "INFO:root:Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.014684\n",
            "INFO:root:Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.015078\n",
            "INFO:root:Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.008041\n",
            "INFO:root:Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.006304\n",
            "INFO:root:Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.016225\n",
            "INFO:root:Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.006848\n",
            "INFO:root:Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.064760\n",
            "INFO:root:Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.150106\n",
            "INFO:root:Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.024907\n",
            "INFO:root:Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.005394\n",
            "INFO:root:Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.020336\n",
            "INFO:root:Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.046019\n",
            "INFO:root:Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.012108\n",
            "INFO:root:Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.022791\n",
            "INFO:root:Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.001658\n",
            "INFO:root:Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.022418\n",
            "INFO:root:Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.007203\n",
            "INFO:root:Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.020575\n",
            "INFO:root:Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.017061\n",
            "INFO:root:Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.020769\n",
            "INFO:root:Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.012280\n",
            "INFO:root:Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.009039\n",
            "INFO:root:Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.023651\n",
            "INFO:root:Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.016694\n",
            "INFO:root:Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.097230\n",
            "INFO:root:Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.009647\n",
            "INFO:root:Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.081863\n",
            "INFO:root:Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.019713\n",
            "INFO:root:Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.017738\n",
            "INFO:root:Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.016312\n",
            "INFO:root:Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.018383\n",
            "INFO:root:Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.002266\n",
            "INFO:root:Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.047006\n",
            "INFO:root:Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.070299\n",
            "INFO:root:Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.019964\n",
            "INFO:root:Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.050597\n",
            "INFO:root:Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.026523\n",
            "INFO:root:Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.007604\n",
            "INFO:root:Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.004940\n",
            "INFO:root:Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.032842\n",
            "INFO:root:Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.011632\n",
            "INFO:root:Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.084956\n",
            "INFO:root:Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.004579\n",
            "INFO:root:Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.012200\n",
            "INFO:root:Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.019320\n",
            "INFO:root:Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.006350\n",
            "INFO:root:Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.173652\n",
            "INFO:root:Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.072197\n",
            "INFO:root:Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.002519\n",
            "INFO:root:Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.009957\n",
            "INFO:root:Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.062970\n",
            "INFO:root:Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.011817\n",
            "INFO:root:Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.011118\n",
            "INFO:root:Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.023306\n",
            "INFO:root:Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.031282\n",
            "INFO:root:Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.028024\n",
            "INFO:root:Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.067027\n",
            "INFO:root:Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.012148\n",
            "INFO:root:Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.062445\n",
            "INFO:root:Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.013444\n",
            "INFO:root:Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.012896\n",
            "INFO:root:Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.010529\n",
            "INFO:root:Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.052985\n",
            "INFO:root:Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.012640\n",
            "INFO:root:Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.064742\n",
            "INFO:root:Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.000867\n",
            "INFO:root:Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.009307\n",
            "INFO:root:Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.007028\n",
            "INFO:root:Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.078988\n",
            "INFO:root:Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.016385\n",
            "INFO:root:Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.022804\n",
            "INFO:root:Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.016299\n",
            "INFO:root:Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.008952\n",
            "INFO:root:Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.041287\n",
            "INFO:root:Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.022533\n",
            "INFO:root:Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.003594\n",
            "INFO:root:Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.004095\n",
            "INFO:root:Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.071689\n",
            "INFO:root:Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.004864\n",
            "INFO:root:Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.023986\n",
            "INFO:root:Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.015626\n",
            "INFO:root:Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.005990\n",
            "INFO:root:Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.002313\n",
            "INFO:root:Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.171575\n",
            "INFO:root:Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.045232\n",
            "INFO:root:Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.003705\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0290, Accuracy: 9913/10000 (99%)\n",
            "\n",
            "INFO:root:Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.038139\n",
            "INFO:root:Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.022195\n",
            "INFO:root:Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.025368\n",
            "INFO:root:Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.085278\n",
            "INFO:root:Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.041128\n",
            "INFO:root:Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.022842\n",
            "INFO:root:Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.102433\n",
            "INFO:root:Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.005435\n",
            "INFO:root:Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.042560\n",
            "INFO:root:Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.014028\n",
            "INFO:root:Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.002290\n",
            "INFO:root:Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.004084\n",
            "INFO:root:Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.008759\n",
            "INFO:root:Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.024670\n",
            "INFO:root:Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.032007\n",
            "INFO:root:Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.022729\n",
            "INFO:root:Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.023973\n",
            "INFO:root:Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.011820\n",
            "INFO:root:Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.012459\n",
            "INFO:root:Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.001956\n",
            "INFO:root:Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.044224\n",
            "INFO:root:Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.014765\n",
            "INFO:root:Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.002387\n",
            "INFO:root:Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.031738\n",
            "INFO:root:Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.008010\n",
            "INFO:root:Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.007590\n",
            "INFO:root:Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.006750\n",
            "INFO:root:Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.033262\n",
            "INFO:root:Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.017441\n",
            "INFO:root:Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.001532\n",
            "INFO:root:Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.006560\n",
            "INFO:root:Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.018870\n",
            "INFO:root:Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.120028\n",
            "INFO:root:Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.003265\n",
            "INFO:root:Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.132133\n",
            "INFO:root:Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.052111\n",
            "INFO:root:Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.032544\n",
            "INFO:root:Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.045941\n",
            "INFO:root:Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.007379\n",
            "INFO:root:Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.013973\n",
            "INFO:root:Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.033848\n",
            "INFO:root:Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.061944\n",
            "INFO:root:Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.002732\n",
            "INFO:root:Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.056832\n",
            "INFO:root:Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.022834\n",
            "INFO:root:Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.025416\n",
            "INFO:root:Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.047840\n",
            "INFO:root:Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.020181\n",
            "INFO:root:Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.002356\n",
            "INFO:root:Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.094188\n",
            "INFO:root:Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.002763\n",
            "INFO:root:Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.006482\n",
            "INFO:root:Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.021149\n",
            "INFO:root:Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.016567\n",
            "INFO:root:Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.048002\n",
            "INFO:root:Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.018416\n",
            "INFO:root:Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.043755\n",
            "INFO:root:Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.013938\n",
            "INFO:root:Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.040407\n",
            "INFO:root:Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.002559\n",
            "INFO:root:Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.000837\n",
            "INFO:root:Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.019710\n",
            "INFO:root:Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.014718\n",
            "INFO:root:Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.014279\n",
            "INFO:root:Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.016644\n",
            "INFO:root:Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.001828\n",
            "INFO:root:Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.008179\n",
            "INFO:root:Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.058660\n",
            "INFO:root:Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.101510\n",
            "INFO:root:Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.009427\n",
            "INFO:root:Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.004884\n",
            "INFO:root:Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.010115\n",
            "INFO:root:Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.005982\n",
            "INFO:root:Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.003896\n",
            "INFO:root:Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.020702\n",
            "INFO:root:Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.013671\n",
            "INFO:root:Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.041246\n",
            "INFO:root:Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.067339\n",
            "INFO:root:Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.024981\n",
            "INFO:root:Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.018426\n",
            "INFO:root:Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.007704\n",
            "INFO:root:Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.011957\n",
            "INFO:root:Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.005966\n",
            "INFO:root:Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.070929\n",
            "INFO:root:Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.019736\n",
            "INFO:root:Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.008616\n",
            "INFO:root:Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.005828\n",
            "INFO:root:Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.028166\n",
            "INFO:root:Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.025844\n",
            "INFO:root:Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.006975\n",
            "INFO:root:Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.066819\n",
            "INFO:root:Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.056160\n",
            "INFO:root:Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.040100\n",
            "INFO:root:Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.021695\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0268, Accuracy: 9912/10000 (99%)\n",
            "\n",
            "INFO:root:Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.034844\n",
            "INFO:root:Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.001657\n",
            "INFO:root:Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.054348\n",
            "INFO:root:Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.026059\n",
            "INFO:root:Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.011489\n",
            "INFO:root:Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.125189\n",
            "INFO:root:Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.068329\n",
            "INFO:root:Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.133823\n",
            "INFO:root:Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.052070\n",
            "INFO:root:Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.010997\n",
            "INFO:root:Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.010816\n",
            "INFO:root:Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.002448\n",
            "INFO:root:Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.007435\n",
            "INFO:root:Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.008348\n",
            "INFO:root:Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.013359\n",
            "INFO:root:Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.019789\n",
            "INFO:root:Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.018378\n",
            "INFO:root:Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.001372\n",
            "INFO:root:Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.035093\n",
            "INFO:root:Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.016444\n",
            "INFO:root:Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.043048\n",
            "INFO:root:Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.075720\n",
            "INFO:root:Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.005250\n",
            "INFO:root:Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.026177\n",
            "INFO:root:Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.001062\n",
            "INFO:root:Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.013717\n",
            "INFO:root:Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.074138\n",
            "INFO:root:Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.037671\n",
            "INFO:root:Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.004511\n",
            "INFO:root:Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.028881\n",
            "INFO:root:Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.013410\n",
            "INFO:root:Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.009392\n",
            "INFO:root:Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.044630\n",
            "INFO:root:Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.036587\n",
            "INFO:root:Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.069409\n",
            "INFO:root:Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.072328\n",
            "INFO:root:Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.077008\n",
            "INFO:root:Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.001616\n",
            "INFO:root:Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.032067\n",
            "INFO:root:Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.045666\n",
            "INFO:root:Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.069225\n",
            "INFO:root:Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.021677\n",
            "INFO:root:Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.013539\n",
            "INFO:root:Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.067139\n",
            "INFO:root:Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.090478\n",
            "INFO:root:Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.029259\n",
            "INFO:root:Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.010314\n",
            "INFO:root:Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.001617\n",
            "INFO:root:Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.009495\n",
            "INFO:root:Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.049683\n",
            "INFO:root:Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.040940\n",
            "INFO:root:Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.013072\n",
            "INFO:root:Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.101774\n",
            "INFO:root:Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.002972\n",
            "INFO:root:Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.073440\n",
            "INFO:root:Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.007240\n",
            "INFO:root:Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.003990\n",
            "INFO:root:Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.023263\n",
            "INFO:root:Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.032189\n",
            "INFO:root:Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.000981\n",
            "INFO:root:Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.006318\n",
            "INFO:root:Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.007765\n",
            "INFO:root:Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.006552\n",
            "INFO:root:Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.156867\n",
            "INFO:root:Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.062342\n",
            "INFO:root:Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.000994\n",
            "INFO:root:Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.032668\n",
            "INFO:root:Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.040017\n",
            "INFO:root:Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.005023\n",
            "INFO:root:Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.004647\n",
            "INFO:root:Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.072238\n",
            "INFO:root:Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.002575\n",
            "INFO:root:Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.006750\n",
            "INFO:root:Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.004507\n",
            "INFO:root:Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.015156\n",
            "INFO:root:Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.145908\n",
            "INFO:root:Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.057963\n",
            "INFO:root:Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.046879\n",
            "INFO:root:Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.004916\n",
            "INFO:root:Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.047428\n",
            "INFO:root:Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.015005\n",
            "INFO:root:Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.048566\n",
            "INFO:root:Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.003664\n",
            "INFO:root:Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.019534\n",
            "INFO:root:Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.022099\n",
            "INFO:root:Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.116940\n",
            "INFO:root:Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.004859\n",
            "INFO:root:Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.009258\n",
            "INFO:root:Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.016629\n",
            "INFO:root:Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.150932\n",
            "INFO:root:Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.004627\n",
            "INFO:root:Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.030798\n",
            "INFO:root:Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.017675\n",
            "INFO:root:Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.036634\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0272, Accuracy: 9915/10000 (99%)\n",
            "\n",
            "INFO:root:Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.116936\n",
            "INFO:root:Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.007559\n",
            "INFO:root:Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.009629\n",
            "INFO:root:Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.005497\n",
            "INFO:root:Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.012460\n",
            "INFO:root:Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.010749\n",
            "INFO:root:Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.006044\n",
            "INFO:root:Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.061415\n",
            "INFO:root:Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.008715\n",
            "INFO:root:Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.021669\n",
            "INFO:root:Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.057032\n",
            "INFO:root:Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.002893\n",
            "INFO:root:Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.001200\n",
            "INFO:root:Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.001567\n",
            "INFO:root:Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.010144\n",
            "INFO:root:Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.010042\n",
            "INFO:root:Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.008013\n",
            "INFO:root:Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.009124\n",
            "INFO:root:Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.014439\n",
            "INFO:root:Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.010268\n",
            "INFO:root:Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.004346\n",
            "INFO:root:Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.035792\n",
            "INFO:root:Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.001705\n",
            "INFO:root:Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.014799\n",
            "INFO:root:Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.004298\n",
            "INFO:root:Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.014358\n",
            "INFO:root:Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.001619\n",
            "INFO:root:Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.009823\n",
            "INFO:root:Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.002595\n",
            "INFO:root:Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.030511\n",
            "INFO:root:Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.004588\n",
            "INFO:root:Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.017374\n",
            "INFO:root:Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.107542\n",
            "INFO:root:Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.046168\n",
            "INFO:root:Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.006179\n",
            "INFO:root:Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.011497\n",
            "INFO:root:Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.010929\n",
            "INFO:root:Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.009989\n",
            "INFO:root:Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.073550\n",
            "INFO:root:Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.019099\n",
            "INFO:root:Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.052836\n",
            "INFO:root:Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.018658\n",
            "INFO:root:Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.001994\n",
            "INFO:root:Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.058865\n",
            "INFO:root:Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.005451\n",
            "INFO:root:Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.005477\n",
            "INFO:root:Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.011471\n",
            "INFO:root:Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.036101\n",
            "INFO:root:Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.075201\n",
            "INFO:root:Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.011303\n",
            "INFO:root:Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.001717\n",
            "INFO:root:Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.037365\n",
            "INFO:root:Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.056980\n",
            "INFO:root:Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.058670\n",
            "INFO:root:Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.003450\n",
            "INFO:root:Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.019426\n",
            "INFO:root:Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.015985\n",
            "INFO:root:Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.071772\n",
            "INFO:root:Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.041785\n",
            "INFO:root:Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.020292\n",
            "INFO:root:Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.004382\n",
            "INFO:root:Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.015556\n",
            "INFO:root:Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.001025\n",
            "INFO:root:Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.052154\n",
            "INFO:root:Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.024218\n",
            "INFO:root:Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.001699\n",
            "INFO:root:Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.008647\n",
            "INFO:root:Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.053909\n",
            "INFO:root:Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.021709\n",
            "INFO:root:Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.009227\n",
            "INFO:root:Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.050498\n",
            "INFO:root:Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.009874\n",
            "INFO:root:Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.021832\n",
            "INFO:root:Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.006281\n",
            "INFO:root:Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.013107\n",
            "INFO:root:Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.009197\n",
            "INFO:root:Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.051459\n",
            "INFO:root:Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.016331\n",
            "INFO:root:Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.019132\n",
            "INFO:root:Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.015730\n",
            "INFO:root:Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.024838\n",
            "INFO:root:Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.052884\n",
            "INFO:root:Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.012408\n",
            "INFO:root:Train Epoch: 13 [53120/60000 (88%)]\tLoss: 0.036527\n",
            "INFO:root:Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.075447\n",
            "INFO:root:Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.017141\n",
            "INFO:root:Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.002883\n",
            "INFO:root:Train Epoch: 13 [55680/60000 (93%)]\tLoss: 0.074443\n",
            "INFO:root:Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.004567\n",
            "INFO:root:Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.008730\n",
            "INFO:root:Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.012831\n",
            "INFO:root:Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.042135\n",
            "INFO:root:Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.011320\n",
            "INFO:root:Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.000994\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0274, Accuracy: 9917/10000 (99%)\n",
            "\n",
            "INFO:root:Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.038572\n",
            "INFO:root:Train Epoch: 14 [640/60000 (1%)]\tLoss: 0.042486\n",
            "INFO:root:Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.011231\n",
            "INFO:root:Train Epoch: 14 [1920/60000 (3%)]\tLoss: 0.011179\n",
            "INFO:root:Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.000047\n",
            "INFO:root:Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.022821\n",
            "INFO:root:Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.007765\n",
            "INFO:root:Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.014894\n",
            "INFO:root:Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.064359\n",
            "INFO:root:Train Epoch: 14 [5760/60000 (10%)]\tLoss: 0.009842\n",
            "INFO:root:Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.025507\n",
            "INFO:root:Train Epoch: 14 [7040/60000 (12%)]\tLoss: 0.000834\n",
            "INFO:root:Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.005378\n",
            "INFO:root:Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.004110\n",
            "INFO:root:Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.012977\n",
            "INFO:root:Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.032581\n",
            "INFO:root:Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.017311\n",
            "INFO:root:Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.000542\n",
            "INFO:root:Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.021908\n",
            "INFO:root:Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.000986\n",
            "INFO:root:Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.020573\n",
            "INFO:root:Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.051225\n",
            "INFO:root:Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.001822\n",
            "INFO:root:Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.003412\n",
            "INFO:root:Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.029901\n",
            "INFO:root:Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.007969\n",
            "INFO:root:Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.005537\n",
            "INFO:root:Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.001233\n",
            "INFO:root:Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.007076\n",
            "INFO:root:Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.043104\n",
            "INFO:root:Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.022915\n",
            "INFO:root:Train Epoch: 14 [19840/60000 (33%)]\tLoss: 0.009219\n",
            "INFO:root:Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.160277\n",
            "INFO:root:Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.001444\n",
            "INFO:root:Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.019753\n",
            "INFO:root:Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.035635\n",
            "INFO:root:Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.005090\n",
            "INFO:root:Train Epoch: 14 [23680/60000 (39%)]\tLoss: 0.070156\n",
            "INFO:root:Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.001789\n",
            "INFO:root:Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.006107\n",
            "INFO:root:Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.024764\n",
            "INFO:root:Train Epoch: 14 [26240/60000 (44%)]\tLoss: 0.085396\n",
            "INFO:root:Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.019634\n",
            "INFO:root:Train Epoch: 14 [27520/60000 (46%)]\tLoss: 0.151575\n",
            "INFO:root:Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.094492\n",
            "INFO:root:Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.029705\n",
            "INFO:root:Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.013791\n",
            "INFO:root:Train Epoch: 14 [30080/60000 (50%)]\tLoss: 0.033576\n",
            "INFO:root:Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.005665\n",
            "INFO:root:Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.004230\n",
            "INFO:root:Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.002730\n",
            "INFO:root:Train Epoch: 14 [32640/60000 (54%)]\tLoss: 0.012036\n",
            "INFO:root:Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.059564\n",
            "INFO:root:Train Epoch: 14 [33920/60000 (57%)]\tLoss: 0.039845\n",
            "INFO:root:Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.076367\n",
            "INFO:root:Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.006352\n",
            "INFO:root:Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.016696\n",
            "INFO:root:Train Epoch: 14 [36480/60000 (61%)]\tLoss: 0.076248\n",
            "INFO:root:Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.043009\n",
            "INFO:root:Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.001490\n",
            "INFO:root:Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.001541\n",
            "INFO:root:Train Epoch: 14 [39040/60000 (65%)]\tLoss: 0.012535\n",
            "INFO:root:Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.004952\n",
            "INFO:root:Train Epoch: 14 [40320/60000 (67%)]\tLoss: 0.040791\n",
            "INFO:root:Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.001510\n",
            "INFO:root:Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.001427\n",
            "INFO:root:Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.004835\n",
            "INFO:root:Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.014662\n",
            "INFO:root:Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.011819\n",
            "INFO:root:Train Epoch: 14 [44160/60000 (74%)]\tLoss: 0.003953\n",
            "INFO:root:Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.003315\n",
            "INFO:root:Train Epoch: 14 [45440/60000 (76%)]\tLoss: 0.016348\n",
            "INFO:root:Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.010725\n",
            "INFO:root:Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.004555\n",
            "INFO:root:Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.036122\n",
            "INFO:root:Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.007536\n",
            "INFO:root:Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.110877\n",
            "INFO:root:Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.024571\n",
            "INFO:root:Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.012689\n",
            "INFO:root:Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.005637\n",
            "INFO:root:Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.021183\n",
            "INFO:root:Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.001551\n",
            "INFO:root:Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.006202\n",
            "INFO:root:Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.015932\n",
            "INFO:root:Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.012454\n",
            "INFO:root:Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.026365\n",
            "INFO:root:Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.002554\n",
            "INFO:root:Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.017252\n",
            "INFO:root:Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.055538\n",
            "INFO:root:Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.002616\n",
            "INFO:root:Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.001393\n",
            "INFO:root:Train Epoch: 14 [58240/60000 (97%)]\tLoss: 0.125593\n",
            "INFO:root:Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.003697\n",
            "INFO:root:Train Epoch: 14 [59520/60000 (99%)]\tLoss: 0.000715\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0292, Accuracy: 9912/10000 (99%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 1240.330137014389\n",
            "Current quantization method: sign\n",
            "Learning rate: 0.1\n",
            "Loss: [0.056386670684814455, 0.04312968482971191, 0.056067458343505856, 0.030958666706085203, 0.03177433080673218, 0.028094346237182617, 0.031341452980041505, 0.028461867237091064, 0.02642816677093506, 0.029019357299804687, 0.026843009376525877, 0.02719264364242554, 0.027368042373657225, 0.0291835636138916]\n",
            "Accuracy: [98.16, 98.48, 98.05, 98.98, 98.94, 99.06, 99.03, 99.02, 99.11, 99.13, 99.12, 99.15, 99.17, 99.12]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "cc_list=[]\n",
        "loss_list=[]\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import logging\n",
        "import bagua.torch_api as bagua\n",
        "import bagua_core \n",
        "import time\n",
        "\n",
        "bagua_core.install_deps()\n",
        "\n",
        "\n",
        "# Model for Neural Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "#------Training Model\n",
        "def train(args, model, train_loader, optimizer, epoch):\n",
        "    #??????? What does model.train() do exactly? Sets mode. \n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        #data: features , target: label\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        #Gradients are set back to zero here to avoid gradient accumulation\n",
        "        optimizer.zero_grad()\n",
        "        # Calculates predicted labels by using the model\n",
        "        output = model(data)\n",
        "        # Loss function using predicted labels and actual labels\n",
        "        loss = F.nll_loss(output, target)\n",
        "        # Backwards propagation    !!!calculates tensor loss gradient \n",
        "        loss.backward()\n",
        "        # Optimizer step selection\n",
        "        if args.fuse_optimizer:\n",
        "            optimizer.fuse_step()\n",
        "        else:\n",
        "            optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            logging.info(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(\n",
        "                output, target, reduction=\"sum\"\n",
        "            ).item()  # sum up batch loss\n",
        "            pred = output.argmax(\n",
        "                dim=1, keepdim=True\n",
        "            )  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    logging.info(\n",
        "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss,\n",
        "            correct,\n",
        "            len(test_loader.dataset),\n",
        "            100.0 * correct / len(test_loader.dataset),\n",
        "        )\n",
        "    )\n",
        "    return test_loss,correct\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description=\"PyTorch MNIST Example\")\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=64,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size for training (default: 64)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--test-batch-size\",\n",
        "        type=int,\n",
        "        default=1000,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size for testing (default: 1000)\",\n",
        "    )\n",
        "\n",
        "# set number of epochs here\n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        type=int,\n",
        "        default=14,\n",
        "        metavar=\"N\",\n",
        "        help=\"number of epochs to train (default: 14)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lr\",\n",
        "        type=float,\n",
        "        default=1.0,\n",
        "        metavar=\"LR\",\n",
        "        help=\"learning rate (default: 1.0)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gamma\",\n",
        "        type=float,\n",
        "        default=0.7,\n",
        "        metavar=\"M\",\n",
        "        help=\"Learning rate step gamma (default: 0.7)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--log-interval\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"N\",\n",
        "        help=\"how many batches to wait before logging training status\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--save-model\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"For Saving the current Model\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--algorithm\",\n",
        "        type=str,\n",
        "        default=\"qsparselocal\",\n",
        "        help=\"gradient_allreduce, bytegrad, decentralized, low_precision_decentralized, qadam, async\",\n",
        "        #Add new algorithm for testing------------------\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--async-sync-interval\",\n",
        "        default=500,\n",
        "        type=int,\n",
        "        help=\"Model synchronization interval(ms) for async algorithm\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--set-deterministic\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"set deterministic or not\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fuse-optimizer\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"fuse optimizer or not\",\n",
        "    )\n",
        "\n",
        "    #args = parser.parse_args() \n",
        "    # New line below solves ipykernel_launcher.py: error: unrecognized arguments\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    \n",
        "    if args.set_deterministic:\n",
        "        print(\"set_deterministic: True\")\n",
        "        np.random.seed(666)\n",
        "        random.seed(666)\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.manual_seed(666)\n",
        "        torch.cuda.manual_seed_all(666 + int(bagua.get_rank()))\n",
        "        torch.set_printoptions(precision=10)\n",
        "\n",
        "    torch.cuda.set_device(bagua.get_local_rank())\n",
        "    bagua.init_process_group()\n",
        "\n",
        "\n",
        "\n",
        "    logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.ERROR)\n",
        "    if bagua.get_rank() == 0:\n",
        "        logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    train_kwargs = {\"batch_size\": args.batch_size}\n",
        "    test_kwargs = {\"batch_size\": args.test_batch_size}\n",
        "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        "    )\n",
        "\n",
        "    if bagua.get_local_rank() == 0:\n",
        "        dataset1 = datasets.MNIST(\n",
        "            \"../data\", train=True, download=True, transform=transform\n",
        "        )\n",
        "        torch.distributed.barrier()\n",
        "    else:\n",
        "        torch.distributed.barrier()\n",
        "        dataset1 = datasets.MNIST(\n",
        "            \"../data\", train=True, download=True, transform=transform\n",
        "        )\n",
        "\n",
        "    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        dataset1, num_replicas=bagua.get_world_size(), rank=bagua.get_rank()\n",
        "    )\n",
        "    train_kwargs.update(\n",
        "        {\n",
        "            \"sampler\": train_sampler,\n",
        "            \"batch_size\": args.batch_size // bagua.get_world_size(),\n",
        "            \"shuffle\": False,\n",
        "        }\n",
        "    )\n",
        "    # Train and Test dataset\n",
        "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "    # ??????????  Throw the instantiation of the network onto the cuda dvice\n",
        "    model = Net().cuda()\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "    if args.algorithm == \"gradient_allreduce\":\n",
        "        from bagua.torch_api.algorithms import gradient_allreduce\n",
        "\n",
        "        algorithm = gradient_allreduce.GradientAllReduceAlgorithm()\n",
        "    elif args.algorithm == \"decentralized\":\n",
        "        from bagua.torch_api.algorithms import decentralized\n",
        "\n",
        "        algorithm = decentralized.DecentralizedAlgorithm()\n",
        "    elif args.algorithm == \"low_precision_decentralized\":\n",
        "        from bagua.torch_api.algorithms import decentralized\n",
        "\n",
        "        algorithm = decentralized.LowPrecisionDecentralizedAlgorithm()\n",
        "    elif args.algorithm == \"bytegrad\":\n",
        "        from bagua.torch_api.algorithms import bytegrad\n",
        "\n",
        "        algorithm = bytegrad.ByteGradAlgorithm()\n",
        "    elif args.algorithm == \"qadam\":\n",
        "        from bagua.torch_api.algorithms import q_adam\n",
        "\n",
        "        optimizer = q_adam.QAdamOptimizer(\n",
        "            model.parameters(), lr=args.lr, warmup_steps=100\n",
        "        )\n",
        "        algorithm = q_adam.QAdamAlgorithm(optimizer)\n",
        "    #################################################################    \n",
        "    elif args.algorithm == \"qsparselocal\":\n",
        "        learning_rate = 0.1\n",
        "        # Set lower learning rate, no convergence for lr = 1\n",
        "        optimizer = QSparseLocalOptimizer(\n",
        "            model.parameters(), lr=learning_rate, schedule = 1\n",
        "        )\n",
        "        algorithm = QSparseLocalAlgorithm(optimizer)\n",
        "    elif args.algorithm == \"async\":\n",
        "        from bagua.torch_api.algorithms import async_model_average\n",
        "\n",
        "        algorithm = async_model_average.AsyncModelAverageAlgorithm(\n",
        "            sync_interval_ms=args.async_sync_interval,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    #  Model von Bagua\n",
        "    model = model.with_bagua(\n",
        "        [optimizer],\n",
        "        algorithm,\n",
        "        do_flatten=not args.fuse_optimizer,\n",
        "    )\n",
        "\n",
        "    # Optimizer from Bagua if args.fuse_optimizer==True\n",
        "    if args.fuse_optimizer:\n",
        "        optimizer = bagua.contrib.fuse_optimizer(optimizer)\n",
        "\n",
        "    #------------ Loss, accuracy\n",
        "    loss_list =[]\n",
        "    acc_list = []\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "    \n",
        "    start = time.time()\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        if args.algorithm == \"async\":\n",
        "            model.bagua_algorithm.resume(model)\n",
        "\n",
        "        train(args, model, train_loader, optimizer, epoch)\n",
        "\n",
        "        if args.algorithm == \"async\":\n",
        "            model.bagua_algorithm.abort(model)\n",
        "\n",
        "        new_loss,new_acc =test(model, test_loader)\n",
        "        loss_list.append(new_loss)\n",
        "        acc_list.append(new_acc/100.0)\n",
        "        scheduler.step()\n",
        "        ####\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if args.save_model:\n",
        "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "    # Used for measuring the time taken for the epochs themselves\n",
        "    end = time.time()\n",
        "    print(\"Elapsed time:\",end-start)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    ep =[]\n",
        "\n",
        "\n",
        "    for i in range(1, args.epochs + 1):\n",
        "      ep.append(i)\n",
        "\n",
        "    print(\"Current quantization method:\",quantization_scheme)\n",
        "    print(\"Learning rate:\",learning_rate)\n",
        "    print(\"Loss:\",loss_list)\n",
        "    print(\"Accuracy:\",acc_list)\n",
        "    \n",
        "     \n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.subplot(211)\n",
        "    plt.plot(ep,loss_list)\n",
        "    plt.subplot(212)\n",
        "    plt.plot(ep,acc_list)\n",
        "\n",
        "    plt.show()  \n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rSo-g6qDIvE",
        "outputId": "42b5d9ea-94ec-459f-d993-57d5f729f603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No traceback available to show.\n"
          ]
        }
      ],
      "source": [
        "%tb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "QSparseLocal_MNIST_4_5_slow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPcV0ehw2U7nfwze+KgUPgo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}