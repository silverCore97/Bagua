{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverCore97/Bagua/blob/main/QSparseLocal_MNIST_5_5_comp_init_op.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FidgI73Gc8m7"
      },
      "source": [
        "##**Check CUDA version**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biBKGSXi6Chp",
        "outputId": "c8557b59-7643-4688-afa9-f81f8621456a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1UzpVrNc3wO"
      },
      "source": [
        "##**Install Bagua**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qCGqkkr5mr4",
        "outputId": "360bb357-3926-4b48-de68-ad4c45dbbae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bagua-cuda111 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: scikit-learn!=1.0,<=1.0.1,>=0.24 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.0.1)\n",
            "Requirement already satisfied: pydantic>=1.8 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.9.0)\n",
            "Requirement already satisfied: pytest-benchmark>=3.4 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (3.4.1)\n",
            "Requirement already satisfied: setuptools-rust in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.3.0)\n",
            "Requirement already satisfied: flask>=2.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.1.2)\n",
            "Requirement already satisfied: gorilla==0.4.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.4.0)\n",
            "Requirement already satisfied: scikit-optimize>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.9.0)\n",
            "Requirement already satisfied: xxhash>=2.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (3.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (4.64.0)\n",
            "Requirement already satisfied: prometheus-client>=0.11 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.14.1)\n",
            "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.27.1)\n",
            "Requirement already satisfied: gevent>=21.8 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (21.12.0)\n",
            "Requirement already satisfied: parallel-ssh==2.8.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.8.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.4.4)\n",
            "Requirement already satisfied: deprecation>=2.1 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.21.6)\n",
            "Requirement already satisfied: ssh-python>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from parallel-ssh==2.8.0->bagua-cuda111) (0.10.0)\n",
            "Requirement already satisfied: ssh2-python>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from parallel-ssh==2.8.0->bagua-cuda111) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deprecation>=2.1->bagua-cuda111) (21.3)\n",
            "Requirement already satisfied: Werkzeug>=2.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (2.1.2)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (8.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (4.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (62.1.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (1.1.2)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (4.5.0)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6.0->flask>=2.0->bagua-cuda111) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6.0->flask>=2.0->bagua-cuda111) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=3.0->flask>=2.0->bagua-cuda111) (2.0.1)\n",
            "Requirement already satisfied: pytest>=3.8 in /usr/local/lib/python3.7/dist-packages (from pytest-benchmark>=3.4->bagua-cuda111) (7.1.2)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.7/dist-packages (from pytest-benchmark>=3.4->bagua-cuda111) (8.0.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (1.11.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (2.0.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (21.4.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (1.1.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (1.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2.10)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (3.1.0)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize>=0.8.1->bagua-cuda111) (21.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize>=0.8.1->bagua-cuda111) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deprecation>=2.1->bagua-cuda111) (3.0.8)\n",
            "Requirement already satisfied: semantic-version<3,>=2.8.2 in /usr/local/lib/python3.7/dist-packages (from setuptools-rust->bagua-cuda111) (2.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install bagua-cuda111"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IThXKxGEcv4z"
      },
      "source": [
        "## **Import environment variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ykSOeQsnTLCz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['RANK'] = '0'\n",
        "os.environ['WORLD_SIZE'] = '1'\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '29500'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M-JsrdWcjzj"
      },
      "source": [
        "## **QSparseLocal Algorithm Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_TpSXZaAd8r8"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from bagua.torch_api.bucket import BaguaBucket\n",
        "from bagua.torch_api.tensor import BaguaTensor\n",
        "from bagua.torch_api.data_parallel.bagua_distributed import BaguaDistributedDataParallel\n",
        "from bagua.torch_api.algorithms import Algorithm, AlgorithmImpl\n",
        "from bagua.torch_api.communication import BaguaProcessGroup\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "sparsify = True\n",
        "use_memory = True\n",
        "quantization_scheme = 'sign'\n",
        "quantization_levels = 256\n",
        "top_k_sparsification = True\n",
        "k = 1000\n",
        "use_normalization = True\n",
        "\n",
        "\n",
        "## input: Uncompressed Gradient tensor\n",
        "## Output: Quantized and sparsified Gradient tensor\n",
        "def qsl(eta_grad,\n",
        "        memory,\n",
        "        qsl_grad,   # No output, instead change in function\n",
        "        topK_flag,\n",
        "        s,\n",
        "        # sparsify,\n",
        "        # use_memory,\n",
        "        # quantization_scheme,\n",
        "        # use_normalization\n",
        "        ):\n",
        "    ###To do: Allow other quantization\n",
        "    def signq(var):\n",
        "        # Normalization according to input\n",
        "        # ||var||_1 * sign(var)/\n",
        "        one_norm = torch.norm(var, p=1)\n",
        "        return one_norm * torch.sign(var + 1e-13) / float(torch.numel(var))\n",
        "        # return torch.sign(var)  # Returns a new tensor with the signs of the elements of input\n",
        "\n",
        "    def qsgd(var):\n",
        "        level_float = s * torch.abs(var) / norm1\n",
        "        previous_level = torch.floor(level_float)\n",
        "        is_next_level = (torch.rand(var.size(), dtype=torch.float32, device = 'cuda') < (level_float - previous_level))\n",
        "        is_next_level = is_next_level.float()\n",
        "        new_level = previous_level + is_next_level\n",
        "        unnormalized = torch.sign(var) * new_level * norm1 / s\n",
        "        beta = float(torch.numel(var)) / float(s * s)\n",
        "        return unnormalized / (1.0 + beta) if use_normalization else unnormalized\n",
        "\n",
        "    def get_quantization(q):\n",
        "        if q == 'qsgd':\n",
        "            return qsgd\n",
        "        elif q == 'sign':\n",
        "            return signq\n",
        "        else:\n",
        "            return lambda x: x\n",
        "\n",
        "    if not sparsify:\n",
        "        norm1 = torch.norm(eta_grad) + torch.constant(1e-5, dtype=torch.float32)\n",
        "        if use_memory:\n",
        "            input = memory + eta_grad\n",
        "        else:\n",
        "            input = eta_grad\n",
        "\n",
        "        func = get_quantization(quantization_scheme)\n",
        "        q = func(input)\n",
        "\n",
        "        return q, input - q\n",
        "\n",
        "    input = memory + eta_grad\n",
        "\n",
        "    org_shape = input.size()\n",
        "    numel = torch.numel(input)\n",
        "    K = min(numel, k)  # k is the optimizer's k,\n",
        "    # K is the actual value used for sparsification\n",
        "\n",
        "    if topK_flag:\n",
        "        # Get values and index tensor of chosen components\n",
        "        # flat shape with absolute values\n",
        "        _, indices = torch.topk(torch.reshape(torch.abs(input), [-1]), K)\n",
        "    else:\n",
        "        indices = torch.from_numpy(np.random.choice(torch.range(numel), K, False))\n",
        "\n",
        "    # Flatten input\n",
        "    flat_input = torch.reshape(input, [-1])\n",
        "    values = torch.gather(flat_input, 0, indices)  # dim=0\n",
        "    norm1 = torch.norm(values)\n",
        "    quantization_func = get_quantization(quantization_scheme)\n",
        "    flattened_quantized = torch.zeros_like(flat_input).scatter(0, indices,\n",
        "                                                               quantization_func(values))\n",
        "    quantization = torch.reshape(flattened_quantized, shape=org_shape)\n",
        "\n",
        "    q_func = lambda: quantization\n",
        "    zero_tensor = lambda: torch.zeros_like(input, dtype=torch.float32)\n",
        "\n",
        "    # q = torch.where( float(0)<norm1, q_func, zero_tensor)    # Where not applicable for choosing functions\n",
        "    if float(0) < norm1:\n",
        "        q = q_func()\n",
        "    else:\n",
        "        q = zero_tensor()\n",
        "\n",
        "    err = input - q\n",
        "\n",
        "    memory.mul_(0).add_(err)\n",
        "    qsl_grad.mul_(0).add_(q)\n",
        "\n",
        "\n",
        "class QSparseLocalOptimizer(Optimizer):\n",
        "    def __init__(\n",
        "            self,\n",
        "            params,\n",
        "            lr: float = 1e-3,  ## Later step dependent learning rate\n",
        "            k: int = 1000,\n",
        "            schedule: int = 1\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create a dedicated optimizer used for\n",
        "        `QSparseLocal <https://tutorials.baguasys.com/algorithms/q-adam>`_ algorithm.\n",
        "\n",
        "        Args:\n",
        "            params (iterable): Iterable of parameters to optimize or dicts defining\n",
        "                parameter groups.\n",
        "            lr: Learning rate.\n",
        "            k: How many tensor components are kept during sparsification\n",
        "            schedule: Description of synchronization schedule\n",
        "        \"\"\"\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0 < schedule:\n",
        "            raise ValueError(\"Invalid schedule: {}\".format(lr))\n",
        "\n",
        "        defaults = dict(lr=lr, k=k, schedule=schedule)\n",
        "        super(QSparseLocalOptimizer, self).__init__(params, defaults)\n",
        "        # TODO: QSparseLocal optimizer maintain `step_id` in its state\n",
        "        self.step_id = 0\n",
        "        self.schedule = schedule\n",
        "        self.k = k\n",
        "        self.lr = lr\n",
        "        self.sync = False\n",
        "\n",
        "        # initialize global and local model, and memory for error compensation\n",
        "        for group_id, group in enumerate(self.param_groups):\n",
        "            params_with_grad = []\n",
        "            for p in group[\"params\"]:\n",
        "                params_with_grad.append(p)\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "\n",
        "                    state[\"global\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    # Set global to initialized value of local weights\n",
        "                    state[\"global\"].add_(p.data)\n",
        "\n",
        "                    state[\"memory\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "\n",
        "                    state[\"qsl_grad\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "\n",
        "                    \"\"\"\n",
        "                    Local parameter vector inside p.data\n",
        "                    global: Global paramenter vector (same for all workers)\n",
        "                    error_comp: Term for error compensation for quantization and sparsification of gradient tensor\n",
        "                    qsl_grad: The quantized and sparsified gradient tensor to be sent to master (to allreduce)\n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(QSparseLocalOptimizer, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        # We start with step 0\n",
        "\n",
        "        \n",
        "\n",
        "        for group_id, group in enumerate(self.param_groups):\n",
        "            for param_id, param in enumerate(group[\"params\"]):\n",
        "                state = self.state[param]\n",
        "\n",
        "                #Calculate new global and local weights\n",
        "                if self.sync:\n",
        "                    # Adjust global and local weights after synchronzation\n",
        "                    state[\"global\"].add_(state[\"qsl_grad\"])\n",
        "                    param.data.mul_(0).add_(state[\"global\"])\n",
        "            \n",
        "                ##### Before allreduce operation\n",
        "                # Compute temporary local parameter vector\n",
        "                # Compute compressed gradient and new error compensation term\n",
        "                # Horovod: eta local-global\n",
        "                param.data.add_(param.grad,alpha=-self.lr) \n",
        "                if self.sync:\n",
        "                    # No output, new values assigned inside the function\n",
        "                    qsl(param.data - state[\"global\"], state[\"memory\"],state[\"qsl_grad\"],\n",
        "                                                topK_flag=top_k_sparsification, s=quantization_levels)\n",
        "                    \n",
        "        self.step_id += 1\n",
        "        # Now schedule defines the number of round per synchronization round\n",
        "        self.sync = self.step_id % self.schedule == 0\n",
        "                    \n",
        "                \n",
        "\n",
        "class QSparseLocalAlgorithmImpl(AlgorithmImpl):\n",
        "    def __init__(\n",
        "            self,\n",
        "            process_group: BaguaProcessGroup,\n",
        "            q_sparse_local_optimizer: QSparseLocalOptimizer,\n",
        "            hierarchical: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implementation of the\n",
        "        `QSparseLocal Algorithm <https://tutorials.baguasys.com/algorithms/q-adam>`_\n",
        "        .\n",
        "        Args:\n",
        "            process_group: The process group to work on.\n",
        "            q_sparse_local_optimizer: A QSparseLocalOptimizer initialized with model parameters.\n",
        "            hierarchical: Enable hierarchical communication.\n",
        "        \"\"\"\n",
        "        super(QSparseLocalAlgorithmImpl, self).__init__(process_group)\n",
        "        self.hierarchical = hierarchical\n",
        "        self.optimizer = q_sparse_local_optimizer\n",
        "        self.sync = self.optimizer.sync\n",
        "\n",
        "    # Needed to switch between synchronization aand local rounds\n",
        "    def need_reset(self):\n",
        "        return True\n",
        "\n",
        "    def init_tensors(self, bagua_distributed_data_parallel: BaguaDistributedDataParallel):\n",
        "        parameters = bagua_distributed_data_parallel.bagua_build_params()\n",
        "\n",
        "        for idx, (name, param) in enumerate(parameters.__reversed__()):\n",
        "            param._q_sparse_local_name = name\n",
        "            param._q_sparse_local_idx = idx\n",
        "\n",
        "        tensor_groups = []\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for param in group[\"params\"]:\n",
        "                #if self.sync:\n",
        "\n",
        "                # Second half Step 4\n",
        "                def set_weights(param, t):\n",
        "                    # Set compressed gradient to mean of all workers compressed gradients\n",
        "                    self.optimizer.state[param][\"qsl_grad\"] = t\n",
        "\n",
        "                registered_tensor = param.bagua_ensure_grad().ensure_bagua_tensor(\n",
        "                    param._q_sparse_local_name,\n",
        "                    bagua_distributed_data_parallel.bagua_module_name,\n",
        "                    getter_closure=lambda param: self.optimizer.state[param][\"qsl_grad\"],\n",
        "                    setter_closure=set_weights,\n",
        "                )\n",
        "                tensor_groups.append(registered_tensor)\n",
        "                #else:\n",
        "                    # Nothing happens\n",
        "                    #pass\n",
        "\n",
        "        tensor_groups.sort(key=lambda x: x._q_sparse_local_idx)\n",
        "        return tensor_groups\n",
        "\n",
        "    def tensors_to_buckets(\n",
        "            self, tensors: List[List[BaguaTensor]], do_flatten: bool\n",
        "    ) -> List[BaguaBucket]:\n",
        "        bagua_buckets = []\n",
        "\n",
        "        for idx, bucket in enumerate(tensors):\n",
        "            bagua_bucket = BaguaBucket(\n",
        "                bucket,\n",
        "                flatten=do_flatten,\n",
        "                # flatten=True,\n",
        "                name=str(idx),\n",
        "                alignment=self.process_group.get_global_communicator().nranks(),\n",
        "            )\n",
        "            bagua_buckets.append(bagua_bucket)\n",
        "        return bagua_buckets\n",
        "\n",
        "    def init_operations(\n",
        "            self,\n",
        "            bagua_distributed_data_parallel: BaguaDistributedDataParallel,\n",
        "            bucket: BaguaBucket,\n",
        "    ):\n",
        "        bucket.clear_ops()\n",
        "        \n",
        "        # For synchronization round we utilize allreduce, else no synchronization takes place\n",
        "        if self.sync:\n",
        "            def preprocess():\n",
        "              for group_id, group in enumerate(self.optimizer.param_groups):\n",
        "                for param_id, param in enumerate(group[\"params\"]):\n",
        "                    state = self.optimizer.state[param]\n",
        "\n",
        "                    ##### Before allreduce operation\n",
        "                    # Compute temporary local parameter vector\n",
        "                    # Compute compressed gradient and new error compensation term\n",
        "                    # Horovod: eta local-global\n",
        "                    param.data.add_(param.grad,alpha=-self.optimizer.lr) \n",
        "                    if self.optimizer.sync:\n",
        "                        # No output, new values assigned inside the function\n",
        "                        qsl(param.data - state[\"global\"], state[\"memory\"],state[\"qsl_grad\"],\n",
        "                                                    topK_flag=top_k_sparsification, s=quantization_levels)\n",
        "\n",
        "            bucket.append_python_op(preprocess, group=self.process_group)\n",
        "\n",
        "            # Compression is done by Bagua\n",
        "            bucket.append_centralized_synchronous_op(\n",
        "                hierarchical=self.hierarchical,\n",
        "                average=True,  # Maybe try false and then average it manually to preserve ints\n",
        "                scattergather=True,\n",
        "                compression=\"MinMaxUInt8\",    \n",
        "                group=self.process_group,\n",
        "            )\n",
        "        else:  # Nothing happens\n",
        "            pass\n",
        "\n",
        "    # Instead of momentum hook, we use a qsl_gradient hook\n",
        "    def init_backward_hook(self, bagua_distributed_data_parallel: BaguaDistributedDataParallel):\n",
        "\n",
        "        def hook_qsl_grad(parameter_name, parameter):\n",
        "            assert (\n",
        "                    parameter.bagua_backend_tensor().data_ptr()\n",
        "                    == self.optimizer.state[parameter][\"qsl_grad\"].data_ptr()\n",
        "            ), \"bagua backend tensor data_ptr should match _q_sparse_local_grad data_ptr\"\n",
        "            parameter.bagua_mark_communication_ready()\n",
        "\n",
        "        return hook_qsl_grad\n",
        "\n",
        "\n",
        "class QSparseLocalAlgorithm(Algorithm):\n",
        "    def __init__(self, q_sparse_local_optimizer: QSparseLocalOptimizer, hierarchical: bool = True):\n",
        "        \"\"\"\n",
        "        Create an instance of the `QSparseLocal Algorithm' .\n",
        "\n",
        "        Args:\n",
        "            q_sparse_local_optimizer: A QSparseLocalOptimizer initialized with model parameters.\n",
        "            hierarchical: Enable hierarchical communication.\n",
        "        \"\"\"\n",
        "        self.hierarchical = hierarchical\n",
        "        self.optimizer = q_sparse_local_optimizer\n",
        "\n",
        "    def reify(self, process_group: BaguaProcessGroup) -> QSparseLocalAlgorithmImpl:\n",
        "        return QSparseLocalAlgorithmImpl(\n",
        "            process_group,\n",
        "            q_sparse_local_optimizer=self.optimizer,\n",
        "            hierarchical=self.hierarchical,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPIWQ8CvcZ0G"
      },
      "source": [
        "## **MNIST example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tjsC05-5qV_",
        "outputId": "fb4df104-a4de-453e-faff-7ea776f24364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc_version:  11.1\n",
            "The destination directory /root/.local/share/bagua/nccl already exists.\n",
            "Installing nccl 2.10.3 for CUDA 11.1 to: /root/.local/share/bagua/nccl\n",
            "Downloading https://developer.download.nvidia.com/compute/redist/nccl/v2.10/nccl_2.10.3-1+cuda11.0_x86_64.txz...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nccl_2.10.3-1+cuda11.0_x86_64.txz: 138MB [00:01, 118MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting...\n",
            "Installing...\n",
            "Cleaning up...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gevent/threadpool.py\", line 157, in _before_run_task\n",
            "    _sys.settrace(_get_thread_trace())\n",
            "\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gevent/threadpool.py\", line 162, in _after_run_task\n",
            "    _sys.settrace(None)\n",
            "\n",
            "INFO:root:Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309355\n",
            "INFO:root:Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.130568\n",
            "INFO:root:Train Epoch: 1 [1280/60000 (2%)]\tLoss: 1.910435\n",
            "INFO:root:Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.223335\n",
            "INFO:root:Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.984603\n",
            "INFO:root:Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.779773\n",
            "INFO:root:Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.706712\n",
            "INFO:root:Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.776294\n",
            "INFO:root:Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.587909\n",
            "INFO:root:Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.513445\n",
            "INFO:root:Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.001026\n",
            "INFO:root:Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.473096\n",
            "INFO:root:Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.298485\n",
            "INFO:root:Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.400623\n",
            "INFO:root:Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.332192\n",
            "INFO:root:Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.396086\n",
            "INFO:root:Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.327504\n",
            "INFO:root:Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.168293\n",
            "INFO:root:Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.267772\n",
            "INFO:root:Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.169692\n",
            "INFO:root:Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.423701\n",
            "INFO:root:Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.279012\n",
            "INFO:root:Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.234941\n",
            "INFO:root:Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.200563\n",
            "INFO:root:Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.191491\n",
            "INFO:root:Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.218165\n",
            "INFO:root:Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.296903\n",
            "INFO:root:Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.203560\n",
            "INFO:root:Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.198916\n",
            "INFO:root:Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.293500\n",
            "INFO:root:Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.237515\n",
            "INFO:root:Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.272220\n",
            "INFO:root:Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.612677\n",
            "INFO:root:Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.290559\n",
            "INFO:root:Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.150251\n",
            "INFO:root:Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.253723\n",
            "INFO:root:Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.116731\n",
            "INFO:root:Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.426611\n",
            "INFO:root:Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.163938\n",
            "INFO:root:Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.184401\n",
            "INFO:root:Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.226186\n",
            "INFO:root:Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.315656\n",
            "INFO:root:Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.091545\n",
            "INFO:root:Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.290100\n",
            "INFO:root:Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.307751\n",
            "INFO:root:Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.194427\n",
            "INFO:root:Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.275696\n",
            "INFO:root:Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.206467\n",
            "INFO:root:Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.110127\n",
            "INFO:root:Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.149073\n",
            "INFO:root:Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.186172\n",
            "INFO:root:Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.193662\n",
            "INFO:root:Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.196972\n",
            "INFO:root:Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.119629\n",
            "INFO:root:Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.230463\n",
            "INFO:root:Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.192767\n",
            "INFO:root:Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.136804\n",
            "INFO:root:Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.201900\n",
            "INFO:root:Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.230419\n",
            "INFO:root:Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.156286\n",
            "INFO:root:Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.035371\n",
            "INFO:root:Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.098240\n",
            "INFO:root:Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.164435\n",
            "INFO:root:Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.372123\n",
            "INFO:root:Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.104823\n",
            "INFO:root:Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.113515\n",
            "INFO:root:Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.095629\n",
            "INFO:root:Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.212572\n",
            "INFO:root:Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.224447\n",
            "INFO:root:Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.057305\n",
            "INFO:root:Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.227913\n",
            "INFO:root:Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.059103\n",
            "INFO:root:Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.114527\n",
            "INFO:root:Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.110421\n",
            "INFO:root:Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.073032\n",
            "INFO:root:Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.087536\n",
            "INFO:root:Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.114835\n",
            "INFO:root:Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.187071\n",
            "INFO:root:Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.121278\n",
            "INFO:root:Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.300195\n",
            "INFO:root:Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.174603\n",
            "INFO:root:Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.157943\n",
            "INFO:root:Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.138976\n",
            "INFO:root:Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.213251\n",
            "INFO:root:Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.079781\n",
            "INFO:root:Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.169445\n",
            "INFO:root:Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.057877\n",
            "INFO:root:Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.266164\n",
            "INFO:root:Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.242486\n",
            "INFO:root:Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.147405\n",
            "INFO:root:Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.092796\n",
            "INFO:root:Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.334899\n",
            "INFO:root:Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.160790\n",
            "INFO:root:Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.137890\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0744, Accuracy: 9757/10000 (98%)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "INFO:root:Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.196148\n",
            "INFO:root:Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.082177\n",
            "INFO:root:Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.099716\n",
            "INFO:root:Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.197081\n",
            "INFO:root:Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.106582\n",
            "INFO:root:Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.209091\n",
            "INFO:root:Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.131809\n",
            "INFO:root:Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.061766\n",
            "INFO:root:Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.106786\n",
            "INFO:root:Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.034585\n",
            "INFO:root:Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.107087\n",
            "INFO:root:Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.069046\n",
            "INFO:root:Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.062905\n",
            "INFO:root:Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.050427\n",
            "INFO:root:Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.124385\n",
            "INFO:root:Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.124923\n",
            "INFO:root:Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.070766\n",
            "INFO:root:Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.071316\n",
            "INFO:root:Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.042513\n",
            "INFO:root:Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.089599\n",
            "INFO:root:Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.075604\n",
            "INFO:root:Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.107957\n",
            "INFO:root:Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.052986\n",
            "INFO:root:Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.071511\n",
            "INFO:root:Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.049068\n",
            "INFO:root:Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.071780\n",
            "INFO:root:Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.140473\n",
            "INFO:root:Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.068529\n",
            "INFO:root:Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.101887\n",
            "INFO:root:Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.101059\n",
            "INFO:root:Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.101625\n",
            "INFO:root:Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.119947\n",
            "INFO:root:Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.224795\n",
            "INFO:root:Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.053059\n",
            "INFO:root:Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.112452\n",
            "INFO:root:Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.092504\n",
            "INFO:root:Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.125680\n",
            "INFO:root:Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.080913\n",
            "INFO:root:Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.048864\n",
            "INFO:root:Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.057822\n",
            "INFO:root:Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.081320\n",
            "INFO:root:Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.288021\n",
            "INFO:root:Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.069312\n",
            "INFO:root:Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.112672\n",
            "INFO:root:Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.205565\n",
            "INFO:root:Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.069318\n",
            "INFO:root:Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.199361\n",
            "INFO:root:Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.030814\n",
            "INFO:root:Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.028902\n",
            "INFO:root:Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.102961\n",
            "INFO:root:Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.090314\n",
            "INFO:root:Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.066249\n",
            "INFO:root:Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.106208\n",
            "INFO:root:Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.045497\n",
            "INFO:root:Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.273042\n",
            "INFO:root:Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.118477\n",
            "INFO:root:Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.146454\n",
            "INFO:root:Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.149997\n",
            "INFO:root:Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.260787\n",
            "INFO:root:Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.113198\n",
            "INFO:root:Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.045953\n",
            "INFO:root:Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.130784\n",
            "INFO:root:Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.060023\n",
            "INFO:root:Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.118368\n",
            "INFO:root:Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.069347\n",
            "INFO:root:Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.038167\n",
            "INFO:root:Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.043992\n",
            "INFO:root:Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.088722\n",
            "INFO:root:Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.199046\n",
            "INFO:root:Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.043404\n",
            "INFO:root:Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.194505\n",
            "INFO:root:Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.055445\n",
            "INFO:root:Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.012307\n",
            "INFO:root:Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.064379\n",
            "INFO:root:Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.060814\n",
            "INFO:root:Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.029188\n",
            "INFO:root:Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.151456\n",
            "INFO:root:Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.067016\n",
            "INFO:root:Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.079632\n",
            "INFO:root:Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.177782\n",
            "INFO:root:Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.082349\n",
            "INFO:root:Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.073229\n",
            "INFO:root:Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.053158\n",
            "INFO:root:Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.150095\n",
            "INFO:root:Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.086340\n",
            "INFO:root:Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.137057\n",
            "INFO:root:Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.028629\n",
            "INFO:root:Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.166448\n"
          ]
        }
      ],
      "source": [
        "cc_list=[]\n",
        "loss_list=[]\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import logging\n",
        "import bagua.torch_api as bagua\n",
        "import bagua_core \n",
        "import time\n",
        "\n",
        "bagua_core.install_deps()\n",
        "\n",
        "\n",
        "# Model for Neural Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "#------Training Model\n",
        "def train(args, model, train_loader, optimizer, epoch):\n",
        "    #??????? What does model.train() do exactly? Sets mode. \n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        #data: features , target: label\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        #Gradients are set back to zero here to avoid gradient accumulation\n",
        "        optimizer.zero_grad()\n",
        "        # Calculates predicted labels by using the model\n",
        "        output = model(data)\n",
        "        # Loss function using predicted labels and actual labels\n",
        "        loss = F.nll_loss(output, target)\n",
        "        # Backwards propagation    !!!calculates tensor loss gradient \n",
        "        loss.backward()\n",
        "        # Optimizer step selection\n",
        "        if args.fuse_optimizer:\n",
        "            optimizer.fuse_step()\n",
        "        else:\n",
        "            optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            logging.info(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(\n",
        "                output, target, reduction=\"sum\"\n",
        "            ).item()  # sum up batch loss\n",
        "            pred = output.argmax(\n",
        "                dim=1, keepdim=True\n",
        "            )  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    logging.info(\n",
        "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss,\n",
        "            correct,\n",
        "            len(test_loader.dataset),\n",
        "            100.0 * correct / len(test_loader.dataset),\n",
        "        )\n",
        "    )\n",
        "    return test_loss,correct\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description=\"PyTorch MNIST Example\")\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=64,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size for training (default: 64)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--test-batch-size\",\n",
        "        type=int,\n",
        "        default=1000,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size for testing (default: 1000)\",\n",
        "    )\n",
        "\n",
        "# set number of epochs here\n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        type=int,\n",
        "        default=14,\n",
        "        metavar=\"N\",\n",
        "        help=\"number of epochs to train (default: 14)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lr\",\n",
        "        type=float,\n",
        "        default=1.0,\n",
        "        metavar=\"LR\",\n",
        "        help=\"learning rate (default: 1.0)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gamma\",\n",
        "        type=float,\n",
        "        default=0.7,\n",
        "        metavar=\"M\",\n",
        "        help=\"Learning rate step gamma (default: 0.7)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--log-interval\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"N\",\n",
        "        help=\"how many batches to wait before logging training status\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--save-model\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"For Saving the current Model\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--algorithm\",\n",
        "        type=str,\n",
        "        default=\"qsparselocal\",\n",
        "        help=\"gradient_allreduce, bytegrad, decentralized, low_precision_decentralized, qadam, async\",\n",
        "        #Add new algorithm for testing------------------\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--async-sync-interval\",\n",
        "        default=500,\n",
        "        type=int,\n",
        "        help=\"Model synchronization interval(ms) for async algorithm\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--set-deterministic\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"set deterministic or not\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fuse-optimizer\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"fuse optimizer or not\",\n",
        "    )\n",
        "\n",
        "    #args = parser.parse_args() \n",
        "    # New line below solves ipykernel_launcher.py: error: unrecognized arguments\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    \n",
        "    if args.set_deterministic:\n",
        "        print(\"set_deterministic: True\")\n",
        "        np.random.seed(666)\n",
        "        random.seed(666)\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.manual_seed(666)\n",
        "        torch.cuda.manual_seed_all(666 + int(bagua.get_rank()))\n",
        "        torch.set_printoptions(precision=10)\n",
        "\n",
        "    torch.cuda.set_device(bagua.get_local_rank())\n",
        "    bagua.init_process_group()\n",
        "\n",
        "\n",
        "\n",
        "    logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.ERROR)\n",
        "    if bagua.get_rank() == 0:\n",
        "        logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    train_kwargs = {\"batch_size\": args.batch_size}\n",
        "    test_kwargs = {\"batch_size\": args.test_batch_size}\n",
        "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        "    )\n",
        "\n",
        "    if bagua.get_local_rank() == 0:\n",
        "        dataset1 = datasets.MNIST(\n",
        "            \"../data\", train=True, download=True, transform=transform\n",
        "        )\n",
        "        torch.distributed.barrier()\n",
        "    else:\n",
        "        torch.distributed.barrier()\n",
        "        dataset1 = datasets.MNIST(\n",
        "            \"../data\", train=True, download=True, transform=transform\n",
        "        )\n",
        "\n",
        "    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        dataset1, num_replicas=bagua.get_world_size(), rank=bagua.get_rank()\n",
        "    )\n",
        "    train_kwargs.update(\n",
        "        {\n",
        "            \"sampler\": train_sampler,\n",
        "            \"batch_size\": args.batch_size // bagua.get_world_size(),\n",
        "            \"shuffle\": False,\n",
        "        }\n",
        "    )\n",
        "    # Train and Test dataset\n",
        "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "    # ??????????  Throw the instantiation of the network onto the cuda dvice\n",
        "    model = Net().cuda()\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "    if args.algorithm == \"gradient_allreduce\":\n",
        "        from bagua.torch_api.algorithms import gradient_allreduce\n",
        "\n",
        "        algorithm = gradient_allreduce.GradientAllReduceAlgorithm()\n",
        "    elif args.algorithm == \"decentralized\":\n",
        "        from bagua.torch_api.algorithms import decentralized\n",
        "\n",
        "        algorithm = decentralized.DecentralizedAlgorithm()\n",
        "    elif args.algorithm == \"low_precision_decentralized\":\n",
        "        from bagua.torch_api.algorithms import decentralized\n",
        "\n",
        "        algorithm = decentralized.LowPrecisionDecentralizedAlgorithm()\n",
        "    elif args.algorithm == \"bytegrad\":\n",
        "        from bagua.torch_api.algorithms import bytegrad\n",
        "\n",
        "        algorithm = bytegrad.ByteGradAlgorithm()\n",
        "    elif args.algorithm == \"qadam\":\n",
        "        from bagua.torch_api.algorithms import q_adam\n",
        "\n",
        "        optimizer = q_adam.QAdamOptimizer(\n",
        "            model.parameters(), lr=args.lr, warmup_steps=100\n",
        "        )\n",
        "        algorithm = q_adam.QAdamAlgorithm(optimizer)\n",
        "    #################################################################    \n",
        "    elif args.algorithm == \"qsparselocal\":\n",
        "        learning_rate = 0.1\n",
        "        # Set lower learning rate, no convergence for lr = 1\n",
        "        optimizer = QSparseLocalOptimizer(\n",
        "            model.parameters(), lr=learning_rate, schedule = 1\n",
        "        )\n",
        "        algorithm = QSparseLocalAlgorithm(optimizer)\n",
        "    elif args.algorithm == \"async\":\n",
        "        from bagua.torch_api.algorithms import async_model_average\n",
        "\n",
        "        algorithm = async_model_average.AsyncModelAverageAlgorithm(\n",
        "            sync_interval_ms=args.async_sync_interval,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    #  Model von Bagua\n",
        "    model = model.with_bagua(\n",
        "        [optimizer],\n",
        "        algorithm,\n",
        "        do_flatten=not args.fuse_optimizer,\n",
        "    )\n",
        "\n",
        "    # Optimizer from Bagua if args.fuse_optimizer==True\n",
        "    if args.fuse_optimizer:\n",
        "        optimizer = bagua.contrib.fuse_optimizer(optimizer)\n",
        "\n",
        "    #------------ Loss, accuracy\n",
        "    loss_list =[]\n",
        "    acc_list = []\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "    \n",
        "    start = time.time()\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        if args.algorithm == \"async\":\n",
        "            model.bagua_algorithm.resume(model)\n",
        "\n",
        "        train(args, model, train_loader, optimizer, epoch)\n",
        "\n",
        "        if args.algorithm == \"async\":\n",
        "            model.bagua_algorithm.abort(model)\n",
        "\n",
        "        new_loss,new_acc =test(model, test_loader)\n",
        "        loss_list.append(new_loss)\n",
        "        acc_list.append(new_acc/100.0)\n",
        "        scheduler.step()\n",
        "        ####\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if args.save_model:\n",
        "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "    # Used for measuring the time taken for the epochs themselves\n",
        "    end = time.time()\n",
        "    print(\"Elapsed time:\",end-start)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    ep =[]\n",
        "\n",
        "\n",
        "    for i in range(1, args.epochs + 1):\n",
        "      ep.append(i)\n",
        "\n",
        "    print(\"Current quantization method:\",quantization_scheme)\n",
        "    print(\"Learning rate:\",learning_rate)\n",
        "    print(\"Loss:\",loss_list)\n",
        "    print(\"Accuracy:\",acc_list)\n",
        "    \n",
        "     \n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.subplot(211)\n",
        "    plt.plot(ep,loss_list)\n",
        "    plt.subplot(212)\n",
        "    plt.plot(ep,acc_list)\n",
        "\n",
        "    plt.show()  \n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rSo-g6qDIvE"
      },
      "outputs": [],
      "source": [
        "%tb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "QSparseLocal_MNIST_5_5_comp_init_op.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNXoVU8cEFik0NXbivghFk7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}