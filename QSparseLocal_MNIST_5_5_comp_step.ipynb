{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverCore97/Bagua/blob/main/QSparseLocal_MNIST_5_5_comp_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FidgI73Gc8m7"
      },
      "source": [
        "##**Check CUDA version**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biBKGSXi6Chp",
        "outputId": "7d3e9457-c660-427b-abe8-7d7ee34525e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1UzpVrNc3wO"
      },
      "source": [
        "##**Install Bagua**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qCGqkkr5mr4",
        "outputId": "61a081b7-cccd-45ab-f0d9-7da239be001f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bagua-cuda111 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: gorilla==0.4.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.4.0)\n",
            "Requirement already satisfied: setuptools-rust in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.3.0)\n",
            "Requirement already satisfied: flask>=2.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (4.64.0)\n",
            "Requirement already satisfied: scikit-optimize>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.9.0)\n",
            "Requirement already satisfied: scikit-learn!=1.0,<=1.0.1,>=0.24 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.0.1)\n",
            "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.27.1)\n",
            "Requirement already satisfied: xxhash>=2.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (3.0.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.4.4)\n",
            "Requirement already satisfied: parallel-ssh==2.8.0 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.8.0)\n",
            "Requirement already satisfied: pydantic>=1.8 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (1.21.6)\n",
            "Requirement already satisfied: deprecation>=2.1 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (2.1.0)\n",
            "Requirement already satisfied: pytest-benchmark>=3.4 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (3.4.1)\n",
            "Requirement already satisfied: gevent>=21.8 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (21.12.0)\n",
            "Requirement already satisfied: prometheus-client>=0.11 in /usr/local/lib/python3.7/dist-packages (from bagua-cuda111) (0.14.1)\n",
            "Requirement already satisfied: ssh2-python>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from parallel-ssh==2.8.0->bagua-cuda111) (0.27.0)\n",
            "Requirement already satisfied: ssh-python>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from parallel-ssh==2.8.0->bagua-cuda111) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deprecation>=2.1->bagua-cuda111) (21.3)\n",
            "Requirement already satisfied: Werkzeug>=2.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (8.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (2.1.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (4.11.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.7/dist-packages (from flask>=2.0->bagua-cuda111) (3.1.2)\n",
            "Requirement already satisfied: zope.event in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (4.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (62.1.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (1.1.2)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.7/dist-packages (from gevent>=21.8->bagua-cuda111) (5.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6.0->flask>=2.0->bagua-cuda111) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6.0->flask>=2.0->bagua-cuda111) (4.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=3.0->flask>=2.0->bagua-cuda111) (2.0.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.7/dist-packages (from pytest-benchmark>=3.4->bagua-cuda111) (8.0.0)\n",
            "Requirement already satisfied: pytest>=3.8 in /usr/local/lib/python3.7/dist-packages (from pytest-benchmark>=3.4->bagua-cuda111) (7.1.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (21.4.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (1.11.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (1.1.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (2.0.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=3.8->pytest-benchmark>=3.4->bagua-cuda111) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25->bagua-cuda111) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=1.0,<=1.0.1,>=0.24->bagua-cuda111) (1.4.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize>=0.8.1->bagua-cuda111) (21.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize>=0.8.1->bagua-cuda111) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deprecation>=2.1->bagua-cuda111) (3.0.8)\n",
            "Requirement already satisfied: semantic-version<3,>=2.8.2 in /usr/local/lib/python3.7/dist-packages (from setuptools-rust->bagua-cuda111) (2.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install bagua-cuda111"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IThXKxGEcv4z"
      },
      "source": [
        "## **Import environment variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ykSOeQsnTLCz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['RANK'] = '0'\n",
        "os.environ['WORLD_SIZE'] = '1'\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '29500'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M-JsrdWcjzj"
      },
      "source": [
        "## **QSparseLocal Algorithm Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_TpSXZaAd8r8"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from bagua.torch_api.bucket import BaguaBucket\n",
        "from bagua.torch_api.tensor import BaguaTensor\n",
        "from bagua.torch_api.data_parallel.bagua_distributed import BaguaDistributedDataParallel\n",
        "from bagua.torch_api.algorithms import Algorithm, AlgorithmImpl\n",
        "from bagua.torch_api.communication import BaguaProcessGroup\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "sparsify = True\n",
        "use_memory = True\n",
        "quantization_scheme = 'sign'\n",
        "quantization_levels = 256\n",
        "top_k_sparsification = True\n",
        "k = 1000\n",
        "use_normalization = True\n",
        "\n",
        "\n",
        "## input: Uncompressed Gradient tensor\n",
        "## Output: Quantized and sparsified Gradient tensor\n",
        "def qsl(eta_grad,\n",
        "        memory,\n",
        "        qsl_grad,   # No output, instead change in function\n",
        "        topK_flag,\n",
        "        s,\n",
        "        # sparsify,\n",
        "        # use_memory,\n",
        "        # quantization_scheme,\n",
        "        # use_normalization\n",
        "        ):\n",
        "    ###To do: Allow other quantization\n",
        "    def signq(var):\n",
        "        # Normalization according to input\n",
        "        # ||var||_1 * sign(var)/\n",
        "        one_norm = torch.norm(var, p=1)\n",
        "        return one_norm * torch.sign(var + 1e-13) / float(torch.numel(var))\n",
        "        # return torch.sign(var)  # Returns a new tensor with the signs of the elements of input\n",
        "\n",
        "    def qsgd(var):\n",
        "        level_float = s * torch.abs(var) / norm1\n",
        "        previous_level = torch.floor(level_float)\n",
        "        is_next_level = (torch.rand(var.size(), dtype=torch.float32, device = 'cuda') < (level_float - previous_level))\n",
        "        is_next_level = is_next_level.float()\n",
        "        new_level = previous_level + is_next_level\n",
        "        unnormalized = torch.sign(var) * new_level * norm1 / s\n",
        "        beta = float(torch.numel(var)) / float(s * s)\n",
        "        return unnormalized / (1.0 + beta) if use_normalization else unnormalized\n",
        "\n",
        "    def get_quantization(q):\n",
        "        if q == 'qsgd':\n",
        "            return qsgd\n",
        "        elif q == 'sign':\n",
        "            return signq\n",
        "        else:\n",
        "            return lambda x: x\n",
        "\n",
        "    if not sparsify:\n",
        "        norm1 = torch.norm(eta_grad) + torch.constant(1e-5, dtype=torch.float32)\n",
        "        if use_memory:\n",
        "            input = memory + eta_grad\n",
        "        else:\n",
        "            input = eta_grad\n",
        "\n",
        "        func = get_quantization(quantization_scheme)\n",
        "        q = func(input)\n",
        "\n",
        "        return q, input - q\n",
        "\n",
        "    input = memory + eta_grad\n",
        "\n",
        "    org_shape = input.size()\n",
        "    numel = torch.numel(input)\n",
        "    K = min(numel, k)  # k is the optimizer's k,\n",
        "    # K is the actual value used for sparsification\n",
        "\n",
        "    if topK_flag:\n",
        "        # Get values and index tensor of chosen components\n",
        "        # flat shape with absolute values\n",
        "        _, indices = torch.topk(torch.reshape(torch.abs(input), [-1]), K)\n",
        "    else:\n",
        "        indices = torch.from_numpy(np.random.choice(torch.range(numel), K, False))\n",
        "\n",
        "    # Flatten input\n",
        "    flat_input = torch.reshape(input, [-1])\n",
        "    values = torch.gather(flat_input, 0, indices)  # dim=0\n",
        "    norm1 = torch.norm(values)\n",
        "    quantization_func = get_quantization(quantization_scheme)\n",
        "    flattened_quantized = torch.zeros_like(flat_input).scatter(0, indices,\n",
        "                                                               quantization_func(values))\n",
        "    quantization = torch.reshape(flattened_quantized, shape=org_shape)\n",
        "\n",
        "    q_func = lambda: quantization\n",
        "    zero_tensor = lambda: torch.zeros_like(input, dtype=torch.float32)\n",
        "\n",
        "    # q = torch.where( float(0)<norm1, q_func, zero_tensor)    # Where not applicable for choosing functions\n",
        "    if float(0) < norm1:\n",
        "        q = q_func()\n",
        "    else:\n",
        "        q = zero_tensor()\n",
        "\n",
        "    err = input - q\n",
        "\n",
        "    #print(\"\\n\\n input in qsl\",\"of shape\",input.size(),\"\\n\", torch.reshape(input, [-1])[:3])\n",
        "    #print(\"q:\",torch.reshape(q, [-1])[:3])\n",
        "    #print(\"err:\", torch.reshape(err, [-1])[:3])\n",
        "\n",
        "    memory.mul_(0).add_(err)\n",
        "    qsl_grad.mul_(0).add_(q)\n",
        "\n",
        "\n",
        "class QSparseLocalOptimizer(Optimizer):\n",
        "    def __init__(\n",
        "            self,\n",
        "            params,\n",
        "            lr: float = 1e-3,  ## Later step dependent learning rate\n",
        "            k: int = 1000,\n",
        "            schedule: int = 1\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create a dedicated optimizer used for\n",
        "        `QSparseLocal <https://tutorials.baguasys.com/algorithms/q-adam>`_ algorithm.\n",
        "\n",
        "        Args:\n",
        "            params (iterable): Iterable of parameters to optimize or dicts defining\n",
        "                parameter groups.\n",
        "            lr: Learning rate.\n",
        "            k: How many tensor components are kept during sparsification\n",
        "            schedule: Description of synchronization schedule\n",
        "        \"\"\"\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0 < schedule:\n",
        "            raise ValueError(\"Invalid schedule: {}\".format(lr))\n",
        "\n",
        "        defaults = dict(lr=lr, k=k, schedule=schedule)\n",
        "        super(QSparseLocalOptimizer, self).__init__(params, defaults)\n",
        "        # TODO: QSparseLocal optimizer maintain `step_id` in its state\n",
        "        self.step_id = 0\n",
        "        self.schedule = schedule\n",
        "        self.k = k\n",
        "        self.lr = lr\n",
        "        self.sync = False\n",
        "\n",
        "        # initialize global and local model, and memory for error compensation\n",
        "        for group_id, group in enumerate(self.param_groups):\n",
        "            params_with_grad = []\n",
        "            for p in group[\"params\"]:\n",
        "                params_with_grad.append(p)\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    ##### Instead of state[\"local\"] use param.data as local model\n",
        "                    # state[\"local\"] = torch.zeros_like(\n",
        "                    #     p, memory_format=torch.preserve_format\n",
        "                    # )\n",
        "\n",
        "                    state[\"global\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    # Set global to initialized value of local weights\n",
        "                    state[\"global\"].add_(p.data)\n",
        "\n",
        "                    # print(\"global init\",state[\"global\"].size())\n",
        "                    state[\"memory\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    # print(\"comp init\", state[\"error_comp\"].size())\n",
        "                    state[\"qsl_grad\"] = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    #print(\"Hello\")\n",
        "                    \"\"\"\n",
        "                    ###local: Local parameter vector\n",
        "                    global: Global paramenter vector (same for all workers)\n",
        "                    error_comp: Term for error compensation for quantization and sparsification of gradient tensor\n",
        "                    qsl_grad: The quantized and sparsified gradient tensor to be sent to master (to allreduce)\n",
        "                    \"\"\"\n",
        "                    \"\"\"  ###Leads to problem with gradient\n",
        "                    # Set local weights to 0 in the beginning, format: torch.Size([32, 1, 3, 3])\n",
        "                    \n",
        "                    # p.data uses normal pytorch initializer by default\n",
        "                    p.data = torch.zeros_like(\n",
        "                        p, memory_format=torch.preserve_format\n",
        "                    )\n",
        "                    \"\"\"\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(QSparseLocalOptimizer, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        self.step_id += 1\n",
        "\n",
        "        # Now schedule defines the number of round per synchronization round\n",
        "        self.sync = self.step_id % self.schedule == 0\n",
        "\n",
        "        for group_id, group in enumerate(self.param_groups):\n",
        "            for param_id, param in enumerate(group[\"params\"]):\n",
        "                state = self.state[param]\n",
        "\n",
        "                #print(\"Data: \",torch.reshape(param.data,[-1])[:10])\n",
        "\n",
        "                \n",
        "                #Calculate new global and local weights\n",
        "                if self.sync:\n",
        "                    #new_var = last_var + hvd.allreduce(var-last_var, var, opt, wipe_memory)\n",
        "                    # Nothing happens for the first step\n",
        "\n",
        "                    state[\"global\"].add_(state[\"qsl_grad\"])\n",
        "                    param.data.mul_(0).add_(state[\"global\"])\n",
        "                \n",
        "\n",
        "                #LARC\n",
        "                #eta=1/math.sqrt(torch.numel(param.grad))        \n",
        "                #param.data.add_(param.grad, alpha=-min(self.lr,eta*torch.norm(param.data, p=1)/torch.norm(param.grad, p=1)))\n",
        "                \n",
        "                param.data.add_(param.grad,alpha=-self.lr) \n",
        "\n",
        "\n",
        "                \n",
        "                ##### In allreduce before allreduce operation\n",
        "                # Compute compressed gradient and new error compensation term\n",
        "                # Horovod: eta local-global\n",
        "                if self.sync:\n",
        "\n",
        "                    # No output, new values assigned inside the function\n",
        "                    qsl(param.data - state[\"global\"], state[\"memory\"],state[\"qsl_grad\"],\n",
        "                                                topK_flag=top_k_sparsification, s=quantization_levels)\n",
        "                    \n",
        "                \n",
        "\n",
        "class QSparseLocalAlgorithmImpl(AlgorithmImpl):\n",
        "    def __init__(\n",
        "            self,\n",
        "            process_group: BaguaProcessGroup,\n",
        "            q_sparse_local_optimizer: QSparseLocalOptimizer,\n",
        "            hierarchical: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implementation of the\n",
        "        `QSparseLocal Algorithm <https://tutorials.baguasys.com/algorithms/q-adam>`_\n",
        "        .\n",
        "        Args:\n",
        "            process_group: The process group to work on.\n",
        "            q_sparse_local_optimizer: A QSparseLocalOptimizer initialized with model parameters.\n",
        "            hierarchical: Enable hierarchical communication.\n",
        "        \"\"\"\n",
        "        super(QSparseLocalAlgorithmImpl, self).__init__(process_group)\n",
        "        self.hierarchical = hierarchical\n",
        "        self.optimizer = q_sparse_local_optimizer\n",
        "        self.sync = self.optimizer.sync\n",
        "\n",
        "    # Needed to switch between synchronization aand local rounds\n",
        "    def need_reset(self):\n",
        "        return True\n",
        "\n",
        "    def init_tensors(self, bagua_distributed_data_parallel: BaguaDistributedDataParallel):\n",
        "        parameters = bagua_distributed_data_parallel.bagua_build_params()\n",
        "\n",
        "        for idx, (name, param) in enumerate(parameters.__reversed__()):\n",
        "            param._q_sparse_local_name = name\n",
        "            param._q_sparse_local_idx = idx\n",
        "\n",
        "        tensor_groups = []\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for param in group[\"params\"]:\n",
        "                #if self.sync:\n",
        "\n",
        "                # Second half Step 4\n",
        "                def set_weights(param, t):\n",
        "                    # Set compressed gradient to mean of all workers compressed gradients\n",
        "                    self.optimizer.state[param][\"qsl_grad\"] = t\n",
        "\n",
        "                registered_tensor = param.bagua_ensure_grad().ensure_bagua_tensor(\n",
        "                    param._q_sparse_local_name,\n",
        "                    bagua_distributed_data_parallel.bagua_module_name,\n",
        "                    getter_closure=lambda param: self.optimizer.state[param][\"qsl_grad\"],\n",
        "                    setter_closure=set_weights,\n",
        "                )\n",
        "                tensor_groups.append(registered_tensor)\n",
        "                #else:\n",
        "                    # Nothing happens\n",
        "                    #pass\n",
        "\n",
        "        tensor_groups.sort(key=lambda x: x._q_sparse_local_idx)\n",
        "        return tensor_groups\n",
        "\n",
        "    def tensors_to_buckets(\n",
        "            self, tensors: List[List[BaguaTensor]], do_flatten: bool\n",
        "    ) -> List[BaguaBucket]:\n",
        "        bagua_buckets = []\n",
        "\n",
        "        for idx, bucket in enumerate(tensors):\n",
        "            bagua_bucket = BaguaBucket(\n",
        "                bucket,\n",
        "                flatten=do_flatten,\n",
        "                # flatten=True,\n",
        "                name=str(idx),\n",
        "                alignment=self.process_group.get_global_communicator().nranks(),\n",
        "            )\n",
        "            bagua_buckets.append(bagua_bucket)\n",
        "        return bagua_buckets\n",
        "\n",
        "    def init_operations(\n",
        "            self,\n",
        "            bagua_distributed_data_parallel: BaguaDistributedDataParallel,\n",
        "            bucket: BaguaBucket,\n",
        "    ):\n",
        "        bucket.clear_ops()\n",
        "        \n",
        "        # For synchronization round we utilize allreduce\n",
        "        if True:  #self.sync:\n",
        "            # Compression is done by Bagua\n",
        "            bucket.append_centralized_synchronous_op(\n",
        "                hierarchical=self.hierarchical,\n",
        "                average=True,  # Maybe try false and then average it manually to preserve ints\n",
        "                scattergather=True,\n",
        "                compression=\"MinMaxUInt8\",      #TO DO:  Make qsl_grad suitable for compression\n",
        "                group=self.process_group,\n",
        "            )\n",
        "        else:  # Nothing happens\n",
        "            pass\n",
        "\n",
        "    # Instead of momentum hook, we use a qsl_gradient hook\n",
        "    def init_backward_hook(self, bagua_distributed_data_parallel: BaguaDistributedDataParallel):\n",
        "\n",
        "        def hook_qsl_grad(parameter_name, parameter):\n",
        "            assert (\n",
        "                    parameter.bagua_backend_tensor().data_ptr()\n",
        "                    == self.optimizer.state[parameter][\"qsl_grad\"].data_ptr()\n",
        "            ), \"bagua backend tensor data_ptr should match _q_sparse_local_grad data_ptr\"\n",
        "            parameter.bagua_mark_communication_ready()\n",
        "\n",
        "        return hook_qsl_grad\n",
        "\n",
        "\n",
        "class QSparseLocalAlgorithm(Algorithm):\n",
        "    def __init__(self, q_sparse_local_optimizer: QSparseLocalOptimizer, hierarchical: bool = True):\n",
        "        \"\"\"\n",
        "        Create an instance of the\n",
        "        `QSparseLocal Algorithm <https://tutorials.baguasys.com/algorithms/q-adam>`_\n",
        "        .\n",
        "\n",
        "        Args:\n",
        "            q_sparse_local_optimizer: A QSparseLocalOptimizer initialized with model parameters.\n",
        "            hierarchical: Enable hierarchical communication.\n",
        "        \"\"\"\n",
        "        self.hierarchical = hierarchical\n",
        "        self.optimizer = q_sparse_local_optimizer\n",
        "\n",
        "    def reify(self, process_group: BaguaProcessGroup) -> QSparseLocalAlgorithmImpl:\n",
        "        return QSparseLocalAlgorithmImpl(\n",
        "            process_group,\n",
        "            q_sparse_local_optimizer=self.optimizer,\n",
        "            hierarchical=self.hierarchical,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPIWQ8CvcZ0G"
      },
      "source": [
        "## **MNIST example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9tjsC05-5qV_",
        "outputId": "e9032bd2-e0d0-4ea5-c8c4-5541a9012fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc_version:  11.1\n",
            "The destination directory /root/.local/share/bagua/nccl already exists.\n",
            "Installing nccl 2.10.3 for CUDA 11.1 to: /root/.local/share/bagua/nccl\n",
            "Downloading https://developer.download.nvidia.com/compute/redist/nccl/v2.10/nccl_2.10.3-1+cuda11.0_x86_64.txz...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "nccl_2.10.3-1+cuda11.0_x86_64.txz: 138MB [00:01, 123MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting...\n",
            "Installing...\n",
            "Cleaning up...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gevent/threadpool.py\", line 157, in _before_run_task\n",
            "    _sys.settrace(_get_thread_trace())\n",
            "\n",
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/gevent/threadpool.py\", line 162, in _after_run_task\n",
            "    _sys.settrace(None)\n",
            "\n",
            "INFO:root:Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305121\n",
            "INFO:root:Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.195660\n",
            "INFO:root:Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.130198\n",
            "INFO:root:Train Epoch: 1 [1920/60000 (3%)]\tLoss: 1.886657\n",
            "INFO:root:Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.445501\n",
            "INFO:root:Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.198297\n",
            "INFO:root:Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.165647\n",
            "INFO:root:Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.940339\n",
            "INFO:root:Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.792713\n",
            "INFO:root:Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.864128\n",
            "INFO:root:Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.899394\n",
            "INFO:root:Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.587050\n",
            "INFO:root:Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.534665\n",
            "INFO:root:Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.563255\n",
            "INFO:root:Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.708257\n",
            "INFO:root:Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.696454\n",
            "INFO:root:Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.468429\n",
            "INFO:root:Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.604372\n",
            "INFO:root:Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.459144\n",
            "INFO:root:Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.497298\n",
            "INFO:root:Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.690794\n",
            "INFO:root:Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.621075\n",
            "INFO:root:Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.431974\n",
            "INFO:root:Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.355198\n",
            "INFO:root:Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.590382\n",
            "INFO:root:Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.625537\n",
            "INFO:root:Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.409295\n",
            "INFO:root:Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.406954\n",
            "INFO:root:Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.526163\n",
            "INFO:root:Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.539476\n",
            "INFO:root:Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.427056\n",
            "INFO:root:Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.590204\n",
            "INFO:root:Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.643930\n",
            "INFO:root:Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.524627\n",
            "INFO:root:Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.417724\n",
            "INFO:root:Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.527612\n",
            "INFO:root:Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.474670\n",
            "INFO:root:Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.690218\n",
            "INFO:root:Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.410698\n",
            "INFO:root:Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.316902\n",
            "INFO:root:Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.465195\n",
            "INFO:root:Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.529422\n",
            "INFO:root:Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.404361\n",
            "INFO:root:Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.470702\n",
            "INFO:root:Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.574738\n",
            "INFO:root:Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.556501\n",
            "INFO:root:Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.502386\n",
            "INFO:root:Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.398223\n",
            "INFO:root:Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.266575\n",
            "INFO:root:Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.385636\n",
            "INFO:root:Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.419811\n",
            "INFO:root:Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.455858\n",
            "INFO:root:Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.491008\n",
            "INFO:root:Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.178511\n",
            "INFO:root:Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.577477\n",
            "INFO:root:Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.566501\n",
            "INFO:root:Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.367720\n",
            "INFO:root:Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.606741\n",
            "INFO:root:Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.490362\n",
            "INFO:root:Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.395281\n",
            "INFO:root:Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.268120\n",
            "INFO:root:Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.344688\n",
            "INFO:root:Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.200053\n",
            "INFO:root:Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.534633\n",
            "INFO:root:Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.272925\n",
            "INFO:root:Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.551834\n",
            "INFO:root:Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.335962\n",
            "INFO:root:Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.512355\n",
            "INFO:root:Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.485242\n",
            "INFO:root:Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.289545\n",
            "INFO:root:Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.201716\n",
            "INFO:root:Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.274676\n",
            "INFO:root:Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.301132\n",
            "INFO:root:Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.248265\n",
            "INFO:root:Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.223112\n",
            "INFO:root:Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.248015\n",
            "INFO:root:Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.310328\n",
            "INFO:root:Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.397564\n",
            "INFO:root:Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.371571\n",
            "INFO:root:Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.512624\n",
            "INFO:root:Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.403092\n",
            "INFO:root:Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.322235\n",
            "INFO:root:Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.298347\n",
            "INFO:root:Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.421534\n",
            "INFO:root:Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.273144\n",
            "INFO:root:Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.325646\n",
            "INFO:root:Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.241783\n",
            "INFO:root:Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.599476\n",
            "INFO:root:Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.446500\n",
            "INFO:root:Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.278466\n",
            "INFO:root:Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.202801\n",
            "INFO:root:Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.433781\n",
            "INFO:root:Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.199586\n",
            "INFO:root:Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.257963\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.2120, Accuracy: 9340/10000 (93%)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "INFO:root:Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.499391\n",
            "INFO:root:Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.354632\n",
            "INFO:root:Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.390128\n",
            "INFO:root:Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.470535\n",
            "INFO:root:Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.164953\n",
            "INFO:root:Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.340891\n",
            "INFO:root:Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.296261\n",
            "INFO:root:Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.308588\n",
            "INFO:root:Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.302353\n",
            "INFO:root:Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.268718\n",
            "INFO:root:Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.601243\n",
            "INFO:root:Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.266834\n",
            "INFO:root:Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.231374\n",
            "INFO:root:Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.305737\n",
            "INFO:root:Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.255427\n",
            "INFO:root:Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.372212\n",
            "INFO:root:Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.358974\n",
            "INFO:root:Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.270476\n",
            "INFO:root:Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.196081\n",
            "INFO:root:Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.241580\n",
            "INFO:root:Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.530195\n",
            "INFO:root:Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.203172\n",
            "INFO:root:Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.272049\n",
            "INFO:root:Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.231973\n",
            "INFO:root:Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.248992\n",
            "INFO:root:Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.244640\n",
            "INFO:root:Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.189306\n",
            "INFO:root:Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.235671\n",
            "INFO:root:Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.147747\n",
            "INFO:root:Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.293752\n",
            "INFO:root:Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.303702\n",
            "INFO:root:Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.460541\n",
            "INFO:root:Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.490294\n",
            "INFO:root:Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.257485\n",
            "INFO:root:Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.262741\n",
            "INFO:root:Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.525806\n",
            "INFO:root:Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.291857\n",
            "INFO:root:Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.335398\n",
            "INFO:root:Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.287583\n",
            "INFO:root:Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.269305\n",
            "INFO:root:Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.315757\n",
            "INFO:root:Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.334767\n",
            "INFO:root:Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.175283\n",
            "INFO:root:Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.250516\n",
            "INFO:root:Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.516770\n",
            "INFO:root:Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.242859\n",
            "INFO:root:Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.305084\n",
            "INFO:root:Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.310788\n",
            "INFO:root:Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.189531\n",
            "INFO:root:Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.182296\n",
            "INFO:root:Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.159376\n",
            "INFO:root:Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.233056\n",
            "INFO:root:Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.384374\n",
            "INFO:root:Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.168755\n",
            "INFO:root:Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.447197\n",
            "INFO:root:Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.345937\n",
            "INFO:root:Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.247588\n",
            "INFO:root:Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.341218\n",
            "INFO:root:Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.361472\n",
            "INFO:root:Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.281536\n",
            "INFO:root:Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.167600\n",
            "INFO:root:Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.223572\n",
            "INFO:root:Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.155715\n",
            "INFO:root:Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.337025\n",
            "INFO:root:Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.115773\n",
            "INFO:root:Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.451141\n",
            "INFO:root:Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.244398\n",
            "INFO:root:Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.290661\n",
            "INFO:root:Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.202858\n",
            "INFO:root:Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.212762\n",
            "INFO:root:Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.158140\n",
            "INFO:root:Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.216352\n",
            "INFO:root:Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.250791\n",
            "INFO:root:Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.214560\n",
            "INFO:root:Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.208113\n",
            "INFO:root:Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.281834\n",
            "INFO:root:Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.224505\n",
            "INFO:root:Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.280176\n",
            "INFO:root:Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.340696\n",
            "INFO:root:Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.396672\n",
            "INFO:root:Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.249059\n",
            "INFO:root:Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.190570\n",
            "INFO:root:Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.342563\n",
            "INFO:root:Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.457979\n",
            "INFO:root:Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.265762\n",
            "INFO:root:Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.272704\n",
            "INFO:root:Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.158371\n",
            "INFO:root:Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.404041\n",
            "INFO:root:Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.384217\n",
            "INFO:root:Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.188136\n",
            "INFO:root:Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.207384\n",
            "INFO:root:Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.271274\n",
            "INFO:root:Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.150170\n",
            "INFO:root:Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.208585\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.1401, Accuracy: 9570/10000 (96%)\n",
            "\n",
            "INFO:root:Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.345756\n",
            "INFO:root:Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.204144\n",
            "INFO:root:Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.299284\n",
            "INFO:root:Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.265042\n",
            "INFO:root:Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.149076\n",
            "INFO:root:Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.310467\n",
            "INFO:root:Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.228437\n",
            "INFO:root:Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.219248\n",
            "INFO:root:Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.181836\n",
            "INFO:root:Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.230647\n",
            "INFO:root:Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.324565\n",
            "INFO:root:Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.178213\n",
            "INFO:root:Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.203925\n",
            "INFO:root:Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.259768\n",
            "INFO:root:Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.334545\n",
            "INFO:root:Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.395736\n",
            "INFO:root:Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.312228\n",
            "INFO:root:Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.126475\n",
            "INFO:root:Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.113869\n",
            "INFO:root:Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.212673\n",
            "INFO:root:Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.473510\n",
            "INFO:root:Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.163947\n",
            "INFO:root:Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.143353\n",
            "INFO:root:Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.118518\n",
            "INFO:root:Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.233114\n",
            "INFO:root:Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.195455\n",
            "INFO:root:Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.171063\n",
            "INFO:root:Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.175457\n",
            "INFO:root:Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.162479\n",
            "INFO:root:Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.216280\n",
            "INFO:root:Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.171348\n",
            "INFO:root:Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.371222\n",
            "INFO:root:Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.396301\n",
            "INFO:root:Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.214285\n",
            "INFO:root:Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.268097\n",
            "INFO:root:Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.296370\n",
            "INFO:root:Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.188196\n",
            "INFO:root:Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.298858\n",
            "INFO:root:Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.205331\n",
            "INFO:root:Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.131439\n",
            "INFO:root:Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.291239\n",
            "INFO:root:Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.360013\n",
            "INFO:root:Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.129882\n",
            "INFO:root:Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.203216\n",
            "INFO:root:Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.500015\n",
            "INFO:root:Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.160192\n",
            "INFO:root:Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.331154\n",
            "INFO:root:Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.210131\n",
            "INFO:root:Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.069244\n",
            "INFO:root:Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.082890\n",
            "INFO:root:Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.155729\n",
            "INFO:root:Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.252443\n",
            "INFO:root:Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.299083\n",
            "INFO:root:Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.131685\n",
            "INFO:root:Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.351663\n",
            "INFO:root:Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.301909\n",
            "INFO:root:Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.255428\n",
            "INFO:root:Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.248229\n",
            "INFO:root:Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.425180\n",
            "INFO:root:Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.233116\n",
            "INFO:root:Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.206800\n",
            "INFO:root:Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.127864\n",
            "INFO:root:Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.172854\n",
            "INFO:root:Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.510064\n",
            "INFO:root:Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.110674\n",
            "INFO:root:Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.241056\n",
            "INFO:root:Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.282508\n",
            "INFO:root:Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.257227\n",
            "INFO:root:Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.344886\n",
            "INFO:root:Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.179919\n",
            "INFO:root:Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.187599\n",
            "INFO:root:Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.133342\n",
            "INFO:root:Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.218113\n",
            "INFO:root:Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.185897\n",
            "INFO:root:Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.283647\n",
            "INFO:root:Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.186608\n",
            "INFO:root:Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.207359\n",
            "INFO:root:Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.283253\n",
            "INFO:root:Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.235613\n",
            "INFO:root:Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.257197\n",
            "INFO:root:Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.156721\n",
            "INFO:root:Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.159757\n",
            "INFO:root:Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.263547\n",
            "INFO:root:Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.380826\n",
            "INFO:root:Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.239660\n",
            "INFO:root:Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.272648\n",
            "INFO:root:Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.168970\n",
            "INFO:root:Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.447506\n",
            "INFO:root:Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.248971\n",
            "INFO:root:Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.221703\n",
            "INFO:root:Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.195633\n",
            "INFO:root:Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.368191\n",
            "INFO:root:Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.158048\n",
            "INFO:root:Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.156028\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.1100, Accuracy: 9675/10000 (97%)\n",
            "\n",
            "INFO:root:Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.217982\n",
            "INFO:root:Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.304977\n",
            "INFO:root:Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.107773\n",
            "INFO:root:Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.295215\n",
            "INFO:root:Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.115759\n",
            "INFO:root:Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.225567\n",
            "INFO:root:Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.189222\n",
            "INFO:root:Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.258706\n",
            "INFO:root:Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.217253\n",
            "INFO:root:Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.135114\n",
            "INFO:root:Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.297708\n",
            "INFO:root:Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.136368\n",
            "INFO:root:Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.259360\n",
            "INFO:root:Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.135730\n",
            "INFO:root:Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.181908\n",
            "INFO:root:Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.257591\n",
            "INFO:root:Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.222460\n",
            "INFO:root:Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.132953\n",
            "INFO:root:Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.149154\n",
            "INFO:root:Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.133768\n",
            "INFO:root:Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.226002\n",
            "INFO:root:Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.106095\n",
            "INFO:root:Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.126798\n",
            "INFO:root:Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.170598\n",
            "INFO:root:Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.163756\n",
            "INFO:root:Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.143288\n",
            "INFO:root:Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.137537\n",
            "INFO:root:Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.192772\n",
            "INFO:root:Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.111146\n",
            "INFO:root:Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.303491\n",
            "INFO:root:Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.187676\n",
            "INFO:root:Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.234328\n",
            "INFO:root:Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.397871\n",
            "INFO:root:Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.186736\n",
            "INFO:root:Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.167836\n",
            "INFO:root:Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.288777\n",
            "INFO:root:Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.158008\n",
            "INFO:root:Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.297132\n",
            "INFO:root:Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.174689\n",
            "INFO:root:Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.136099\n",
            "INFO:root:Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.218736\n",
            "INFO:root:Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.144809\n",
            "INFO:root:Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.086601\n",
            "INFO:root:Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.183061\n",
            "INFO:root:Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.292265\n",
            "INFO:root:Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.197408\n",
            "INFO:root:Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.449344\n",
            "INFO:root:Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.118672\n",
            "INFO:root:Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.078300\n",
            "INFO:root:Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.079627\n",
            "INFO:root:Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.207143\n",
            "INFO:root:Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.206901\n",
            "INFO:root:Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.287738\n",
            "INFO:root:Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.068896\n",
            "INFO:root:Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.331162\n",
            "INFO:root:Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.234371\n",
            "INFO:root:Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.166217\n",
            "INFO:root:Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.173369\n",
            "INFO:root:Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.207444\n",
            "INFO:root:Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.090215\n",
            "INFO:root:Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.160279\n",
            "INFO:root:Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.066346\n",
            "INFO:root:Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.120508\n",
            "INFO:root:Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.472171\n",
            "INFO:root:Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.104307\n",
            "INFO:root:Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.236598\n",
            "INFO:root:Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.128586\n",
            "INFO:root:Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.198249\n",
            "INFO:root:Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.182864\n",
            "INFO:root:Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.100227\n",
            "INFO:root:Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.151806\n",
            "INFO:root:Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.115583\n",
            "INFO:root:Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.118050\n",
            "INFO:root:Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.118972\n",
            "INFO:root:Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.126499\n",
            "INFO:root:Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.133190\n",
            "INFO:root:Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.146032\n",
            "INFO:root:Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.191590\n",
            "INFO:root:Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.190545\n",
            "INFO:root:Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.187023\n",
            "INFO:root:Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.203717\n",
            "INFO:root:Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.168711\n",
            "INFO:root:Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.159750\n",
            "INFO:root:Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.367241\n",
            "INFO:root:Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.171397\n",
            "INFO:root:Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.165785\n",
            "INFO:root:Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.128236\n",
            "INFO:root:Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.324312\n",
            "INFO:root:Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.278178\n",
            "INFO:root:Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.202299\n",
            "INFO:root:Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.144631\n",
            "INFO:root:Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.186178\n",
            "INFO:root:Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.180544\n",
            "INFO:root:Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.107015\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0882, Accuracy: 9727/10000 (97%)\n",
            "\n",
            "INFO:root:Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.303577\n",
            "INFO:root:Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.149477\n",
            "INFO:root:Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.122448\n",
            "INFO:root:Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.142092\n",
            "INFO:root:Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.099799\n",
            "INFO:root:Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.215425\n",
            "INFO:root:Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.127431\n",
            "INFO:root:Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.265577\n",
            "INFO:root:Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.110507\n",
            "INFO:root:Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.079239\n",
            "INFO:root:Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.209783\n",
            "INFO:root:Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.040736\n",
            "INFO:root:Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.146662\n",
            "INFO:root:Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.134513\n",
            "INFO:root:Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.183331\n",
            "INFO:root:Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.284392\n",
            "INFO:root:Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.206037\n",
            "INFO:root:Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.113160\n",
            "INFO:root:Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.138853\n",
            "INFO:root:Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.176652\n",
            "INFO:root:Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.167701\n",
            "INFO:root:Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.116291\n",
            "INFO:root:Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.168165\n",
            "INFO:root:Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.070226\n",
            "INFO:root:Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.177226\n",
            "INFO:root:Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.090343\n",
            "INFO:root:Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.101158\n",
            "INFO:root:Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.260835\n",
            "INFO:root:Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.062494\n",
            "INFO:root:Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.124370\n",
            "INFO:root:Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.182205\n",
            "INFO:root:Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.263913\n",
            "INFO:root:Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.394065\n",
            "INFO:root:Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.263152\n",
            "INFO:root:Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.169166\n",
            "INFO:root:Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.158359\n",
            "INFO:root:Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.074941\n",
            "INFO:root:Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.332535\n",
            "INFO:root:Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.110888\n",
            "INFO:root:Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.135484\n",
            "INFO:root:Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.198485\n",
            "INFO:root:Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.211364\n",
            "INFO:root:Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.069650\n",
            "INFO:root:Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.110643\n",
            "INFO:root:Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.333174\n",
            "INFO:root:Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.146269\n",
            "INFO:root:Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.267070\n",
            "INFO:root:Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.116979\n",
            "INFO:root:Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.071608\n",
            "INFO:root:Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.127413\n",
            "INFO:root:Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.124198\n",
            "INFO:root:Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.122398\n",
            "INFO:root:Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.228866\n",
            "INFO:root:Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.107183\n",
            "INFO:root:Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.459804\n",
            "INFO:root:Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.181004\n",
            "INFO:root:Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.202280\n",
            "INFO:root:Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.213573\n",
            "INFO:root:Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.264673\n",
            "INFO:root:Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.142125\n",
            "INFO:root:Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.101123\n",
            "INFO:root:Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.069921\n",
            "INFO:root:Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.099613\n",
            "INFO:root:Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.226808\n",
            "INFO:root:Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.055619\n",
            "INFO:root:Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.149710\n",
            "INFO:root:Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.097410\n",
            "INFO:root:Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.206096\n",
            "INFO:root:Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.132851\n",
            "INFO:root:Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.111567\n",
            "INFO:root:Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.127631\n",
            "INFO:root:Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.122272\n",
            "INFO:root:Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.119479\n",
            "INFO:root:Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.084351\n",
            "INFO:root:Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.146316\n",
            "INFO:root:Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.114191\n",
            "INFO:root:Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.240654\n",
            "INFO:root:Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.178682\n",
            "INFO:root:Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.122590\n",
            "INFO:root:Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.171643\n",
            "INFO:root:Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.238435\n",
            "INFO:root:Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.082851\n",
            "INFO:root:Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.065069\n",
            "INFO:root:Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.248311\n",
            "INFO:root:Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.155896\n",
            "INFO:root:Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.144330\n",
            "INFO:root:Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.118075\n",
            "INFO:root:Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.363585\n",
            "INFO:root:Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.175796\n",
            "INFO:root:Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.108894\n",
            "INFO:root:Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.193794\n",
            "INFO:root:Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.187263\n",
            "INFO:root:Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.098221\n",
            "INFO:root:Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.131656\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0760, Accuracy: 9771/10000 (98%)\n",
            "\n",
            "INFO:root:Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.186647\n",
            "INFO:root:Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.085725\n",
            "INFO:root:Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.181360\n",
            "INFO:root:Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.189838\n",
            "INFO:root:Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.062481\n",
            "INFO:root:Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.078761\n",
            "INFO:root:Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.093246\n",
            "INFO:root:Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.104606\n",
            "INFO:root:Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.083106\n",
            "INFO:root:Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.065802\n",
            "INFO:root:Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.181692\n",
            "INFO:root:Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.087771\n",
            "INFO:root:Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.089916\n",
            "INFO:root:Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.076900\n",
            "INFO:root:Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.092047\n",
            "INFO:root:Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.304381\n",
            "INFO:root:Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.088559\n",
            "INFO:root:Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.038972\n",
            "INFO:root:Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.079887\n",
            "INFO:root:Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.057960\n",
            "INFO:root:Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.148697\n",
            "INFO:root:Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.076130\n",
            "INFO:root:Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.061288\n",
            "INFO:root:Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.181405\n",
            "INFO:root:Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.071030\n",
            "INFO:root:Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.086415\n",
            "INFO:root:Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.130583\n",
            "INFO:root:Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.100529\n",
            "INFO:root:Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.090370\n",
            "INFO:root:Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.234324\n",
            "INFO:root:Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.135112\n",
            "INFO:root:Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.332221\n",
            "INFO:root:Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.309859\n",
            "INFO:root:Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.129835\n",
            "INFO:root:Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.132607\n",
            "INFO:root:Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.226940\n",
            "INFO:root:Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.090242\n",
            "INFO:root:Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.127639\n",
            "INFO:root:Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.088340\n",
            "INFO:root:Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.048208\n",
            "INFO:root:Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.181507\n",
            "INFO:root:Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.160556\n",
            "INFO:root:Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.077420\n",
            "INFO:root:Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.099586\n",
            "INFO:root:Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.266379\n",
            "INFO:root:Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.115876\n",
            "INFO:root:Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.200092\n",
            "INFO:root:Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.187463\n",
            "INFO:root:Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.051683\n",
            "INFO:root:Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.125759\n",
            "INFO:root:Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.082592\n",
            "INFO:root:Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.130708\n",
            "INFO:root:Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.146375\n",
            "INFO:root:Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.026486\n",
            "INFO:root:Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.192312\n",
            "INFO:root:Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.143387\n",
            "INFO:root:Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.119702\n",
            "INFO:root:Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.138288\n",
            "INFO:root:Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.185403\n",
            "INFO:root:Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.116900\n",
            "INFO:root:Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.051936\n",
            "INFO:root:Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.074906\n",
            "INFO:root:Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.060521\n",
            "INFO:root:Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.257700\n",
            "INFO:root:Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.061769\n",
            "INFO:root:Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.233463\n",
            "INFO:root:Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.193280\n",
            "INFO:root:Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.077208\n",
            "INFO:root:Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.144910\n",
            "INFO:root:Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.085107\n",
            "INFO:root:Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.086890\n",
            "INFO:root:Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.081020\n",
            "INFO:root:Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.074256\n",
            "INFO:root:Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.124127\n",
            "INFO:root:Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.129784\n",
            "INFO:root:Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.065149\n",
            "INFO:root:Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.138042\n",
            "INFO:root:Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.150250\n",
            "INFO:root:Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.220948\n",
            "INFO:root:Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.100983\n",
            "INFO:root:Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.127002\n",
            "INFO:root:Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.041427\n",
            "INFO:root:Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.123518\n",
            "INFO:root:Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.234055\n",
            "INFO:root:Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.140296\n",
            "INFO:root:Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.129834\n",
            "INFO:root:Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.055282\n",
            "INFO:root:Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.233992\n",
            "INFO:root:Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.126800\n",
            "INFO:root:Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.080270\n",
            "INFO:root:Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.098316\n",
            "INFO:root:Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.213245\n",
            "INFO:root:Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.057127\n",
            "INFO:root:Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.023944\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0627, Accuracy: 9803/10000 (98%)\n",
            "\n",
            "INFO:root:Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.179778\n",
            "INFO:root:Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.071936\n",
            "INFO:root:Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.126224\n",
            "INFO:root:Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.135132\n",
            "INFO:root:Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.062347\n",
            "INFO:root:Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.178783\n",
            "INFO:root:Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.094044\n",
            "INFO:root:Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.060401\n",
            "INFO:root:Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.140406\n",
            "INFO:root:Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.089924\n",
            "INFO:root:Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.213039\n",
            "INFO:root:Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.063126\n",
            "INFO:root:Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.106528\n",
            "INFO:root:Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.036702\n",
            "INFO:root:Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.128586\n",
            "INFO:root:Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.291448\n",
            "INFO:root:Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.072032\n",
            "INFO:root:Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.046958\n",
            "INFO:root:Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.110937\n",
            "INFO:root:Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.064470\n",
            "INFO:root:Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.210320\n",
            "INFO:root:Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.072371\n",
            "INFO:root:Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.100015\n",
            "INFO:root:Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.130308\n",
            "INFO:root:Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.061918\n",
            "INFO:root:Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.040733\n",
            "INFO:root:Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.083120\n",
            "INFO:root:Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.055545\n",
            "INFO:root:Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.049216\n",
            "INFO:root:Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.113193\n",
            "INFO:root:Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.053248\n",
            "INFO:root:Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.265892\n",
            "INFO:root:Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.288470\n",
            "INFO:root:Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.064852\n",
            "INFO:root:Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.163614\n",
            "INFO:root:Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.094851\n",
            "INFO:root:Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.093881\n",
            "INFO:root:Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.182074\n",
            "INFO:root:Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.113488\n",
            "INFO:root:Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.054514\n",
            "INFO:root:Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.219941\n",
            "INFO:root:Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.267744\n",
            "INFO:root:Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.049078\n",
            "INFO:root:Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.053316\n",
            "INFO:root:Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.276929\n",
            "INFO:root:Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.070046\n",
            "INFO:root:Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.200423\n",
            "INFO:root:Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.088119\n",
            "INFO:root:Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.041151\n",
            "INFO:root:Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.073024\n",
            "INFO:root:Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.084833\n",
            "INFO:root:Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.042952\n",
            "INFO:root:Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.149305\n",
            "INFO:root:Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.030369\n",
            "INFO:root:Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.323646\n",
            "INFO:root:Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.147489\n",
            "INFO:root:Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.152833\n",
            "INFO:root:Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.143938\n",
            "INFO:root:Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.181911\n",
            "INFO:root:Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.095363\n",
            "INFO:root:Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.040581\n",
            "INFO:root:Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.051364\n",
            "INFO:root:Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.057955\n",
            "INFO:root:Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.312236\n",
            "INFO:root:Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.078606\n",
            "INFO:root:Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.095883\n",
            "INFO:root:Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.071914\n",
            "INFO:root:Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.086978\n",
            "INFO:root:Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.078691\n",
            "INFO:root:Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.105955\n",
            "INFO:root:Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.107946\n",
            "INFO:root:Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.048818\n",
            "INFO:root:Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.054701\n",
            "INFO:root:Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.060556\n",
            "INFO:root:Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.091910\n",
            "INFO:root:Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.076280\n",
            "INFO:root:Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.090801\n",
            "INFO:root:Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.130279\n",
            "INFO:root:Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.078165\n",
            "INFO:root:Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.103150\n",
            "INFO:root:Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.122027\n",
            "INFO:root:Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.057709\n",
            "INFO:root:Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.085412\n",
            "INFO:root:Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.174726\n",
            "INFO:root:Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.197708\n",
            "INFO:root:Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.115481\n",
            "INFO:root:Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.136762\n",
            "INFO:root:Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.136124\n",
            "INFO:root:Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.177942\n",
            "INFO:root:Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.083525\n",
            "INFO:root:Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.091475\n",
            "INFO:root:Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.127847\n",
            "INFO:root:Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.081563\n",
            "INFO:root:Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.106246\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0538, Accuracy: 9823/10000 (98%)\n",
            "\n",
            "INFO:root:Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.073821\n",
            "INFO:root:Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.045970\n",
            "INFO:root:Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.092195\n",
            "INFO:root:Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.178212\n",
            "INFO:root:Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.055333\n",
            "INFO:root:Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.095469\n",
            "INFO:root:Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.054151\n",
            "INFO:root:Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.121668\n",
            "INFO:root:Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.090645\n",
            "INFO:root:Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.055586\n",
            "INFO:root:Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.103594\n",
            "INFO:root:Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.037400\n",
            "INFO:root:Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.068742\n",
            "INFO:root:Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.061050\n",
            "INFO:root:Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.164470\n",
            "INFO:root:Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.213480\n",
            "INFO:root:Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.091587\n",
            "INFO:root:Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.025104\n",
            "INFO:root:Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.066381\n",
            "INFO:root:Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.040955\n",
            "INFO:root:Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.089554\n",
            "INFO:root:Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.084032\n",
            "INFO:root:Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.031883\n",
            "INFO:root:Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.079896\n",
            "INFO:root:Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.054984\n",
            "INFO:root:Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.080651\n",
            "INFO:root:Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.117053\n",
            "INFO:root:Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.064698\n",
            "INFO:root:Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.031120\n",
            "INFO:root:Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.104530\n",
            "INFO:root:Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.047303\n",
            "INFO:root:Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.154911\n",
            "INFO:root:Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.255327\n",
            "INFO:root:Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.078628\n",
            "INFO:root:Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.226785\n",
            "INFO:root:Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.130201\n",
            "INFO:root:Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.095263\n",
            "INFO:root:Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.075401\n",
            "INFO:root:Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.068563\n",
            "INFO:root:Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.077646\n",
            "INFO:root:Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.138052\n",
            "INFO:root:Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.196275\n",
            "INFO:root:Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.064002\n",
            "INFO:root:Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.053221\n",
            "INFO:root:Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.222538\n",
            "INFO:root:Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.082075\n",
            "INFO:root:Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.201634\n",
            "INFO:root:Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.088180\n",
            "INFO:root:Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.046400\n",
            "INFO:root:Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.069698\n",
            "INFO:root:Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.058158\n",
            "INFO:root:Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.082809\n",
            "INFO:root:Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.102055\n",
            "INFO:root:Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.033335\n",
            "INFO:root:Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.234447\n",
            "INFO:root:Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.181595\n",
            "INFO:root:Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.076319\n",
            "INFO:root:Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.106312\n",
            "INFO:root:Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.249075\n",
            "INFO:root:Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.155602\n",
            "INFO:root:Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.024889\n",
            "INFO:root:Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.109011\n",
            "INFO:root:Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.078772\n",
            "INFO:root:Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.172803\n",
            "INFO:root:Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.050353\n",
            "INFO:root:Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.113499\n",
            "INFO:root:Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.037208\n",
            "INFO:root:Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.075999\n",
            "INFO:root:Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.146694\n",
            "INFO:root:Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.031817\n",
            "INFO:root:Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.060400\n",
            "INFO:root:Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.040658\n",
            "INFO:root:Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.087358\n",
            "INFO:root:Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.070560\n",
            "INFO:root:Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.101746\n",
            "INFO:root:Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.053240\n",
            "INFO:root:Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.128196\n",
            "INFO:root:Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.131302\n",
            "INFO:root:Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.072083\n",
            "INFO:root:Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.107811\n",
            "INFO:root:Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.132540\n",
            "INFO:root:Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.059973\n",
            "INFO:root:Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.085245\n",
            "INFO:root:Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.250087\n",
            "INFO:root:Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.129274\n",
            "INFO:root:Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.131734\n",
            "INFO:root:Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.069974\n",
            "INFO:root:Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.168300\n",
            "INFO:root:Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.123132\n",
            "INFO:root:Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.069841\n",
            "INFO:root:Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.082862\n",
            "INFO:root:Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.136946\n",
            "INFO:root:Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.110480\n",
            "INFO:root:Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.059012\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0497, Accuracy: 9839/10000 (98%)\n",
            "\n",
            "INFO:root:Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.104371\n",
            "INFO:root:Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.034781\n",
            "INFO:root:Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.122598\n",
            "INFO:root:Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.205427\n",
            "INFO:root:Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.036163\n",
            "INFO:root:Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.106154\n",
            "INFO:root:Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.076202\n",
            "INFO:root:Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.120821\n",
            "INFO:root:Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.063485\n",
            "INFO:root:Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.043412\n",
            "INFO:root:Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.132068\n",
            "INFO:root:Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.030957\n",
            "INFO:root:Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.157107\n",
            "INFO:root:Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.026392\n",
            "INFO:root:Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.122543\n",
            "INFO:root:Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.085543\n",
            "INFO:root:Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.040317\n",
            "INFO:root:Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.067042\n",
            "INFO:root:Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.037344\n",
            "INFO:root:Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.033996\n",
            "INFO:root:Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.055217\n",
            "INFO:root:Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.064361\n",
            "INFO:root:Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.028492\n",
            "INFO:root:Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.084026\n",
            "INFO:root:Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.043256\n",
            "INFO:root:Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.049992\n",
            "INFO:root:Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.122619\n",
            "INFO:root:Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.063938\n",
            "INFO:root:Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.046400\n",
            "INFO:root:Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.078794\n",
            "INFO:root:Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.030848\n",
            "INFO:root:Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.066327\n",
            "INFO:root:Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.336925\n",
            "INFO:root:Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.076394\n",
            "INFO:root:Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.051219\n",
            "INFO:root:Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.061673\n",
            "INFO:root:Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.089378\n",
            "INFO:root:Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.071473\n",
            "INFO:root:Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.065527\n",
            "INFO:root:Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.044041\n",
            "INFO:root:Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.103143\n",
            "INFO:root:Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.108747\n",
            "INFO:root:Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.061474\n",
            "INFO:root:Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.071114\n",
            "INFO:root:Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.218362\n",
            "INFO:root:Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.044339\n",
            "INFO:root:Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.131090\n",
            "INFO:root:Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.078590\n",
            "INFO:root:Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.023230\n",
            "INFO:root:Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.045923\n",
            "INFO:root:Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.039864\n",
            "INFO:root:Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.074625\n",
            "INFO:root:Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.158545\n",
            "INFO:root:Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.055728\n",
            "INFO:root:Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.230127\n",
            "INFO:root:Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.099626\n",
            "INFO:root:Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.087244\n",
            "INFO:root:Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.043174\n",
            "INFO:root:Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.184070\n",
            "INFO:root:Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.014516\n",
            "INFO:root:Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.082851\n",
            "INFO:root:Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.035470\n",
            "INFO:root:Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.065605\n",
            "INFO:root:Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.091334\n",
            "INFO:root:Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.046328\n",
            "INFO:root:Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.072477\n",
            "INFO:root:Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.054702\n",
            "INFO:root:Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.049320\n",
            "INFO:root:Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.043346\n",
            "INFO:root:Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.077543\n",
            "INFO:root:Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.102646\n",
            "INFO:root:Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.024089\n",
            "INFO:root:Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.044542\n",
            "INFO:root:Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.066678\n",
            "INFO:root:Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.101265\n",
            "INFO:root:Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.069316\n",
            "INFO:root:Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.094575\n",
            "INFO:root:Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.114038\n",
            "INFO:root:Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.074180\n",
            "INFO:root:Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.042050\n",
            "INFO:root:Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.038683\n",
            "INFO:root:Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.073736\n",
            "INFO:root:Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.073936\n",
            "INFO:root:Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.152139\n",
            "INFO:root:Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.129210\n",
            "INFO:root:Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.149063\n",
            "INFO:root:Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.051211\n",
            "INFO:root:Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.141669\n",
            "INFO:root:Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.109365\n",
            "INFO:root:Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.048596\n",
            "INFO:root:Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.061360\n",
            "INFO:root:Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.194892\n",
            "INFO:root:Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.096199\n",
            "INFO:root:Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.030751\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0462, Accuracy: 9847/10000 (98%)\n",
            "\n",
            "INFO:root:Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.096502\n",
            "INFO:root:Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.051550\n",
            "INFO:root:Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.117309\n",
            "INFO:root:Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.113913\n",
            "INFO:root:Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.041117\n",
            "INFO:root:Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.051039\n",
            "INFO:root:Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.063649\n",
            "INFO:root:Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.104460\n",
            "INFO:root:Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.081523\n",
            "INFO:root:Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.150415\n",
            "INFO:root:Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.128063\n",
            "INFO:root:Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.026211\n",
            "INFO:root:Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.044206\n",
            "INFO:root:Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.035073\n",
            "INFO:root:Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.124048\n",
            "INFO:root:Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.138066\n",
            "INFO:root:Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.034981\n",
            "INFO:root:Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.092870\n",
            "INFO:root:Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.058715\n",
            "INFO:root:Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.064556\n",
            "INFO:root:Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.101064\n",
            "INFO:root:Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.060566\n",
            "INFO:root:Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.015311\n",
            "INFO:root:Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.093243\n",
            "INFO:root:Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.076662\n",
            "INFO:root:Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.012350\n",
            "INFO:root:Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.038441\n",
            "INFO:root:Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.078951\n",
            "INFO:root:Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.020845\n",
            "INFO:root:Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.114600\n",
            "INFO:root:Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.057663\n",
            "INFO:root:Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.162620\n",
            "INFO:root:Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.268888\n",
            "INFO:root:Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.067304\n",
            "INFO:root:Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.105795\n",
            "INFO:root:Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.064598\n",
            "INFO:root:Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.037106\n",
            "INFO:root:Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.082881\n",
            "INFO:root:Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.068275\n",
            "INFO:root:Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.033817\n",
            "INFO:root:Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.114116\n",
            "INFO:root:Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.186318\n",
            "INFO:root:Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.022737\n",
            "INFO:root:Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.039376\n",
            "INFO:root:Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.080405\n",
            "INFO:root:Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.057997\n",
            "INFO:root:Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.086235\n",
            "INFO:root:Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.064528\n",
            "INFO:root:Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.022087\n",
            "INFO:root:Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.039601\n",
            "INFO:root:Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.108434\n",
            "INFO:root:Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.035460\n",
            "INFO:root:Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.133819\n",
            "INFO:root:Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.044920\n",
            "INFO:root:Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.151196\n",
            "INFO:root:Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.113901\n",
            "INFO:root:Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.131925\n",
            "INFO:root:Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.074221\n",
            "INFO:root:Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.236510\n",
            "INFO:root:Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.080923\n",
            "INFO:root:Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.010909\n",
            "INFO:root:Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.069461\n",
            "INFO:root:Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.044679\n",
            "INFO:root:Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.131013\n",
            "INFO:root:Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.033764\n",
            "INFO:root:Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.013631\n",
            "INFO:root:Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.045635\n",
            "INFO:root:Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.094404\n",
            "INFO:root:Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.089714\n",
            "INFO:root:Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.058194\n",
            "INFO:root:Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.054701\n",
            "INFO:root:Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.047232\n",
            "INFO:root:Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.034531\n",
            "INFO:root:Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.078293\n",
            "INFO:root:Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.083001\n",
            "INFO:root:Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.085359\n",
            "INFO:root:Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.081938\n",
            "INFO:root:Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.114638\n",
            "INFO:root:Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.094018\n",
            "INFO:root:Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.052652\n",
            "INFO:root:Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.155381\n",
            "INFO:root:Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.023882\n",
            "INFO:root:Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.052029\n",
            "INFO:root:Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.159480\n",
            "INFO:root:Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.124388\n",
            "INFO:root:Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.163610\n",
            "INFO:root:Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.071845\n",
            "INFO:root:Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.113558\n",
            "INFO:root:Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.088714\n",
            "INFO:root:Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.029636\n",
            "INFO:root:Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.043962\n",
            "INFO:root:Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.161755\n",
            "INFO:root:Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.044292\n",
            "INFO:root:Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.023102\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0448, Accuracy: 9853/10000 (99%)\n",
            "\n",
            "INFO:root:Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.125599\n",
            "INFO:root:Train Epoch: 11 [640/60000 (1%)]\tLoss: 0.043099\n",
            "INFO:root:Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.074640\n",
            "INFO:root:Train Epoch: 11 [1920/60000 (3%)]\tLoss: 0.074543\n",
            "INFO:root:Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.036921\n",
            "INFO:root:Train Epoch: 11 [3200/60000 (5%)]\tLoss: 0.068943\n",
            "INFO:root:Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.031776\n",
            "INFO:root:Train Epoch: 11 [4480/60000 (7%)]\tLoss: 0.044702\n",
            "INFO:root:Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.138434\n",
            "INFO:root:Train Epoch: 11 [5760/60000 (10%)]\tLoss: 0.029454\n",
            "INFO:root:Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.114344\n",
            "INFO:root:Train Epoch: 11 [7040/60000 (12%)]\tLoss: 0.033011\n",
            "INFO:root:Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.085375\n",
            "INFO:root:Train Epoch: 11 [8320/60000 (14%)]\tLoss: 0.062717\n",
            "INFO:root:Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.022311\n",
            "INFO:root:Train Epoch: 11 [9600/60000 (16%)]\tLoss: 0.092727\n",
            "INFO:root:Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.082382\n",
            "INFO:root:Train Epoch: 11 [10880/60000 (18%)]\tLoss: 0.025431\n",
            "INFO:root:Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.041360\n",
            "INFO:root:Train Epoch: 11 [12160/60000 (20%)]\tLoss: 0.016513\n",
            "INFO:root:Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.080793\n",
            "INFO:root:Train Epoch: 11 [13440/60000 (22%)]\tLoss: 0.056724\n",
            "INFO:root:Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.007746\n",
            "INFO:root:Train Epoch: 11 [14720/60000 (25%)]\tLoss: 0.055245\n",
            "INFO:root:Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.105831\n",
            "INFO:root:Train Epoch: 11 [16000/60000 (27%)]\tLoss: 0.014718\n",
            "INFO:root:Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.036809\n",
            "INFO:root:Train Epoch: 11 [17280/60000 (29%)]\tLoss: 0.030347\n",
            "INFO:root:Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.020613\n",
            "INFO:root:Train Epoch: 11 [18560/60000 (31%)]\tLoss: 0.062419\n",
            "INFO:root:Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.078252\n",
            "INFO:root:Train Epoch: 11 [19840/60000 (33%)]\tLoss: 0.088684\n",
            "INFO:root:Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.396933\n",
            "INFO:root:Train Epoch: 11 [21120/60000 (35%)]\tLoss: 0.018598\n",
            "INFO:root:Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.163648\n",
            "INFO:root:Train Epoch: 11 [22400/60000 (37%)]\tLoss: 0.066741\n",
            "INFO:root:Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.043214\n",
            "INFO:root:Train Epoch: 11 [23680/60000 (39%)]\tLoss: 0.050975\n",
            "INFO:root:Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.022730\n",
            "INFO:root:Train Epoch: 11 [24960/60000 (42%)]\tLoss: 0.059805\n",
            "INFO:root:Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.238102\n",
            "INFO:root:Train Epoch: 11 [26240/60000 (44%)]\tLoss: 0.079246\n",
            "INFO:root:Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.029095\n",
            "INFO:root:Train Epoch: 11 [27520/60000 (46%)]\tLoss: 0.031571\n",
            "INFO:root:Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.133497\n",
            "INFO:root:Train Epoch: 11 [28800/60000 (48%)]\tLoss: 0.055777\n",
            "INFO:root:Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.157026\n",
            "INFO:root:Train Epoch: 11 [30080/60000 (50%)]\tLoss: 0.049706\n",
            "INFO:root:Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.022976\n",
            "INFO:root:Train Epoch: 11 [31360/60000 (52%)]\tLoss: 0.045284\n",
            "INFO:root:Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.058729\n",
            "INFO:root:Train Epoch: 11 [32640/60000 (54%)]\tLoss: 0.036258\n",
            "INFO:root:Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.113987\n",
            "INFO:root:Train Epoch: 11 [33920/60000 (57%)]\tLoss: 0.055596\n",
            "INFO:root:Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.211005\n",
            "INFO:root:Train Epoch: 11 [35200/60000 (59%)]\tLoss: 0.175463\n",
            "INFO:root:Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.149070\n",
            "INFO:root:Train Epoch: 11 [36480/60000 (61%)]\tLoss: 0.096965\n",
            "INFO:root:Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.111371\n",
            "INFO:root:Train Epoch: 11 [37760/60000 (63%)]\tLoss: 0.014148\n",
            "INFO:root:Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.024378\n",
            "INFO:root:Train Epoch: 11 [39040/60000 (65%)]\tLoss: 0.090739\n",
            "INFO:root:Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.030911\n",
            "INFO:root:Train Epoch: 11 [40320/60000 (67%)]\tLoss: 0.168392\n",
            "INFO:root:Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.024823\n",
            "INFO:root:Train Epoch: 11 [41600/60000 (69%)]\tLoss: 0.036141\n",
            "INFO:root:Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.052259\n",
            "INFO:root:Train Epoch: 11 [42880/60000 (71%)]\tLoss: 0.050405\n",
            "INFO:root:Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.044558\n",
            "INFO:root:Train Epoch: 11 [44160/60000 (74%)]\tLoss: 0.077079\n",
            "INFO:root:Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.090244\n",
            "INFO:root:Train Epoch: 11 [45440/60000 (76%)]\tLoss: 0.028307\n",
            "INFO:root:Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.027897\n",
            "INFO:root:Train Epoch: 11 [46720/60000 (78%)]\tLoss: 0.019570\n",
            "INFO:root:Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.086538\n",
            "INFO:root:Train Epoch: 11 [48000/60000 (80%)]\tLoss: 0.037892\n",
            "INFO:root:Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.055837\n",
            "INFO:root:Train Epoch: 11 [49280/60000 (82%)]\tLoss: 0.073478\n",
            "INFO:root:Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.087051\n",
            "INFO:root:Train Epoch: 11 [50560/60000 (84%)]\tLoss: 0.086583\n",
            "INFO:root:Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.106745\n",
            "INFO:root:Train Epoch: 11 [51840/60000 (86%)]\tLoss: 0.058802\n",
            "INFO:root:Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.053555\n",
            "INFO:root:Train Epoch: 11 [53120/60000 (88%)]\tLoss: 0.172280\n",
            "INFO:root:Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.101969\n",
            "INFO:root:Train Epoch: 11 [54400/60000 (91%)]\tLoss: 0.145707\n",
            "INFO:root:Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.063240\n",
            "INFO:root:Train Epoch: 11 [55680/60000 (93%)]\tLoss: 0.202536\n",
            "INFO:root:Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.107391\n",
            "INFO:root:Train Epoch: 11 [56960/60000 (95%)]\tLoss: 0.068089\n",
            "INFO:root:Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.051214\n",
            "INFO:root:Train Epoch: 11 [58240/60000 (97%)]\tLoss: 0.122669\n",
            "INFO:root:Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.069157\n",
            "INFO:root:Train Epoch: 11 [59520/60000 (99%)]\tLoss: 0.028463\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0403, Accuracy: 9864/10000 (99%)\n",
            "\n",
            "INFO:root:Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.075688\n",
            "INFO:root:Train Epoch: 12 [640/60000 (1%)]\tLoss: 0.030601\n",
            "INFO:root:Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.044004\n",
            "INFO:root:Train Epoch: 12 [1920/60000 (3%)]\tLoss: 0.098713\n",
            "INFO:root:Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.044178\n",
            "INFO:root:Train Epoch: 12 [3200/60000 (5%)]\tLoss: 0.067775\n",
            "INFO:root:Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.048228\n",
            "INFO:root:Train Epoch: 12 [4480/60000 (7%)]\tLoss: 0.088071\n",
            "INFO:root:Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.054096\n",
            "INFO:root:Train Epoch: 12 [5760/60000 (10%)]\tLoss: 0.087826\n",
            "INFO:root:Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.073856\n",
            "INFO:root:Train Epoch: 12 [7040/60000 (12%)]\tLoss: 0.037407\n",
            "INFO:root:Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.046536\n",
            "INFO:root:Train Epoch: 12 [8320/60000 (14%)]\tLoss: 0.030954\n",
            "INFO:root:Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.049826\n",
            "INFO:root:Train Epoch: 12 [9600/60000 (16%)]\tLoss: 0.119161\n",
            "INFO:root:Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.079105\n",
            "INFO:root:Train Epoch: 12 [10880/60000 (18%)]\tLoss: 0.025721\n",
            "INFO:root:Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.082545\n",
            "INFO:root:Train Epoch: 12 [12160/60000 (20%)]\tLoss: 0.037421\n",
            "INFO:root:Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.066141\n",
            "INFO:root:Train Epoch: 12 [13440/60000 (22%)]\tLoss: 0.032298\n",
            "INFO:root:Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.011488\n",
            "INFO:root:Train Epoch: 12 [14720/60000 (25%)]\tLoss: 0.031493\n",
            "INFO:root:Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.031815\n",
            "INFO:root:Train Epoch: 12 [16000/60000 (27%)]\tLoss: 0.047988\n",
            "INFO:root:Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.016040\n",
            "INFO:root:Train Epoch: 12 [17280/60000 (29%)]\tLoss: 0.085313\n",
            "INFO:root:Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.019215\n",
            "INFO:root:Train Epoch: 12 [18560/60000 (31%)]\tLoss: 0.063588\n",
            "INFO:root:Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.039522\n",
            "INFO:root:Train Epoch: 12 [19840/60000 (33%)]\tLoss: 0.143569\n",
            "INFO:root:Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.167394\n",
            "INFO:root:Train Epoch: 12 [21120/60000 (35%)]\tLoss: 0.053879\n",
            "INFO:root:Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.024923\n",
            "INFO:root:Train Epoch: 12 [22400/60000 (37%)]\tLoss: 0.106664\n",
            "INFO:root:Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.081184\n",
            "INFO:root:Train Epoch: 12 [23680/60000 (39%)]\tLoss: 0.058300\n",
            "INFO:root:Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.025975\n",
            "INFO:root:Train Epoch: 12 [24960/60000 (42%)]\tLoss: 0.016824\n",
            "INFO:root:Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.098126\n",
            "INFO:root:Train Epoch: 12 [26240/60000 (44%)]\tLoss: 0.107277\n",
            "INFO:root:Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.023380\n",
            "INFO:root:Train Epoch: 12 [27520/60000 (46%)]\tLoss: 0.042813\n",
            "INFO:root:Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.142859\n",
            "INFO:root:Train Epoch: 12 [28800/60000 (48%)]\tLoss: 0.091123\n",
            "INFO:root:Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.112719\n",
            "INFO:root:Train Epoch: 12 [30080/60000 (50%)]\tLoss: 0.031352\n",
            "INFO:root:Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.016432\n",
            "INFO:root:Train Epoch: 12 [31360/60000 (52%)]\tLoss: 0.020850\n",
            "INFO:root:Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.038572\n",
            "INFO:root:Train Epoch: 12 [32640/60000 (54%)]\tLoss: 0.040525\n",
            "INFO:root:Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.053626\n",
            "INFO:root:Train Epoch: 12 [33920/60000 (57%)]\tLoss: 0.018334\n",
            "INFO:root:Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.186910\n",
            "INFO:root:Train Epoch: 12 [35200/60000 (59%)]\tLoss: 0.135849\n",
            "INFO:root:Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.081801\n",
            "INFO:root:Train Epoch: 12 [36480/60000 (61%)]\tLoss: 0.081619\n",
            "INFO:root:Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.106961\n",
            "INFO:root:Train Epoch: 12 [37760/60000 (63%)]\tLoss: 0.039210\n",
            "INFO:root:Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.025928\n",
            "INFO:root:Train Epoch: 12 [39040/60000 (65%)]\tLoss: 0.087035\n",
            "INFO:root:Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.028997\n",
            "INFO:root:Train Epoch: 12 [40320/60000 (67%)]\tLoss: 0.151757\n",
            "INFO:root:Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.077413\n",
            "INFO:root:Train Epoch: 12 [41600/60000 (69%)]\tLoss: 0.025863\n",
            "INFO:root:Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.037209\n",
            "INFO:root:Train Epoch: 12 [42880/60000 (71%)]\tLoss: 0.071379\n",
            "INFO:root:Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.146027\n",
            "INFO:root:Train Epoch: 12 [44160/60000 (74%)]\tLoss: 0.054863\n",
            "INFO:root:Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.067355\n",
            "INFO:root:Train Epoch: 12 [45440/60000 (76%)]\tLoss: 0.019524\n",
            "INFO:root:Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.012646\n",
            "INFO:root:Train Epoch: 12 [46720/60000 (78%)]\tLoss: 0.027373\n",
            "INFO:root:Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.044268\n",
            "INFO:root:Train Epoch: 12 [48000/60000 (80%)]\tLoss: 0.079038\n",
            "INFO:root:Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.110320\n",
            "INFO:root:Train Epoch: 12 [49280/60000 (82%)]\tLoss: 0.074910\n",
            "INFO:root:Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.052754\n",
            "INFO:root:Train Epoch: 12 [50560/60000 (84%)]\tLoss: 0.075224\n",
            "INFO:root:Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.150852\n",
            "INFO:root:Train Epoch: 12 [51840/60000 (86%)]\tLoss: 0.033466\n",
            "INFO:root:Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.068502\n",
            "INFO:root:Train Epoch: 12 [53120/60000 (88%)]\tLoss: 0.135304\n",
            "INFO:root:Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.056911\n",
            "INFO:root:Train Epoch: 12 [54400/60000 (91%)]\tLoss: 0.178829\n",
            "INFO:root:Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.039214\n",
            "INFO:root:Train Epoch: 12 [55680/60000 (93%)]\tLoss: 0.108976\n",
            "INFO:root:Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.124230\n",
            "INFO:root:Train Epoch: 12 [56960/60000 (95%)]\tLoss: 0.110548\n",
            "INFO:root:Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.030378\n",
            "INFO:root:Train Epoch: 12 [58240/60000 (97%)]\tLoss: 0.107465\n",
            "INFO:root:Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.049641\n",
            "INFO:root:Train Epoch: 12 [59520/60000 (99%)]\tLoss: 0.018043\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0372, Accuracy: 9862/10000 (99%)\n",
            "\n",
            "INFO:root:Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.074104\n",
            "INFO:root:Train Epoch: 13 [640/60000 (1%)]\tLoss: 0.019231\n",
            "INFO:root:Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.032850\n",
            "INFO:root:Train Epoch: 13 [1920/60000 (3%)]\tLoss: 0.036013\n",
            "INFO:root:Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.021937\n",
            "INFO:root:Train Epoch: 13 [3200/60000 (5%)]\tLoss: 0.066199\n",
            "INFO:root:Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.085834\n",
            "INFO:root:Train Epoch: 13 [4480/60000 (7%)]\tLoss: 0.021782\n",
            "INFO:root:Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.049585\n",
            "INFO:root:Train Epoch: 13 [5760/60000 (10%)]\tLoss: 0.022361\n",
            "INFO:root:Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.133988\n",
            "INFO:root:Train Epoch: 13 [7040/60000 (12%)]\tLoss: 0.022166\n",
            "INFO:root:Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.055522\n",
            "INFO:root:Train Epoch: 13 [8320/60000 (14%)]\tLoss: 0.085254\n",
            "INFO:root:Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.122740\n",
            "INFO:root:Train Epoch: 13 [9600/60000 (16%)]\tLoss: 0.095106\n",
            "INFO:root:Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.039439\n",
            "INFO:root:Train Epoch: 13 [10880/60000 (18%)]\tLoss: 0.035575\n",
            "INFO:root:Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.025196\n",
            "INFO:root:Train Epoch: 13 [12160/60000 (20%)]\tLoss: 0.023715\n",
            "INFO:root:Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.045745\n",
            "INFO:root:Train Epoch: 13 [13440/60000 (22%)]\tLoss: 0.040408\n",
            "INFO:root:Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.023701\n",
            "INFO:root:Train Epoch: 13 [14720/60000 (25%)]\tLoss: 0.048572\n",
            "INFO:root:Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.049060\n",
            "INFO:root:Train Epoch: 13 [16000/60000 (27%)]\tLoss: 0.056388\n",
            "INFO:root:Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.032444\n",
            "INFO:root:Train Epoch: 13 [17280/60000 (29%)]\tLoss: 0.026517\n",
            "INFO:root:Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.021688\n",
            "INFO:root:Train Epoch: 13 [18560/60000 (31%)]\tLoss: 0.057325\n",
            "INFO:root:Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.055519\n",
            "INFO:root:Train Epoch: 13 [19840/60000 (33%)]\tLoss: 0.145360\n",
            "INFO:root:Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.209441\n",
            "INFO:root:Train Epoch: 13 [21120/60000 (35%)]\tLoss: 0.052771\n",
            "INFO:root:Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.042262\n",
            "INFO:root:Train Epoch: 13 [22400/60000 (37%)]\tLoss: 0.041012\n",
            "INFO:root:Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.055362\n",
            "INFO:root:Train Epoch: 13 [23680/60000 (39%)]\tLoss: 0.040327\n",
            "INFO:root:Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.026738\n",
            "INFO:root:Train Epoch: 13 [24960/60000 (42%)]\tLoss: 0.043307\n",
            "INFO:root:Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.113651\n",
            "INFO:root:Train Epoch: 13 [26240/60000 (44%)]\tLoss: 0.066914\n",
            "INFO:root:Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.020935\n",
            "INFO:root:Train Epoch: 13 [27520/60000 (46%)]\tLoss: 0.031638\n",
            "INFO:root:Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.201221\n",
            "INFO:root:Train Epoch: 13 [28800/60000 (48%)]\tLoss: 0.069523\n",
            "INFO:root:Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.058603\n",
            "INFO:root:Train Epoch: 13 [30080/60000 (50%)]\tLoss: 0.035814\n",
            "INFO:root:Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.009666\n",
            "INFO:root:Train Epoch: 13 [31360/60000 (52%)]\tLoss: 0.048818\n",
            "INFO:root:Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.027060\n",
            "INFO:root:Train Epoch: 13 [32640/60000 (54%)]\tLoss: 0.039236\n",
            "INFO:root:Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.131154\n",
            "INFO:root:Train Epoch: 13 [33920/60000 (57%)]\tLoss: 0.068317\n",
            "INFO:root:Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.176016\n",
            "INFO:root:Train Epoch: 13 [35200/60000 (59%)]\tLoss: 0.077228\n",
            "INFO:root:Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.092039\n",
            "INFO:root:Train Epoch: 13 [36480/60000 (61%)]\tLoss: 0.052391\n",
            "INFO:root:Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.125899\n",
            "INFO:root:Train Epoch: 13 [37760/60000 (63%)]\tLoss: 0.046171\n",
            "INFO:root:Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.009277\n",
            "INFO:root:Train Epoch: 13 [39040/60000 (65%)]\tLoss: 0.049403\n",
            "INFO:root:Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.070710\n",
            "INFO:root:Train Epoch: 13 [40320/60000 (67%)]\tLoss: 0.164453\n",
            "INFO:root:Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.016634\n",
            "INFO:root:Train Epoch: 13 [41600/60000 (69%)]\tLoss: 0.018033\n",
            "INFO:root:Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.095487\n",
            "INFO:root:Train Epoch: 13 [42880/60000 (71%)]\tLoss: 0.021712\n",
            "INFO:root:Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.084841\n",
            "INFO:root:Train Epoch: 13 [44160/60000 (74%)]\tLoss: 0.063985\n",
            "INFO:root:Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.032773\n",
            "INFO:root:Train Epoch: 13 [45440/60000 (76%)]\tLoss: 0.023791\n",
            "INFO:root:Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.056295\n",
            "INFO:root:Train Epoch: 13 [46720/60000 (78%)]\tLoss: 0.071602\n",
            "INFO:root:Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.035231\n",
            "INFO:root:Train Epoch: 13 [48000/60000 (80%)]\tLoss: 0.065670\n",
            "INFO:root:Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.102599\n",
            "INFO:root:Train Epoch: 13 [49280/60000 (82%)]\tLoss: 0.137444\n",
            "INFO:root:Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.064984\n",
            "INFO:root:Train Epoch: 13 [50560/60000 (84%)]\tLoss: 0.070548\n",
            "INFO:root:Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.070970\n",
            "INFO:root:Train Epoch: 13 [51840/60000 (86%)]\tLoss: 0.031848\n",
            "INFO:root:Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.076746\n",
            "INFO:root:Train Epoch: 13 [53120/60000 (88%)]\tLoss: 0.112926\n",
            "INFO:root:Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.083351\n",
            "INFO:root:Train Epoch: 13 [54400/60000 (91%)]\tLoss: 0.088001\n",
            "INFO:root:Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.038539\n",
            "INFO:root:Train Epoch: 13 [55680/60000 (93%)]\tLoss: 0.094291\n",
            "INFO:root:Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.118905\n",
            "INFO:root:Train Epoch: 13 [56960/60000 (95%)]\tLoss: 0.069430\n",
            "INFO:root:Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.028907\n",
            "INFO:root:Train Epoch: 13 [58240/60000 (97%)]\tLoss: 0.150808\n",
            "INFO:root:Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.073726\n",
            "INFO:root:Train Epoch: 13 [59520/60000 (99%)]\tLoss: 0.041594\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0362, Accuracy: 9877/10000 (99%)\n",
            "\n",
            "INFO:root:Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.085591\n",
            "INFO:root:Train Epoch: 14 [640/60000 (1%)]\tLoss: 0.037259\n",
            "INFO:root:Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.028578\n",
            "INFO:root:Train Epoch: 14 [1920/60000 (3%)]\tLoss: 0.046012\n",
            "INFO:root:Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.026898\n",
            "INFO:root:Train Epoch: 14 [3200/60000 (5%)]\tLoss: 0.052680\n",
            "INFO:root:Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.051123\n",
            "INFO:root:Train Epoch: 14 [4480/60000 (7%)]\tLoss: 0.022129\n",
            "INFO:root:Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.177081\n",
            "INFO:root:Train Epoch: 14 [5760/60000 (10%)]\tLoss: 0.011339\n",
            "INFO:root:Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.094051\n",
            "INFO:root:Train Epoch: 14 [7040/60000 (12%)]\tLoss: 0.014242\n",
            "INFO:root:Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.096051\n",
            "INFO:root:Train Epoch: 14 [8320/60000 (14%)]\tLoss: 0.031972\n",
            "INFO:root:Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.073780\n",
            "INFO:root:Train Epoch: 14 [9600/60000 (16%)]\tLoss: 0.110332\n",
            "INFO:root:Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.063637\n",
            "INFO:root:Train Epoch: 14 [10880/60000 (18%)]\tLoss: 0.027278\n",
            "INFO:root:Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.016597\n",
            "INFO:root:Train Epoch: 14 [12160/60000 (20%)]\tLoss: 0.105006\n",
            "INFO:root:Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.063789\n",
            "INFO:root:Train Epoch: 14 [13440/60000 (22%)]\tLoss: 0.070978\n",
            "INFO:root:Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.025530\n",
            "INFO:root:Train Epoch: 14 [14720/60000 (25%)]\tLoss: 0.096170\n",
            "INFO:root:Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.034356\n",
            "INFO:root:Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.034246\n",
            "INFO:root:Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.036626\n",
            "INFO:root:Train Epoch: 14 [17280/60000 (29%)]\tLoss: 0.029533\n",
            "INFO:root:Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.018133\n",
            "INFO:root:Train Epoch: 14 [18560/60000 (31%)]\tLoss: 0.023718\n",
            "INFO:root:Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.032847\n",
            "INFO:root:Train Epoch: 14 [19840/60000 (33%)]\tLoss: 0.127277\n",
            "INFO:root:Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.173692\n",
            "INFO:root:Train Epoch: 14 [21120/60000 (35%)]\tLoss: 0.060390\n",
            "INFO:root:Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.093907\n",
            "INFO:root:Train Epoch: 14 [22400/60000 (37%)]\tLoss: 0.106910\n",
            "INFO:root:Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.037680\n",
            "INFO:root:Train Epoch: 14 [23680/60000 (39%)]\tLoss: 0.125850\n",
            "INFO:root:Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.078432\n",
            "INFO:root:Train Epoch: 14 [24960/60000 (42%)]\tLoss: 0.060182\n",
            "INFO:root:Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.084442\n",
            "INFO:root:Train Epoch: 14 [26240/60000 (44%)]\tLoss: 0.063887\n",
            "INFO:root:Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.027109\n",
            "INFO:root:Train Epoch: 14 [27520/60000 (46%)]\tLoss: 0.027857\n",
            "INFO:root:Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.129676\n",
            "INFO:root:Train Epoch: 14 [28800/60000 (48%)]\tLoss: 0.073591\n",
            "INFO:root:Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.071941\n",
            "INFO:root:Train Epoch: 14 [30080/60000 (50%)]\tLoss: 0.018584\n",
            "INFO:root:Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.004271\n",
            "INFO:root:Train Epoch: 14 [31360/60000 (52%)]\tLoss: 0.022047\n",
            "INFO:root:Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.028163\n",
            "INFO:root:Train Epoch: 14 [32640/60000 (54%)]\tLoss: 0.015364\n",
            "INFO:root:Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.103883\n",
            "INFO:root:Train Epoch: 14 [33920/60000 (57%)]\tLoss: 0.020377\n",
            "INFO:root:Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.145964\n",
            "INFO:root:Train Epoch: 14 [35200/60000 (59%)]\tLoss: 0.160916\n",
            "INFO:root:Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.086632\n",
            "INFO:root:Train Epoch: 14 [36480/60000 (61%)]\tLoss: 0.151698\n",
            "INFO:root:Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.146319\n",
            "INFO:root:Train Epoch: 14 [37760/60000 (63%)]\tLoss: 0.033619\n",
            "INFO:root:Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.012798\n",
            "INFO:root:Train Epoch: 14 [39040/60000 (65%)]\tLoss: 0.043013\n",
            "INFO:root:Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.036589\n",
            "INFO:root:Train Epoch: 14 [40320/60000 (67%)]\tLoss: 0.223012\n",
            "INFO:root:Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.012018\n",
            "INFO:root:Train Epoch: 14 [41600/60000 (69%)]\tLoss: 0.025915\n",
            "INFO:root:Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.051256\n",
            "INFO:root:Train Epoch: 14 [42880/60000 (71%)]\tLoss: 0.053617\n",
            "INFO:root:Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.064890\n",
            "INFO:root:Train Epoch: 14 [44160/60000 (74%)]\tLoss: 0.065314\n",
            "INFO:root:Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.113141\n",
            "INFO:root:Train Epoch: 14 [45440/60000 (76%)]\tLoss: 0.015217\n",
            "INFO:root:Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.052102\n",
            "INFO:root:Train Epoch: 14 [46720/60000 (78%)]\tLoss: 0.010768\n",
            "INFO:root:Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.045135\n",
            "INFO:root:Train Epoch: 14 [48000/60000 (80%)]\tLoss: 0.048201\n",
            "INFO:root:Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.059151\n",
            "INFO:root:Train Epoch: 14 [49280/60000 (82%)]\tLoss: 0.097086\n",
            "INFO:root:Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.024980\n",
            "INFO:root:Train Epoch: 14 [50560/60000 (84%)]\tLoss: 0.073945\n",
            "INFO:root:Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.061246\n",
            "INFO:root:Train Epoch: 14 [51840/60000 (86%)]\tLoss: 0.064813\n",
            "INFO:root:Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.056393\n",
            "INFO:root:Train Epoch: 14 [53120/60000 (88%)]\tLoss: 0.188506\n",
            "INFO:root:Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.036014\n",
            "INFO:root:Train Epoch: 14 [54400/60000 (91%)]\tLoss: 0.070557\n",
            "INFO:root:Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.015946\n",
            "INFO:root:Train Epoch: 14 [55680/60000 (93%)]\tLoss: 0.081545\n",
            "INFO:root:Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.034778\n",
            "INFO:root:Train Epoch: 14 [56960/60000 (95%)]\tLoss: 0.075554\n",
            "INFO:root:Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.046306\n",
            "INFO:root:Train Epoch: 14 [58240/60000 (97%)]\tLoss: 0.208542\n",
            "INFO:root:Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.036055\n",
            "INFO:root:Train Epoch: 14 [59520/60000 (99%)]\tLoss: 0.081128\n",
            "INFO:root:\n",
            "Test set: Average loss: 0.0348, Accuracy: 9880/10000 (99%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time: 1106.7788808345795\n",
            "Current quantization method: sign\n",
            "Learning rate: 0.01\n",
            "Loss: [0.21204949951171875, 0.140087060546875, 0.1100113639831543, 0.08817563858032226, 0.07599704666137695, 0.06270452499389649, 0.05377242012023926, 0.0497161075592041, 0.046186084365844725, 0.04481059284210205, 0.04028206577301025, 0.03719010162353516, 0.036209318161010744, 0.034846486282348633]\n",
            "Accuracy: [93.4, 95.7, 96.75, 97.27, 97.71, 98.03, 98.23, 98.39, 98.47, 98.53, 98.64, 98.62, 98.77, 98.8]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "cc_list=[]\n",
        "loss_list=[]\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import logging\n",
        "import bagua.torch_api as bagua\n",
        "import bagua_core \n",
        "import time\n",
        "\n",
        "bagua_core.install_deps()\n",
        "\n",
        "\n",
        "# Model for Neural Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "#------Training Model\n",
        "def train(args, model, train_loader, optimizer, epoch):\n",
        "    #??????? What does model.train() do exactly? Sets mode. \n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        #data: features , target: label\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        #Gradients are set back to zero here to avoid gradient accumulation\n",
        "        optimizer.zero_grad()\n",
        "        # Calculates predicted labels by using the model\n",
        "        output = model(data)\n",
        "        # Loss function using predicted labels and actual labels\n",
        "        loss = F.nll_loss(output, target)\n",
        "        # Backwards propagation    !!!calculates tensor loss gradient \n",
        "        loss.backward()\n",
        "        # Optimizer step selection\n",
        "        if args.fuse_optimizer:\n",
        "            optimizer.fuse_step()\n",
        "        else:\n",
        "            optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            logging.info(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(\n",
        "                output, target, reduction=\"sum\"\n",
        "            ).item()  # sum up batch loss\n",
        "            pred = output.argmax(\n",
        "                dim=1, keepdim=True\n",
        "            )  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    logging.info(\n",
        "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss,\n",
        "            correct,\n",
        "            len(test_loader.dataset),\n",
        "            100.0 * correct / len(test_loader.dataset),\n",
        "        )\n",
        "    )\n",
        "    return test_loss,correct\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description=\"PyTorch MNIST Example\")\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=64,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size for training (default: 64)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--test-batch-size\",\n",
        "        type=int,\n",
        "        default=1000,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size for testing (default: 1000)\",\n",
        "    )\n",
        "\n",
        "# set number of epochs here\n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        type=int,\n",
        "        default=14,\n",
        "        metavar=\"N\",\n",
        "        help=\"number of epochs to train (default: 14)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lr\",\n",
        "        type=float,\n",
        "        default=1.0,\n",
        "        metavar=\"LR\",\n",
        "        help=\"learning rate (default: 1.0)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gamma\",\n",
        "        type=float,\n",
        "        default=0.7,\n",
        "        metavar=\"M\",\n",
        "        help=\"Learning rate step gamma (default: 0.7)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--log-interval\",\n",
        "        type=int,\n",
        "        default=10,\n",
        "        metavar=\"N\",\n",
        "        help=\"how many batches to wait before logging training status\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--save-model\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"For Saving the current Model\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--algorithm\",\n",
        "        type=str,\n",
        "        default=\"qsparselocal\",\n",
        "        help=\"gradient_allreduce, bytegrad, decentralized, low_precision_decentralized, qadam, async\",\n",
        "        #Add new algorithm for testing------------------\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--async-sync-interval\",\n",
        "        default=500,\n",
        "        type=int,\n",
        "        help=\"Model synchronization interval(ms) for async algorithm\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--set-deterministic\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"set deterministic or not\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fuse-optimizer\",\n",
        "        action=\"store_true\",\n",
        "        default=False,\n",
        "        help=\"fuse optimizer or not\",\n",
        "    )\n",
        "\n",
        "    #args = parser.parse_args() \n",
        "    # New line below solves ipykernel_launcher.py: error: unrecognized arguments\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    \n",
        "    if args.set_deterministic:\n",
        "        print(\"set_deterministic: True\")\n",
        "        np.random.seed(666)\n",
        "        random.seed(666)\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.manual_seed(666)\n",
        "        torch.cuda.manual_seed_all(666 + int(bagua.get_rank()))\n",
        "        torch.set_printoptions(precision=10)\n",
        "\n",
        "    torch.cuda.set_device(bagua.get_local_rank())\n",
        "    bagua.init_process_group()\n",
        "\n",
        "\n",
        "\n",
        "    logging.basicConfig(format=\"%(levelname)s:%(message)s\", level=logging.ERROR)\n",
        "    if bagua.get_rank() == 0:\n",
        "        logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    train_kwargs = {\"batch_size\": args.batch_size}\n",
        "    test_kwargs = {\"batch_size\": args.test_batch_size}\n",
        "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        "    )\n",
        "\n",
        "    if bagua.get_local_rank() == 0:\n",
        "        dataset1 = datasets.MNIST(\n",
        "            \"../data\", train=True, download=True, transform=transform\n",
        "        )\n",
        "        torch.distributed.barrier()\n",
        "    else:\n",
        "        torch.distributed.barrier()\n",
        "        dataset1 = datasets.MNIST(\n",
        "            \"../data\", train=True, download=True, transform=transform\n",
        "        )\n",
        "\n",
        "    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        dataset1, num_replicas=bagua.get_world_size(), rank=bagua.get_rank()\n",
        "    )\n",
        "    train_kwargs.update(\n",
        "        {\n",
        "            \"sampler\": train_sampler,\n",
        "            \"batch_size\": args.batch_size // bagua.get_world_size(),\n",
        "            \"shuffle\": False,\n",
        "        }\n",
        "    )\n",
        "    # Train and Test dataset\n",
        "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "    # ??????????  Throw the instantiation of the network onto the cuda dvice\n",
        "    model = Net().cuda()\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
        "\n",
        "\n",
        "    if args.algorithm == \"gradient_allreduce\":\n",
        "        from bagua.torch_api.algorithms import gradient_allreduce\n",
        "\n",
        "        algorithm = gradient_allreduce.GradientAllReduceAlgorithm()\n",
        "    elif args.algorithm == \"decentralized\":\n",
        "        from bagua.torch_api.algorithms import decentralized\n",
        "\n",
        "        algorithm = decentralized.DecentralizedAlgorithm()\n",
        "    elif args.algorithm == \"low_precision_decentralized\":\n",
        "        from bagua.torch_api.algorithms import decentralized\n",
        "\n",
        "        algorithm = decentralized.LowPrecisionDecentralizedAlgorithm()\n",
        "    elif args.algorithm == \"bytegrad\":\n",
        "        from bagua.torch_api.algorithms import bytegrad\n",
        "\n",
        "        algorithm = bytegrad.ByteGradAlgorithm()\n",
        "    elif args.algorithm == \"qadam\":\n",
        "        from bagua.torch_api.algorithms import q_adam\n",
        "\n",
        "        optimizer = q_adam.QAdamOptimizer(\n",
        "            model.parameters(), lr=args.lr, warmup_steps=100\n",
        "        )\n",
        "        algorithm = q_adam.QAdamAlgorithm(optimizer)\n",
        "    #################################################################    \n",
        "    elif args.algorithm == \"qsparselocal\":\n",
        "        learning_rate = 0.01\n",
        "        # Set lower learning rate, no convergence for lr = 1\n",
        "        optimizer = QSparseLocalOptimizer(\n",
        "            model.parameters(), lr=learning_rate, schedule = 2\n",
        "        )\n",
        "        algorithm = QSparseLocalAlgorithm(optimizer)\n",
        "    elif args.algorithm == \"async\":\n",
        "        from bagua.torch_api.algorithms import async_model_average\n",
        "\n",
        "        algorithm = async_model_average.AsyncModelAverageAlgorithm(\n",
        "            sync_interval_ms=args.async_sync_interval,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    #  Model von Bagua\n",
        "    model = model.with_bagua(\n",
        "        [optimizer],\n",
        "        algorithm,\n",
        "        do_flatten=not args.fuse_optimizer,\n",
        "    )\n",
        "\n",
        "    # Optimizer from Bagua if args.fuse_optimizer==True\n",
        "    if args.fuse_optimizer:\n",
        "        optimizer = bagua.contrib.fuse_optimizer(optimizer)\n",
        "\n",
        "    #------------ Loss, accuracy\n",
        "    loss_list =[]\n",
        "    acc_list = []\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "    \n",
        "    start = time.time()\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        if args.algorithm == \"async\":\n",
        "            model.bagua_algorithm.resume(model)\n",
        "\n",
        "        train(args, model, train_loader, optimizer, epoch)\n",
        "\n",
        "        if args.algorithm == \"async\":\n",
        "            model.bagua_algorithm.abort(model)\n",
        "\n",
        "        new_loss,new_acc =test(model, test_loader)\n",
        "        loss_list.append(new_loss)\n",
        "        acc_list.append(new_acc/100.0)\n",
        "        scheduler.step()\n",
        "        ####\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    if args.save_model:\n",
        "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
        "\n",
        "    # Used for measuring the time taken for the epochs themselves\n",
        "    end = time.time()\n",
        "    print(\"Elapsed time:\",end-start)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    ep =[]\n",
        "\n",
        "\n",
        "    for i in range(1, args.epochs + 1):\n",
        "      ep.append(i)\n",
        "\n",
        "    print(\"Current quantization method:\",quantization_scheme)\n",
        "    print(\"Learning rate:\",learning_rate)\n",
        "    print(\"Loss:\",loss_list)\n",
        "    print(\"Accuracy:\",acc_list)\n",
        "    \n",
        "     \n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.subplot(211)\n",
        "    plt.plot(ep,loss_list)\n",
        "    plt.subplot(212)\n",
        "    plt.plot(ep,acc_list)\n",
        "\n",
        "    plt.show()  \n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rSo-g6qDIvE",
        "outputId": "399e2644-68e0-4767-a689-3a966e5e5ae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No traceback available to show.\n"
          ]
        }
      ],
      "source": [
        "%tb"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "QSparseLocal_MNIST_5_5_comp_step.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO6ArZLQecgxyvWZNWVgyq4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}