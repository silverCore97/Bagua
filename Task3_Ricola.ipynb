{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverCore97/Bagua/blob/main/Task3_Ricola.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imported modules"
      ],
      "metadata": {
        "id": "_JxvFit3mGhA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KJ5uadTt2ahV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# Seed required to be fixed\n",
        "tf.random.set_seed(38)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Activation, Dense, Dropout, Concatenate, BatchNormalization\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
        "import tensorflow.compat.v2 as tfv2\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from matplotlib import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount drive and set required paths. Just load train_triplets.txt, test_triplets.txt and food.zip into your Colab Notebooks folder. Read list below to make sure all required files are in the folder."
      ],
      "metadata": {
        "id": "lNT6wcScmMMw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBh2exAfErhW",
        "outputId": "f121a8c5-d8bd-44e9-c61a-6fd8bcc033a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['cnn.ipynb', 'food', 'Model_savings', 'resnet152_Test_3', 'Kopie von Task4_transfer_learning_individual_image_transformation-resnet152.ipynb', 'task4_v6.ipynb', 'Project (1).ipynb', 'Task4.ipynb', 'Project.ipynb', 'Kopie von W2_exercise (2).ipynb', 'Kopie von W2_exercise (1).ipynb', 'Kopie von W2_exercise.ipynb', 'Benchmark test.ipynb', 'BaguaTest.ipynb', 'CNN_3.ipynb', 'ResNet blocks.ipynb', 'Cifar100.ipynb', 'Batch Normalization.ipynb', 'tensorflow basics.ipynb', '卷起来 tensorflow.ipynb', 'APPG.ipynb', 'Untitled0.ipynb', 'Least squares.ipynb', 'Parameter to group server.ipynb', 'Prague.ipynb', 'Efficient Allreduce.ipynb', 'BaguaExampleImagenet.ipynb', 'bagua torch_api contrib fuse optimizer.py.ipynb', 'Task2.ipynb', 'Horovod_new_alg.ipynb', 'qadam.ipynb', 'qslocal.ipynb', 'Untitled1.ipynb', 'Untitled (1)', 'Bagua_Example_MNIST_with_alg.ipynb', 'Untitled2.ipynb', '3D.ipynb', 'Kopie von Task3.ipynb', 'Kopie von colab2.ipynb', 'Task3.ipynb', 'food_270_224_in', 'Kopie von Kopie von colab2.ipynb', 'Task3 with Keras_kqian.ipynb', 'food_standard2', 'results.txt', 'submission.txt', 'Kopie von Task3_Ricola.ipynb', 'train_triplets.txt', 'test_triplets.txt', 'food.zip', 'task4_hint_on_pretraining.pdf', 'task4_hr35z9.zip', 'solarml', 'Kopie von QSparseLocal_5_5_working_correct_order.ipynb', 'Unzip in drive.ipynb', 'HOMO-LUMO.ipynb', 'Task4 - pytorch', 'IML_Task4_Ricola.ipynb', '_IML_Task4.ipynb', 'ricola_task4_final_version_1.ipynb', 'ricola_task4_final_version.ipynb', 'QSparseLocal_16_5_working_correct_order.ipynb', 'Kopie von QSparseLocal_26_5_MNIST_benchmark_VGG.ipynb', 'Kopie von kun_debug', 'Kopie von QSparseLocal_19_6_MNIST_benchmark_VGG (1).ipynb', 'Kopie von QSparseLocal_19_6_MNIST_benchmark_VGG.ipynb', 'QSparseLocal_19_6_MNIST_benchmark_VGG.ipynb', 'Kopie von Basic of C++.ipynb', 'C++ex.ipynb', 'Kopie von QSparseLocal_26_5_MNIST_benchmark.ipynb', 'kun_debug_', 'Untitled', 'CheckIO.ipynb', 'Kopie von debug', 'Kopie von _IML_Task4.ipynb', 'SolarML', 'val_triplets_custom.txt', 'train_triplets_custom.txt', 'Task3_Ricola.ipynb']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_triplets_path = '/content/drive/MyDrive/Colab Notebooks/train_triplets.txt'\n",
        "test_triplets_path = '/content/drive/MyDrive/Colab Notebooks/test_triplets.txt'\n",
        "train_triplets_path = '/content/drive/MyDrive/Colab Notebooks/train_triplets_custom.txt'\n",
        "val_triplets_path = '/content/drive/MyDrive/Colab Notebooks/val_triplets_custom.txt'\n",
        "food_path = '/content/drive/MyDrive/Colab Notebooks/food.zip'\n",
        "print(os.listdir(\"/content/drive/MyDrive/Colab Notebooks\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to generate new disjoint training and validation picture sets and their correponding triplets from the original training set."
      ],
      "metadata": {
        "id": "UWl7EAIUgeID"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K1Aes3uVePn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef1357a-9f64-4d86-8905-94e5bcecf473"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ * ] Generating triplets\n"
          ]
        }
      ],
      "source": [
        "# Read triplets as list of tuples\n",
        "def read_file(filename: str, n=None):\n",
        "    \"\"\"\n",
        "    reads one of the files test_triplets.txt or train_triplets.txt and return a list of tuples (a, b, c)\n",
        "        as described in the exercise description.\n",
        "    :param filename:\n",
        "    :param n: how many lines to load. all if n is empty or None\n",
        "    :return: list of tuples (a, b, c), where a, b, c are all integers in [0, 9999] padded with 0 s.t len == 5\n",
        "    \"\"\"\n",
        "    with open(filename) as fobj:\n",
        "        r = [tuple(map(int, line.strip().split(\" \"))) for i, line in enumerate(fobj)]\n",
        "        return r[:n] if n else r\n",
        "\n",
        "\n",
        "# Create train and validation set using disjoint picture sets\n",
        "def generate_train_val_triplets(n = 1000):\n",
        "    \"\"\"\n",
        "    generete a train_triplets_custom.txt and a val_triplets_custom.txt file, which can then be loaded by\n",
        "    ImageTripletClass (see helper.py), s.t. the images for validating and training are distinct.\n",
        "    :param n: how many images approx in the val set\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    print(\"[ * ] Generating triplets\")\n",
        "    triplets = read_file(\"drive/MyDrive/Colab Notebooks/train_triplets.txt\")\n",
        "    train_triplets = 0\n",
        "    val_triplets = 0\n",
        "\n",
        "    val_numbers = {random.randint(0, 4999) for _ in range(n)}\n",
        "\n",
        "    with open(\"drive/MyDrive/Colab Notebooks/train_triplets_custom.txt\", \"w\") as train_wobj, open(\"drive/MyDrive/Colab Notebooks/val_triplets_custom.txt\", \"w\") as val_wobj:\n",
        "        for a, b, c in triplets:\n",
        "            if a not in val_numbers and b not in val_numbers and c not in val_numbers:\n",
        "                train_wobj.write(f\"{a} {b} {c}\\n\")\n",
        "                train_triplets += 1\n",
        "\n",
        "            elif a in val_numbers and b in val_numbers and c in val_numbers:\n",
        "                val_wobj.write(f\"{a} {b} {c}\\n\")\n",
        "                val_triplets += 1\n",
        "\n",
        "    print(train_triplets, val_triplets)\n",
        "    print(\"[ * ] Finished\")\n",
        "\n",
        "\n",
        "generate_train_val_triplets(1900) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING: Read these triplets"
      ],
      "metadata": {
        "id": "Wc8ZWNnjgLMn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8K7nqc2LQO1"
      },
      "outputs": [],
      "source": [
        "train_triplets = pd.read_csv(train_triplets_path, delim_whitespace=True, header=None, names =['anchor','positive','negative'])\n",
        "test_triplets = pd.read_csv(test_triplets_path, delim_whitespace=True, header=None, names =['anchor','positive','negative'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VALIDATION: Read these triplets"
      ],
      "metadata": {
        "id": "v8hRRwt6hcsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_triplets_path = '/content/drive/MyDrive/Colab Notebooks/train_triplets_custom.txt'\n",
        "val_triplets_path = '/content/drive/MyDrive/Colab Notebooks/val_triplets_custom.txt'\n",
        "train_triplets = pd.read_csv(train_triplets_path, delim_whitespace=True, header=None, names =['anchor','positive','negative'])\n",
        "val_triplets = pd.read_csv(val_triplets_path, delim_whitespace=True, header=None, names =['anchor','positive','negative'])"
      ],
      "metadata": {
        "id": "KSdHDN6ehgd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the food.zip file if you have not done so already.\n"
      ],
      "metadata": {
        "id": "i8x7UzSUfndD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B8FI3QkigNl"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('/content/drive/MyDrive/Colab Notebooks/food_standard2/food'):\n",
        "  with zipfile.ZipFile(food_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall('/content/drive/MyDrive/Colab Notebooks/food_standard2')\n",
        "\n",
        "print('Extracted images to directory.')\n",
        "print(len(os.listdir(\"/content/drive/MyDrive/Colab Notebooks/food_standard2/food\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we preprocess the 10000 food pictures into an equal number of feature vectors. First we reshape the pictures to the required input shape of Inception-Resnet v2.\n",
        "\n",
        "Then we let it run through the preprocessing from keras for this network. After preprocessing, \n",
        "each preprocessed picture is run through the convolutional network to be turned into feature vectors.\n",
        "We have include_top=False so that the final fully connected layer of the network is not included.\n",
        "(So we can put our own there.)"
      ],
      "metadata": {
        "id": "SQUnM_BJfhNS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVAx_VxmAYt1"
      },
      "outputs": [],
      "source": [
        "# Needed for input of the pretrained model\n",
        "input_shape = (299,299,3)\n",
        "\n",
        "# Generate Dataset\n",
        "train_res = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/food_standard2/food',\n",
        "    label_mode=None,\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    image_size=(299, 299),\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "base_model_inception = tf.keras.applications.InceptionResNetV2(pooling='avg',include_top=False)\n",
        "# base_model_inception = hub.KerasLayer('https://tfhub.dev/google/aiy/vision/classifier/food_V1/1')\n",
        "\n",
        "\n",
        "# freeze the weights\n",
        "base_model_inception.trainable=False\n",
        "\n",
        "# declare input\n",
        "inputs = Input(shape=input_shape)\n",
        "x = tf.keras.applications.inception_resnet_v2.preprocess_input(inputs)\n",
        "outputs = base_model_inception(x)\n",
        "\n",
        "# obtain model and return\n",
        "base_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compute features\n",
        "features = base_model.predict(train_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we simply copy the original triplets and invert negative and positive picture for each triplet as well setting the label of those triplets to 0 so that the network will not be biased to any label.\n",
        "\n",
        "Then we stack the original and inverted triplets to receive the combined training triplets."
      ],
      "metadata": {
        "id": "uJ7AD54rdTZv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE7i-KVNkQXY"
      },
      "outputs": [],
      "source": [
        "df = train_triplets.copy()\n",
        "df_switched = df[['anchor', 'negative', 'positive']]\n",
        "df_switched = df_switched.rename(columns={\"anchor\": \"anchor\", \"negative\": \"positive\", 'positive': 'negative'})\n",
        "\n",
        "df['label'] = 1\n",
        "df_switched['label'] = 0\n",
        "\n",
        "df_combined = pd.concat([df, df_switched])\n",
        "df_combined = df_combined.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Discarded method of wrong validation (validation pictures not independent from training pictures)\n",
        "# df_train, df_val = train_test_split(df_combined, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VALIDATION"
      ],
      "metadata": {
        "id": "WhlyzzX6pZvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_triplets['label'] = 1"
      ],
      "metadata": {
        "id": "MoxMGJMGpdHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We generate the actual triplet feature vectors here by concatenating the three feature vectors corresponding to each triplet in the triplet DataFrame object. The label of each triplets is stored in an empty labels array to which the label values are appended. In the test case, no labels exists."
      ],
      "metadata": {
        "id": "R8HwOVRsiPQY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrWNs9Gg9lgq"
      },
      "outputs": [],
      "source": [
        "# Create triplet features from train or test triplets\n",
        "def generate_feature_tensor(features, triplet_df, train=False):\n",
        "  if train:\n",
        "    # For correct shape of tensor\n",
        "    train_tensor = np.zeros((triplet_df.shape[0],3*features.shape[1]))\n",
        "    labels = np.empty(0)\n",
        "\n",
        "    for i in range(triplet_df.shape[0]):\n",
        "      anchor, positive, negative, label = triplet_df.iloc[i,:]\n",
        "\n",
        "      triplet_features = np.concatenate([features[int(anchor),:],features[int(positive),:],features[int(negative),:]])\n",
        "      train_tensor[i] = triplet_features\n",
        "      labels = np.append(labels, label)\n",
        "\n",
        "    return train_tensor, labels\n",
        "\n",
        "  else:\n",
        "    # For correct shape of tensor\n",
        "    test_tensor = np.zeros((triplet_df.shape[0],3*features.shape[1]))\n",
        "    for i in range(triplet_df.shape[0]):\n",
        "      anchor, positive, negative = triplet_df.iloc[i,:]\n",
        "\n",
        "      triplet_features = np.concatenate([features[int(anchor),:],features[int(positive),:],features[int(negative),:]])\n",
        "      test_tensor[i] = triplet_features\n",
        "\n",
        "    return test_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING: Only training feature tensors generated here to save space."
      ],
      "metadata": {
        "id": "DBVo4pIjh2z5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyVqUnvUCqHw"
      },
      "outputs": [],
      "source": [
        "test_tensor = None\n",
        "predictions = None\n",
        "train_tensor, labels = generate_feature_tensor(features, df_combined, train=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VALIDATION: Labels exist for both training and validation set"
      ],
      "metadata": {
        "id": "wEcR1KGWh8iQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cymdXfYxjUjU"
      },
      "outputs": [],
      "source": [
        "train_tensor, labels = generate_feature_tensor(features, df_combined, train=True)\n",
        "val_tensor, val_labels = generate_feature_tensor(features, val_triplets, train=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each concatenated triplet feature vector is now run through multiple layers of a trainable fully connected network ending with a sigmoid activation function. (To keep the final value between 0 and 1)"
      ],
      "metadata": {
        "id": "pqsR4fctjx6w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1EbFg04oJhP"
      },
      "outputs": [],
      "source": [
        "input = Input(train_tensor.shape[1:])\n",
        "x0 = Activation('relu')(input)\n",
        "x1 = Dropout(0.6)(x0)\n",
        "x2 = Dense(1600)(x1)\n",
        "x3 = Activation('relu')(x2)\n",
        "x4 = Dropout(0.3)(x3)\n",
        "x5 = Dense(400)(x4)\n",
        "x6 = Activation('relu')(x5)\n",
        "x7 = Dropout(0.1)(x6)\n",
        "x8 = Dense(100)(x7)\n",
        "x9 = Activation('relu')(x8)\n",
        "x10 = Dense(25)(x9)\n",
        "x11 = Activation('relu')(x10)\n",
        "x12 = Dense(1)(x11)\n",
        "x13 = Activation('sigmoid')(x12)\n",
        "model = Model(inputs=input, outputs=x13)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING: We run the model for 15 epochs"
      ],
      "metadata": {
        "id": "9fOVsnLtkJ8A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTXl1Q2OsfcW"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x=train_tensor, y=labels, epochs=15, verbose=1)  # Can see progress"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VALIDATION: We run the training for 20 epochs, one epoch at a time. After each epoch, we determine the validation accuracy.\n",
        "\n",
        "Note: Rerunning loop after a few epochs does not change current state of model."
      ],
      "metadata": {
        "id": "LSXmij6sk9EZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gBZHVfhP48y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use for validation with custom train/val sets\n",
        "for i in range(20):  \n",
        "  history = model.fit(x=train_tensor, y=labels, epochs=1,verbose = 1)\n",
        "  val_predict=model.predict(val_tensor)\n",
        "  bool_list =[p <0.5 for p in val_predict]\n",
        "  int_list = [0 if b else 1 for b in bool_list]\n",
        "  accuracy_diff=tf.math.reduce_mean(tf.abs(val_labels-int_list))\n",
        "  accuracy_diff_actual=tf.math.reduce_mean(tf.abs(val_labels-val_predict))\n",
        "  print(\"Epoch\",i,\": Validation accuracy binary value: \",1-accuracy_diff)\n",
        "  print(\"Epoch\",i,\": Validation accuracy actual value: \",1-accuracy_diff_actual)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VALIDATION END"
      ],
      "metadata": {
        "id": "9MfHhBn5llbI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpSbmJv_6a9S"
      },
      "outputs": [],
      "source": [
        "# Remove unneeded variables by setting them to None\n",
        "train_tensor = None\n",
        "history = None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We generate the test concatenated triplet feature vectors now to save space."
      ],
      "metadata": {
        "id": "X26hzbz6lo51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2HLmJrb6u7M"
      },
      "outputs": [],
      "source": [
        "test_tensor = generate_feature_tensor(features, test_triplets, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiIpAx1-60fi"
      },
      "outputs": [],
      "source": [
        "# Features no longer needed\n",
        "del features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction of test labels according to trained model."
      ],
      "metadata": {
        "id": "GhsX50kcl-pp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjIZERymS53l"
      },
      "outputs": [],
      "source": [
        "# Predict\n",
        "predictions = model.predict(test_tensor)\n",
        "bool_list =[p <0.5 for p in predictions]\n",
        "int_list = [0 if b else 1 for b in bool_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write results of the test into 'results.txt'."
      ],
      "metadata": {
        "id": "y-KfL50gl1cs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGdM96eZT0Xz"
      },
      "outputs": [],
      "source": [
        "with open('drive/MyDrive/Colab Notebooks/results.txt', 'w') as my_list_file:\n",
        "    # looping over the each ist element\n",
        "    for element in int_list:\n",
        "        # writing to file line by line\n",
        "        my_list_file.write('%s\\n' % element)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Task3_Ricola.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}